# -*- coding: utf-8 -*-
"""Data Industry Salary Prediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lDlqmcv6xav48C5uj2YLJwK5UJ3BNYET

# **Introduction**

The purpose of this project is to determine which data analysis skills are associated with higher salaries. We will use data scraped from [Glassdoor](https://www.glassdoor.com/member/home/index.htm). 

**Job Titles:**
*   [Data Analyst](https://colab.research.google.com/drive/1AW0rNTs84Cq7GrJiDq86WWCisB4q6ZwO?usp=sharing)
*   [Business Analyst](https://colab.research.google.com/drive/1I1lolzuZgoUZGBwAEP9hZNf7my8XItbz?usp=sharing)
*   [Data Engineer](https://colab.research.google.com/drive/18HMhZXcv4xwxDUXGDauYEGdnDJLjNYxf?usp=sharing)
*   [Data Scientist](https://colab.research.google.com/drive/1i8UQMiObiOKAyzlrAsFCBPitqIh90Y5O?usp=sharing)

**Locations**
*   Austin, TX
*   Boston, MA
*   Chicago, IL
*   Colorado
*   Los Angeles, CA
*   New York City, NY
*   San Francisco, CA
*   Seattle, WA

This file contains original exploration of scraping desired info and combination of data after being scraped for analysis. To view the scraping files of each role, please select the respective link in the Job Titles list.

## **Set Environment & Initial Request**
"""

#set environment
import collections
import matplotlib.pyplot as plt
import nltk
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import requests
import scipy.stats as stats
import seaborn as sns
import spacy
import statsmodels.api as sm
import string
import textwrap

from bs4 import BeautifulSoup as soup
from collections import Counter
from IPython.core.display import clear_output
from nltk.corpus import stopwords
from random import randint
from sklearn import svm
from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_extraction import text
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.metrics import  classification_report, mean_absolute_error, precision_score, recall_score, f1_score, accuracy_score, confusion_matrix
from sklearn.neighbors import KNeighborsRegressor, LocalOutlierFactor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from xgboost.sklearn import XGBRegressor
from statsmodels.tools.eval_measures import mse, rmse
from time import time, sleep
from wordcloud import WordCloud

nltk.download('stopwords')

#ignore warnings
import warnings
warnings.filterwarnings("ignore")

# #set url & headers for data analyst roles
# da_url = 'https://www.glassdoor.com/Job/chicago-data-analyst-jobs-SRCH_IL.0,7_IC1128808_KO8,20_IP2.htm?radius=100&minSalary=26000&includeNoSalaryJobs=false&maxSalary=112000&fromAge=7'
# headers = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36 RuxitSynthetic/1.0 v8980925411 t38550 ath9b965f92 altpub cvcv=2'}

# #get info from url
# da_response = requests.get(da_url, headers=headers)
# da_response.status_code

# #create a BeautifulSoup object from fetched data
# da_soup = soup(da_response.text, 'html.parser')

"""# **Extracting One Piece of Information at a Time**

## **Companies**
"""

# #Extract Company Names
# da_companies= da_soup.select('.css-l2wjgv.e1n63ojh0.jobLink')
# #extract just the Company Names text
# da_co_names = [da_co_name.get_text().strip() for da_co_name in da_companies]
# da_co_names

# len(da_co_names) #check length matches other queries

"""## **Company Ratings**"""

# #Extract the Ratings
# da_rankings= da_soup.select('.css-19pjha7.e1cjmv6j1')
# #extract just the Job Titles text
# da_co_rank = [da_co_rank.get_text().strip() for da_co_rank in da_rankings]
# da_co_rank

# len(da_co_rank) #check length matches other queries

"""## **Job Titles**"""

# #Extract the Job Titles
# da_job_titles= da_soup.select('.jobLink.css-1rd3saf.eigr9kq2')
# #extract just the Job Titles text
# da_positions = [da_position.get_text().strip() for da_position in da_job_titles]
# da_positions

# len(da_positions) #check length matches other queries

"""## **Locations**"""

# #Extract Job Locations
# da_job_locations= da_soup.select('.pr-xxsm.css-1ndif2q.e1rrn5ka0')
# #extract just the Job Locations text
# da_locations = [da_location.get_text() for da_location in da_job_locations]
# da_locations

# len(da_locations) #check length matches other queries

"""## **Salary Ranges**"""

# #Extract Job Salary Ranges
# da_salary_ranges= da_soup.select('.css-1imh2hq.e1wijj242')
# #extract just the Salary Ranges text
# da_sal_ranges = [da_sal_range.get_text() for da_sal_range in da_salary_ranges]
# da_sal_ranges

# len(da_sal_ranges) #check length matches other queries

"""## **Links**"""

# #Extract Job Links
# da_job_links= da_soup.select('.css-l2wjgv.e1n63ojh0.jobLink')
# da_job_links

# #extract just the Links text
# da_links = [da_link.get_text() for da_link in da_job_links]
# da_links

# #get_text() for job links didn't extract anticipated info
# #let's investigate first element of da_job_links list
# da_job_links[0]

# #see if we can access the link by adding ['href'] to previous call
# da_job_links[0]['href']

# #Extract Job Links using list theory
# da_job_links= da_soup.select('.css-l2wjgv.e1n63ojh0.jobLink') #find link class
# da_links = [] #initizlize list to store links
# for i in range(len(da_job_links)): #iterate over items in da_job links
#   #set link to have prefix of 'https://www.glassdoor.com' with whatever was in href section
#   link = 'https://www.glassdoor.com' + da_job_links[i]['href']
#   da_links.append(link) # add link to da_links list
# da_links #view links

# len(da_links)

"""## **Job Descriptions from Links**"""

# #investigate how to extrapolate the description from one job
# link_url = requests.get(da_links[0],headers=headers) #set link as first line from da_links
# link_soup = soup(link_url.content, 'html.parser') #turn contents in link to a bs object
# link_desc = link_soup.select('.desc.css-58vpdc.ecgq1xb4') #find the description
# link_desc[0].get_text() #get the text from the description

# #initialize list for descriptions
# da_descriptions = []

# #iterate over da_links list
# for i in range(len(da_links)):
#   #repeat process from last cell for all links in da_links
#   link_url = requests.get(da_links[i],headers=headers)
#   link_soup = soup(link_url.content, 'html.parser')
#   link_desc = link_soup.select('.desc.css-58vpdc.ecgq1xb4')
#   for i in range(len(link_desc)):
#     desc = link_desc[0].get_text()
#     da_descriptions.append(desc)
# #view descriptions
# da_descriptions

# len(da_descriptions) #check length matches other queries

"""## **Industry from Links**"""

# #investigate how to extrapolate the industry from one job
# link_url = requests.get(da_links[0],headers=headers) #set link as first line from da_links
# link_soup = soup(link_url.content, 'html.parser') #turn contents in link to a bs object
# link_industry = link_soup.select('.css-sr4ps0.e18tf5om4') #find the industry
# link_industry[1].get_text()

# #initialize list for industries
# da_industries = []

# #iterate over da_links list
# for i in range(len(da_links)):
#   #repeat process from descriptions
#   link_url = requests.get(da_links[i],headers=headers)
#   link_soup = soup(link_url.content, 'html.parser')
#   link_industry = link_soup.select('.css-sr4ps0.e18tf5om4')
#   for i in range(len(link_industry)):
#     industry = link_industry[1].get_text()
#   da_industries.append(industry)
# #view descriptions
# da_industries

# len(da_industries) #check length matches other queries

"""# **Scrape Multiple Pages**

### **Function to Process One Page**
"""

# def process_jobs(soup_obj, jobs):

#   #Create a soup object where are listings appear
#   all_jobs = soup_obj.select('.react-job-listing')

#   #Extract info for each job
#   for job in all_jobs:
#     company = job.select_one('.css-l2wjgv.e1n63ojh0.jobLink').get_text().strip()
#     rank = job.select_one('.css-19pjha7.e1cjmv6j1').get_text().strip()
#     job_title = job.select_one('.jobLink.css-1rd3saf.eigr9kq2').get_text().strip()
#     location = job.select_one('.pr-xxsm.css-1ndif2q.e1rrn5ka0').get_text().strip()
#     try:
#       salary_range = job.select_one('.css-1imh2hq.e1wijj242').get_text().strip()
#     except:
#       pass

#     #Extract Job Links
#     job_links= job.select_one('.css-l2wjgv.e1n63ojh0.jobLink') #find link class
#     links = [] #initizlize list to store links
#     for i in range(len(job_links)): #iterate over items in job_links
#       #set link to have prefix of 'https://www.glassdoor.com' with whatever was in href section
#       job_link = 'https://www.glassdoor.com' + job_links['href']
#       links.append(job_link) # add link to links list
#     link = links[i]

#     #initialize list for descriptions & industries
#     job_descriptions = []
#     job_industries = []

#     #iterate over links list to get descriptions & industries
#     for i in range(len(links)):
#       link_url = requests.get(links[i],headers=headers)
#       link_soup = soup(link_url.content, 'html.parser')
      
#       #descriptions
#       link_desc = link_soup.select('.desc.css-58vpdc.ecgq1xb4')
#       #get description from each link
#       for i in range(len(link_desc)):
#         desc = link_desc[0].get_text()
#         job_descriptions.append(desc)

#       #industries
#       link_industry = link_soup.select('.css-sr4ps0.e18tf5om4')
#       #get industry from each link
#       for i in range(len(link_industry)):
#         ind = link_industry[1].get_text()
#         job_industries.append(ind)
    
#     #assign one description & one industry to one job
#     for i in range(len(job_descriptions)):
#       description = job_descriptions[i]
    
#     for i in range(len(job_industries)):
#       industry = job_industries[i]

    
#     #construct a dictionary of job info
#     new_job = {'company':company, 'industry':industry, 'rank':rank, 
#               'job_title':job_title, 'location':location, 
#               'salary_range': salary_range, 'link': link,
#                'description':description}
    
#     jobs.append(new_job)

"""### **Process Sample Data Analyst Roles**"""

# #scraping data analyst roles
# start_time = time() #note time of scraping start
# request_count = 1 #track number of requests made

# #create a variable to store fetched data
# sample_da_jobs = []

# #variables to handle request loop
# max_requests = 3
# page_num = 1
# headers = {'user-agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36 RuxitSynthetic/1.0 v8980925411 t38550 ath9b965f92 altpub cvcv=2'}

# #set url, the url can be finicky, try the commented out one if the first doesn't work
# #also try running a glassdoor search of your own and see if the min & max salary values change
# da_url = 'https://www.glassdoor.com/Job/chicago-data-analyst-jobs-SRCH_IL.0,7_IC1128808_KO8,20_IP'+str(page_num)+'.htm?radius=100&minSalary=26000&includeNoSalaryJobs=false&maxSalary=112000&fromAge=7'
# #da_url = 'https://www.glassdoor.com/Job/chicago-data-analyst-jobs-SRCH_IL.0,7_IC1128808_KO8,20_IP'+str(page_num)+'.htm?radius=100&minSalary=26000&includeNoSalaryJobs=false&maxSalary=112000&fromAge=7'

# while request_count <= max_requests:
#   print('Scraping started')
#   #keep output clear
#   clear_output(wait=True)

#   #make request
#   da_response = requests.get(da_url, headers = headers)

#   if da_response.status_code == 200:
#     da_soup = soup(da_response.text, 'html.parser')
#     process_jobs(da_soup, sample_da_jobs)

#     #increment request count
#     request_count+=1

#     #go to sleep for a bit in between request
#     sleep(randint(1,5))

#     #output monitoring info
#     elapsed_time = time() - start_time
#     print('Requests: {}, Frequency: {} requests/s, {} jobs processed.'.format(request_count, request_count/elapsed_time, len(sample_da_jobs)))

#     #increment page_num
#     page_num+=1

#   else:
#     print('Error')

# print('******Scraping Complete******')
# print('Total Requests: {}, Frequency: {} requests/s, {} total jobs processed.'.format(request_count, request_count/elapsed_time, len(sample_da_jobs)))
# sample_da_jobs[:10]

"""### **Convert Sample Data Analyst Roles to a Dataframe**"""

# #convert da_jobs list to a df
# sample_da_jobs_df = pd.DataFrame(data = sample_da_jobs, columns = ['company', 'industry', 
#               'rank', 'job_title', 'location', 'salary_range', 'link','description'])

# sample_da_jobs_df

"""# **Combine Scraped Data from Each Role**

### **Import Each Role's Scraped Data**
"""

da_321_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/da_jobs_321_df.csv?dl=0')
ba_321_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/ba_jobs_321_df.csv?dl=0')
de_321_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/de_jobs_321_df.csv?dl=0')
ds_321_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/ds_jobs_321_df.csv?dl=0')

da_328_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/da_jobs_328_df.csv?dl=0')
ba_328_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/ba_jobs_328_df.csv?dl=0')
de_328_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/de_jobs_328_df.csv?dl=0')
ds_328_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/ds_jobs_328_df.csv?dl=0')

da_404_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/da_jobs_404_df.csv?dl=0')
ba_404_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/ba_jobs_404_df.csv?dl=0')
de_404_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/de_jobs_404_df.csv?dl=0')
ds_404_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/ds_jobs_404_df.csv?dl=0')

da_411_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/da_jobs_411_df.csv?dl=0')
ba_411_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/ba_jobs_411_df.csv?dl=0')
de_411_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/de_jobs_411_df.csv?dl=0')
ds_411_jobs = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/ds_jobs_411_df.csv?dl=0')

"""### **Combine Scraped Data**"""

#combine df's
search_list = [da_321_jobs, ba_321_jobs, de_321_jobs, ds_321_jobs, da_328_jobs, 
               ba_328_jobs, de_328_jobs, ds_328_jobs, da_404_jobs, 
               ba_404_jobs, de_404_jobs, ds_404_jobs, da_411_jobs, 
               ba_411_jobs, de_411_jobs, ds_411_jobs]
scraped_data = pd.concat(search_list, ignore_index=True)
scraped_data

"""### **Format Scraped Data**

#### **Drop Unnamed Column**
"""

#drop Unnamed column
scraped_data.drop(columns=['Unnamed: 0', 'Unnamed: 0.1'], inplace=True)

scraped_data

"""#### **Check for Duplicate Rows**"""

#Checking for duplicate rows based on specific columns
duplicate = scraped_data[scraped_data.duplicated(['company', 'industry', 'rank', 'job_title', 'location', 'salary_range',
       'description','search_city'])] 
print('Duplicate Rows:')
duplicate

#investigate one company for duplicates
duplicate_lib_mutual = scraped_data['company']=='Liberty Mutual Insurance'
scraped_data.iloc[duplicate_lib_mutual.values]

#investigate the rows that were duplicates
dup_lib_mutual = duplicate['company']=='Liberty Mutual Insurance'
duplicate.iloc[dup_lib_mutual.values]

#create a df with no duplicates to use for further eda
jobs_df = scraped_data.drop_duplicates(subset=['company', 'industry', 'rank', 
      'job_title', 'location', 'salary_range','description','search_city'])
jobs_df

"""#### **Missing Data**"""

#Check for missing data
total_missing = jobs_df.isnull().sum().sort_values(ascending=False)
percent_missing = (jobs_df.isnull().sum()/jobs_df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total_missing, percent_missing], axis=1, keys=['Total', 'Percent'])
missing_data

#drop the missing salary, industry, and rank observations
jobs_df.dropna(inplace=True)

#Check for missing data
total_missing = jobs_df.isnull().sum().sort_values(ascending=False)
percent_missing = (jobs_df.isnull().sum()/jobs_df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total_missing, percent_missing], axis=1, keys=['Total', 'Percent'])
missing_data

"""#### **Missing Rank**"""

#*Drop rows where rank was missing
missing_rank = jobs_df[jobs_df['rank']== -1].index
jobs_df.drop(missing_rank, inplace=True)
jobs_df.shape

"""#### **Fix Salray Range Issues**"""

#get rid of rows where salary_range is per hour instead of annual
jobs_df = jobs_df[~jobs_df['salary_range'].str.contains('Hour')]
jobs_df.shape

#get rid of rows where salary_range is exact amount instead of a range
jobs_df = jobs_df[jobs_df['salary_range'].str.contains('-')]
jobs_df.shape

#reset index values to reflect actual number of rows
jobs_df.reset_index(inplace=True)

"""#### **Split Location & Salaries into Separate Columns**"""

#create a df of split jobs['location'] to split column into city & state
new_loc = jobs_df['location'].str.split(',', expand= True)
new_loc

# getting 3 columns in new_loc was unexpected
#investigate to find issue
new_loc.sort_values(by=2)[:10]

#create a sub df to view indices of all listings that had Greenwood Village,
#Arapahoe, CO as location
is_arapahoe = jobs_df[(jobs_df['location'])=='Greenwood Village, Arapahoe, CO']
#extract desired indices
#list of indices that have Greenwood Village, Arapahoe, CO listed as location
is_arapahoe_indices = is_arapahoe.index.values.tolist()

#iterate over above indices to change location to Arapahoe, CO
for i in is_arapahoe_indices:
  jobs_df['location'][i] = 'Arapahoe, CO'

#see if it worked
jobs_df.loc[81]

#set location of jobs_df index 385 to Los Angeles, CA
jobs_df['location'][385] = 'Los Angeles, CA'
jobs_df.loc[385]

#try again to create a df of split jobs['location'] to split column into city & state
new_loc = jobs_df['location'].str.split(',', expand= True)
new_loc

#create a df of split jobs_df['salary_range] to split column into min, max, &
#estimate type in next cell block
new_sal = jobs_df['salary_range'].str.split(' ', expand=True)
new_sal

#assign city & state to jobs_df
jobs_df['city'] = new_loc[0]
jobs_df['state'] = new_loc[1]
jobs_df['state'] = jobs_df['state'].str.replace(' ','')

#assign min, max, & estimate to jobs_df
jobs_df['min_sal'] = new_sal[0]
jobs_df['max_sal'] = new_sal[2]
jobs_df['estimate_type'] = new_sal[3]

#drop previous salary_range column
jobs_df.drop(columns=['salary_range', 'location'], inplace=True)

#reformat some puntuation within certain columns
jobs_df['min_sal'] = jobs_df['min_sal'].astype(str).str.replace('$','')
jobs_df['min_sal'] = jobs_df['min_sal'].astype(str).str.replace('K','')
jobs_df['max_sal'] = jobs_df['max_sal'].astype(str).str.replace('$','')
jobs_df['max_sal'] = jobs_df['max_sal'].astype(str).str.replace('K','')
jobs_df['estimate_type'] = jobs_df['estimate_type'].astype(str).str.replace('(','')
jobs_df['estimate_type'] = jobs_df['estimate_type'].astype(str).str.replace(')','')
jobs_df['estimate_type'] = jobs_df['estimate_type'].astype(str).str.replace(' est.','')
jobs_df['estimate_type'] = jobs_df['estimate_type'].astype(str).str.replace('Per HourGlassdoor','Glassdoor')

#convert min & max salary estimates to numeric
jobs_df['min_sal'] = jobs_df['min_sal'].astype(int)
jobs_df['max_sal'] = jobs_df['max_sal'].astype(int)

#multiply min & max sal by 1,000
jobs_df['min_sal'] = jobs_df['min_sal']*1000
jobs_df['max_sal'] = jobs_df['max_sal']*1000

#create an average salary column, this is also median since there are only two 
#values we're considering 
jobs_df['avg_sal'] = (jobs_df['min_sal']+jobs_df['max_sal'])/2

#convert job_title to str
jobs_df['job_title'] = jobs_df['job_title'].astype(str)

jobs_df

"""#### **Fix Missing City Issue**"""

#find all observations with state WA
no_state = jobs_df[(jobs_df['state'].isnull())]
no_state

#set city and state of above jobs where the listing link inlcuded the state
jobs_df['city'][354]='Denver'
jobs_df['state'][354]='CO'

jobs_df['city'][361]='Denver'
jobs_df['state'][361]='CO'

jobs_df['city'][857]='Denver'
jobs_df['state'][857]='CO'

jobs_df['city'][859]='Denver'
jobs_df['state'][859]='CO'

jobs_df['city'][3199]='Annapolis'
jobs_df['state'][3199]='MD'

jobs_df['city'][3355]='Austin'
jobs_df['state'][3355]='TX'

jobs_df['city'][3462]='Denver'
jobs_df['state'][3462]='CO'

jobs_df['city'][3464]='Denver'
jobs_df['state'][3464]='CO'

jobs_df['city'][3466]='Denver'
jobs_df['state'][3466]='CO'

jobs_df['city'][3472]='Denver'
jobs_df['state'][3472]='CO'

# jobs_df['city'][]=''
# jobs_df['state'][]=''

#drop remaining missing state columns
jobs_df.dropna(inplace=True)
#reset index values to reflect actual number of rows
jobs_df.reset_index(inplace=True)
jobs_df

jobs_df.drop(columns=['level_0', 'index'], inplace=True)
jobs_df.columns

#Check for missing data now that we added some new cols
total_missing = jobs_df.isnull().sum().sort_values(ascending=False)
percent_missing = (jobs_df.isnull().sum()/jobs_df.isnull().count()).sort_values(ascending=False)
missing_data = pd.concat([total_missing, percent_missing], axis=1, keys=['Total', 'Percent'])
missing_data

"""# **Exploratory Data Analysis**

### **Descriptive Statistics**
"""

#view data types of each col
jobs_df.dtypes

#descriptive statistics of numeric cols
jobs_df[['rank', 'min_sal', 'max_sal', 'avg_sal']].describe().T

#amount of unique values per column
jobs_df.nunique()

"""### **Visualizations**

#### **Rank & Salary Distributions**
"""

#How are rank, min_salary, max_salary, & avg_sal distributed?
#rank
plt.figure(figsize=(12,8))
plt.hist(jobs_df['rank'], color='turquoise')
plt.tick_params(labelsize = 15)
plt.title('Distribution of Ranks', fontsize = 20)
plt.xlabel('Ranks', fontsize = 20)
plt.ylabel('Frequency of Ranks', fontsize = 20)
plt.show()

#min salary
plt.figure(figsize=(12,8))
plt.hist(jobs_df['min_sal'], color='orange')
plt.tick_params(labelsize = 15)
plt.title('Distribution of Min Salary Estimate', fontsize = 20)
plt.xlabel('Minimum Salaries ($)', fontsize = 20)
plt.ylabel('Frequency of Minimum Salaries', fontsize = 20)
plt.show()

#max salary
plt.figure(figsize=(12,8))
plt.hist(jobs_df['max_sal'], color='purple')
plt.tick_params(labelsize = 15)
plt.title('Distribution of Max Salary Estimate', fontsize = 20)
plt.xlabel('Maximum Salaries ($)', fontsize = 20)
plt.ylabel('Frequency of Maximum Salaries', fontsize = 20)
plt.show()

#avg salary
plt.figure(figsize=(12,8))
plt.hist(jobs_df['avg_sal'], color='blue')
plt.tick_params(labelsize = 15)
plt.title('Distribution of Average Salary Estimate', fontsize = 20)
plt.xlabel('Avg Salaries ($)', fontsize = 20)
plt.ylabel('Frequency of Average Salaries', fontsize = 20)
plt.show()

"""**Conclusion**

All numeric columns seem to have somewhat of a normal distribution. There may be some skewness to each but overall the bell shape is there.

#### **Salary Spread by Search City**
"""

#how are salaries spread per city?
plt.figure(figsize=(12,7))
sns.boxplot(x='avg_sal', y='search_city', data= jobs_df, palette = 'Set3')
plt.xlabel('Average Annual Salary ($)')
plt.title('Average Annual Salary Per City')
plt.show()

"""#### **Salary Spread by Search Job**"""

#how are salaries spread per job?
plt.figure(figsize=(12,5))
sns.boxplot(x='avg_sal', y='search_job', data= jobs_df, palette = 'Set3')
plt.xlabel('Average Annual Salary ($)')
plt.title('Average Annual Salary Per Job')
plt.show()

#how are average salaries spread?
plt.figure(figsize=(15,3))
sns.boxplot(x='avg_sal', data= jobs_df, palette = 'Set3')
plt.xlabel('Average Annual Salary ($)')
plt.title('Average Annual Salary')
plt.show()

"""#### **How many jobs from each search_job?**"""

#How many jobs from each search_job?
plt.figure(figsize=(10,7))
sns.countplot(y='search_job', palette='Set3',data=jobs_df,
  order=jobs_df['search_job'].value_counts().index)
plt.show()

"""#### **Most Popular Job Titles**"""

#Most Popular Job Titles
plt.figure(figsize=(10,7))
sns.countplot(y='job_title', palette='Set3',data=jobs_df,
  order=jobs_df['job_title'].value_counts().iloc[:21].index)
plt.show()

"""#### **Most Popular Search Cities**"""

#Most Popular Search City
plt.figure(figsize=(10,7))
sns.countplot(y='search_city', palette='Set3',data=jobs_df,
  order=jobs_df['search_city'].value_counts().index)
plt.show()

"""#### **Most Popular States**"""

#Most Popular State
plt.figure(figsize=(10,7))
sns.countplot(y='state', palette='Set3',data=jobs_df,
  order=jobs_df['state'].value_counts().index)
plt.show()

"""#### **Glassdoor Salary Estimates vs. Employer Salary Estimates**"""

#Estimate Types
plt.figure(figsize=(10,7))
sns.countplot(y='estimate_type', palette='Set3',data=jobs_df,
  order=jobs_df['estimate_type'].value_counts().index)
plt.show()

"""#### **Most Popular Companies**"""

#Most Popular Companies
plt.figure(figsize=(10,7))
sns.countplot(y='company', palette='Set3',data=jobs_df,
  order=jobs_df['company'].value_counts().iloc[:21].index)
plt.show()

"""#### **Most Popular Industries**"""

#Most Popular Industries
plt.figure(figsize=(10,7))
sns.countplot(y='industry', palette='Set3',data=jobs_df,
  order=jobs_df['industry'].value_counts().iloc[:21].index)
plt.show()

"""#### **Job Description Word Cloud**"""

#most popular words in job_description
# Generate a word cloud image
wordcloud = WordCloud(background_color='white').generate(" ".join(jobs_df['description']))
plt.figure(figsize=(20,15))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""**Conclusion**

While removal of common words is still neccessary, it seems removal or exclusion of the disclosures about race, gender, religion, disability, and veteran status may also be necessary.

### **Description Text Cleaning**
"""

#make a df of just job_titles & descriptions

#create a function to clean the job_titles
def job_title_cleaning(text):
  #lowercase all letters
  text = text.lower()
  #change sr. to senior
  text = re.sub('sr.', 'senior', text)
  return text

#reg expression that will apply cleaning
round0 = lambda x: job_title_cleaning(x)

#create the df & apply cleaning to job_title
desc_df = pd.DataFrame(jobs_df.job_title.apply(round0))

#add descriptions to the df
desc_df['description'] =jobs_df['description']

desc_df.head()

"""#### **Text Formatting**"""

#create a function for first round of description cleaning
def text_cleaning(text):
  #change all lowercase letters followed by uppercase letters to include a space in between
  text = re.sub('(?<=[a-z])(?=[A-Z])', ' \g<0>', text)
  #replace / with a space
  text = re.sub('/', ' ', text)
  #make all lowercase letters
  text = text.lower()
  #put a space between any numbers adjacent to a letter
  text = re.sub('(?<=[0-9])(?=[a-z])',' \g<0>', text)
  #remove  characters
  text = re.sub('', '', text)
  #convert + to plus
  text = re.sub('[+]', 'plus', text)
  #remove brackets
  text = re.sub('\[.*?\]', '', text)
  #replace: with a pace
  text = re.sub(':', ' ', text)
  #remove punctuation
  text = re.sub('[%s]' % re.escape(string.punctuation), '', text)
  #remove \n\n
  text = re.sub('\n\n', ' ', text)
  #remove quotation marks
  text = re.sub('[‘’“”…]', '', text)
  #change years to year
  text = re.sub('years', 'year', text)
  #change yrs to year
  text = re.sub('yrs', 'year', text)
  #change yr to year
  text = re.sub('yr', 'year', text)
  #chagnge experienceworking to experience
  text = re.sub('experienceworking', 'experience', text)
  #add a space after word degree
  text = re.sub('degree', 'degree ', text)
  #change ba to bachelors
  text = re.sub(' ba ', 'bachelors', text)
  #change bs to bachelors
  text = re.sub(' bs ', 'bachelors', text)
  #change ma to masters
  text = re.sub(' ma ', 'masters', text)
  #change ms to masters
  text = re.sub(' ms ', 'masters', text)
  #change doctoral to doctorate
  text = re.sub('doctoral', 'doctorate', text)
  #change doctorates to doctorate
  text = re.sub('doctorates', 'doctorate', text)
  #change phd to doctorate
  text = re.sub('phd', 'doctorate', text)
  #change artificial intelligence to ai
  text = re.sub('artificial intelligence', 'ai', text)
  #change natural language to nlp
  text = re.sub('natural language', 'nlp', text)
  #change nltk to nlp
  text = re.sub('nltk', 'nlp', text)
  #change machine learning to ml
  text = re.sub('machine learning', 'ml', text)
  #change mysql to sql
  text = re.sub('mysql', 'sql', text)
  #change postgresql to sql
  text = re.sub('postgresql', 'sql', text)
  #change ab testing to hypothesis testing
  text = re.sub('ab testing', 'hypothesis testing', text)
  #change dashboards to dashboard
  text = re.sub('dashboards', 'dashboard', text)
  #change dashboarding to dashboard
  text = re.sub('dashboarding', 'dashboard', text)
  #change handson analytic to nothing
  text = re.sub('handson analytic', '', text)
  #change data development to nothing
  text = re.sub('data development', '', text)
  #add space after word requirements
  text = re.sub('requirements', 'requirements ', text)
  return text

round1 = lambda x: text_cleaning(x)

#view updated descriptions
clean_desc = pd.DataFrame(desc_df.description.apply(round1))
clean_desc['job_title'] = desc_df['job_title']
clean_desc.head(20)

clean_desc.head(20)

"""#### **Excluding Stopwords**"""

#set stop_words
stop_words = nltk.corpus.stopwords.words('english')

#add some words to stop_words
new_stop_words = ['combined', 'related', 'working', 'overall', 'work', 'more', 
               'recent', 'professional', 'relevant', 'industry']

for word in new_stop_words:
  stop_words.append(word)

#filter out all stop words
cleaned_desc = pd.DataFrame(data = (clean_desc['description'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))))

cleaned_desc['job_title'] = clean_desc['job_title']
cleaned_desc.head()

"""#### **Most Popular Words WordCloud**"""

#most popular words in job_description
wordcloud = WordCloud(background_color='white').generate(" ".join(cleaned_desc['description']))
plt.figure(figsize=(20,15))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis("off")
plt.show()

"""**Conclusion**

The cleaned descriptions still contain several irrelevant words. Let's stop cleaning the text here and focus efforts on extracting pieces of information from the description with regular expressions.

### **Description Text Extraction**

#### **Text Extraction Function**
"""

#function to return which option(s) form a defined pattern a descrription contains
def pattern_searcher(search_str:str, search_list:str):
    search_obj = re.search(search_list, search_str)
    if search_obj :
        return_str = search_str[search_obj.start(): search_obj.end()]
    else:
        return_str = 'NA'
    return return_str

"""#### **Extract Years of Experience**"""

#make a column to see if description has certain degree

years_exp = [
'6 months experience'
'1 year experience',
'one year experience',
'1plus year experience',
'oneplus year experience',
'1 plus year experience',
'one plus year experience',
'one more year experience'
'1plus year',
'oneplus year',
'1 plus year',
'one plus year',
'one year',
'1 year',
'1plus experience',
'oneplus experience',
'1 plus experience',
'one plus experience',
'2 year experience',
'two year experience',
'2plus year experience',
'twoplus year experience',
'2 plus year experience',
'two plus year experience',
'two more year experience',
'twoplus year',
'2plus year',
'two plus year',
'2 plus year',
'two year',
'2 year',
'2plus experience',
'twoplus experience',
'2 plus experience',
'two plus experience',
'3 year experience',
'35 year experience',
'three year experience',
'3plus year experience',
'threeplus year experience',
'3 plus year experience',
'three plus year experience',
'three more year experience',
'3plus year',
'threeplus year',
'3 plus year',
'three plus year',
'three year',
'3 year',
'3plus experience',
'threeplus experience',
'3 plus experience',
'three plus experience',
'4 year experience',
'four year experience',
'4plus year experience',
'fourplus year experience',
'4 plus year experience',
'four plus year experience',
'four more year experience',
'4plus year',
'fourplus year',
'4 plus year',
'four plus year',
'four year',
'4 year',
'4plus experience',
'fourplus experience',
'4 plus experience',
'four plus experience',
'5 year experience',
'five year experience',
'5plus year experience',
'fiveplus year experience',
'5 plus year experience',
'five plus year experience',
'five more year experience',
'5plus year',
'fiveplus year',
'5 plus year',
'five plus year',
'five year',
'5 year',
'5plus experience',
'fiveplus experience',
'5 plus experience',
'five plus experience',
'57 experience',
'6 year experience',
'six year experience',
'6plus year',
'sixplus year',
'6 plus year',
'six plus year',
'6plus year experience',
'sixplus year experience',
'6 plus year experience',
'six plus year experience',
'six more year experience',
'six year',
'6 year',
'6plus experience',
'sixplus experience',
'6 plus experience',
'six plus experience',
'7 year experience',
'seven year experience',
'7plus year experience',
'sevenplus year experience',
'7 plus year experience',
'seven plus year experience',
'seven more year experience',
'710 year experience',
'7plus year',
'sevenplus year',
'7 plus year',
'seven plus year',
'seven year',
'7 year',
'7plus experience',
'sevenplus experience',
'7 plus experience',
'seven plus experience',
'8 year experience',
'eight year experience',
'8plus year experience',
'eightplus year experience',
'8 plus year experience',
'eight plus year experience',
'eight more year experience',
'8plus year',
'eightplus year',
'8 plus year',
'eight plus year',
'eight year',
'8 year',
'8plus experience',
'eightplus experience',
'8 plus experience',
'eight plus experience',
'9 year experience',
'nine year experience',
'9plus year experience',
'nineplus year experience',
'9 plus year experience',
'nine plus year experience',
'nine more year experience',
'9plus year',
'nineplus year',
'9 plus year',
'nine plus year',
'nine year',
'9 year',
'9plus experience',
'nineplus experience',
'9 plus experience',
'nine plus experience',
'10 year experience',
'ten year experience',
'10plus year experience',
'tenplus year experience',
'10 plus year experience',
'ten plus year experience',
'ten more year experience',
'10plus year',
'tenplus year',
'10 plus year',
'ten plus year',
'ten year',
'10 year',
'10plus experience',
'tenplus experience',
'10 plus experience',
'ten plus experience'
]

#create a pattern
exp_pattern = '|'.join([f'(?i){year}' for year in years_exp])

#make a new column in jobs_df to represent which degree(s) a post mentions
cleaned_desc['years_exp'] = cleaned_desc['description'].apply(lambda x: pattern_searcher(search_str=x, search_list=exp_pattern))
cleaned_desc

cleaned_desc['years_exp'][82] = '1plus year'

#check how many observations are NA
cleaned_desc_na = cleaned_desc['years_exp'] == 'NA'
cleaned_desc.iloc[cleaned_desc_na.values]

#change years_exp to one specific number or NA value
six_mos = ['6 months experience']

one_year = ['1 year experience','one year experience','1plus year experience',
'oneplus year experience','1 plus year experience','one plus year experience',
'one more year experience''1plus year','oneplus year','1 plus year',
'one plus year','one year','1 year','1plus experience','oneplus experience',
'1 plus experience','one plus experience']

two_years = ['2 year experience','two year experience','2plus year experience',
'twoplus year experience','2 plus year experience','two plus year experience',
'two more year experience','twoplus year','2plus year','two plus year',
'2 plus year','two year','2 year','2plus experience','twoplus experience',
'2 plus experience','two plus experience']

three_years = ['3 year experience','35 year experience','three year experience',
'3plus year experience','threeplus year experience','3 plus year experience',
'three plus year experience','three more year experience','3plus year',
'threeplus year','3 plus year','three plus year','three year','3 year',
'3plus experience','threeplus experience','3 plus experience',
'three plus experience']

four_years = ['4 year experience','four year experience','4plus year experience',
'fourplus year experience','4 plus year experience','four plus year experience',
'four more year experience','4plus year','fourplus year','4 plus year',
'four plus year','four year','4 year','4plus experience','fourplus experience',
'4 plus experience','four plus experience']

five_years = ['5 year experience','five year experience','5plus year experience',
'fiveplus year experience','5 plus year experience','five plus year experience',
'five more year experience','5plus year','fiveplus year','5 plus year',
'five plus year','five year','5 year','5plus experience','fiveplus experience',
'5 plus experience','five plus experience','57 experience']

six_years = ['6 year experience','six year experience','6plus year',
'sixplus year','6 plus year','six plus year','6plus year experience',
'sixplus year experience','6 plus year experience','six plus year experience',
'six more year experience','six year','6 year','6plus experience',
'sixplus experience','6 plus experience','six plus experience']

seven_years = ['7 year experience','seven year experience','7plus year experience',
'sevenplus year experience','7 plus year experience','seven plus year experience',
'seven more year experience','710 year experience','7plus year','sevenplus year',
'7 plus year','seven plus year','seven year','7 year','7plus experience',
'sevenplus experience','7 plus experience','seven plus experience']

eight_years = ['8 year experience','eight year experience',
'8plus year experience','eightplus year experience','8 plus year experience',
'eight plus year experience','eight more year experience','8plus year',
'eightplus year','8 plus year','eight plus year','eight year','8 year',
'8plus experience','eightplus experience','8 plus experience',
'eight plus experience']

nine_years = ['9 year experience','nine year experience','9plus year experience',
'nineplus year experience','9 plus year experience','nine plus year experience',
'nine more year experience','9plus year','nineplus year','9 plus year',
'nine plus year','nine year','9 year','9plus experience','nineplus experience',
'9 plus experience','nine plus experience']

ten_years = ['10 year experience','ten year experience','10plus year experience',
'tenplus year experience','10 plus year experience','ten plus year experience',
'ten more year experience','10plus year','tenplus year','10 plus year',
'ten plus year','ten year','10 year','10plus experience','tenplus experience',
'10 plus experience','ten plus experience']

cleaned_desc['years_exp'] = cleaned_desc['years_exp'].apply(lambda x:0.5 if x in six_mos else 1 if x in one_year else 2 if x in two_years else 3 if x in three_years else 4 if x in four_years else 5 if x in five_years else 6 if x in six_years else 7 if x in seven_years else 8 if x in eight_years else 9 if x in nine_years else 10 if x in ten_years else -1)
cleaned_desc

#create a df for nlp analysis in a separate notebook
# nlp_desc = pd.DataFrame()
# nlp_desc['description'] = cleaned_desc['description']
# nlp_desc['avg_sal'] = jobs_df['avg_sal']
# nlp_desc

"""**Note**

Since we found around 84% of the postings mentioning years of experience, analysis may contain model testing on one dataframe that includes years of experience and on others that would contain less data, without years of experience. Correlation of avg_sal & years of experience will tell us which route is appropriate.

#### **Extract Degrees**
"""

#make a column to see if description has certain degree
degrees = ['associates', 'bachelors', 'masters', 'doctorate']

for degree in degrees:
  cleaned_desc[degree] = clean_desc['description'].apply(lambda x: 1 if degree in x else 0)

cleaned_desc

"""#### **Extract Senior Job Titles**"""

senior_lead = ['senior', 'lead', 'principal', 'manager']
for item in senior_lead:
  cleaned_desc[item] = clean_desc['job_title'].apply(lambda x: 1 if item in x else 0)

cleaned_desc

#check if any of the senior job titles are all 0
cleaned_desc[['senior', 'lead', 'principal', 'manager']].nunique()

"""#### **Extract Languages**"""

languages = ['bash','cplusplus','html','java','javascript','julia','matlab',
'oracle','python',' r ','ruby','sas','scala','sql']
for language in languages:
  cleaned_desc[language] = clean_desc['description'].apply(lambda x: 1 if language in x else 0)

cleaned_desc

#check if any of the languages are all 0
cleaned_desc[['bash','cplusplus','html','java','javascript','julia','matlab',
'oracle','python',' r ','ruby','sas','scala','sql']].nunique()

"""#### **Extract Coding Interfaces**"""

code_interfraces = ['colab','command line','git ','github','jupyter','notebook']
for item in code_interfraces:
  cleaned_desc[item] = clean_desc['description'].apply(lambda x: 1 if item in x else 0)

cleaned_desc

#check if any of the code interfraces are all 0
cleaned_desc[['colab','command line','git ','github','jupyter','notebook']].nunique()

#drop colab from df
cleaned_desc.drop(columns='colab', inplace=True)

"""#### **Extract Visualization Platforms**"""

dashboards = ['dashboard','data studio','looker','power bi','sisense','tableau']
for dashboard in dashboards:
  cleaned_desc[dashboard] = clean_desc['description'].apply(lambda x: 1 if dashboard in x else 0)

cleaned_desc

#check if any of the dashboards are all 0
cleaned_desc[['dashboard','data studio','looker','power bi','sisense','tableau']].nunique()

"""#### **Extract Big Data Skills**"""

big_data = ['airflow','big data','bigquery','cassandra','docker','etl','hadoop',
'hbase','hdfs','hive','impala','kafka','kubeflow','kubernetes']
for tool in big_data:
  cleaned_desc[tool] = clean_desc['description'].apply(lambda x: 1 if tool in x else 0)

cleaned_desc

#check if any of the big data tools had only 1 unique value
cleaned_desc[['airflow','big data','bigquery','cassandra','docker','etl','hadoop',
'hbase','hdfs','hive','impala','kafka','kubeflow','kubernetes']].nunique()

"""#### **Extract Cloud Skills**"""

clouds = ['aws','azure','dynamodb','ec2','gcp','google cloud','lambda','mongodb',
'redshift','s3','saas','snowflake']
for cloud in clouds:
  cleaned_desc[cloud] = clean_desc['description'].apply(lambda x: 1 if cloud in x else 0)

cleaned_desc

#check if any of the cloud tools are all 0
cleaned_desc[['aws','azure','dynamodb','ec2','gcp','google cloud','lambda','mongodb',
'redshift','s3','saas','snowflake']].nunique()

"""#### **Extract Outside Platform Tools**"""

platforms = ['google analytics','google sheets','linux','microsoft', 'microsoft excel',
'powerpoint','salesforce']
for item in platforms:
  cleaned_desc[item] = clean_desc['description'].apply(lambda x: 1 if item in x else 0)

cleaned_desc

#check if any of the outside platform tools are all 0
cleaned_desc[['google analytics','google sheets','linux','microsoft', 'microsoft excel',
'powerpoint','salesforce']].nunique()

"""#### **Extract Data Science Concepts**"""

concepts = [' ai ','ajax','api','etl','forecasting','hypothesis testing',
'json','kpi',' ml ','nlp','pipeline','predictive','regression','statistical',
'time series']
for item in concepts:
  cleaned_desc[item] = clean_desc['description'].apply(lambda x: 1 if item in x else 0)

cleaned_desc

#check if any of the ds concepts are all 0
cleaned_desc[[' ai ','ajax','api','etl','forecasting','hypothesis testing',
'json','kpi',' ml ','nlp','pipeline','predictive','regression','statistical',
'time series']].nunique()

"""#### **Extract Libraries**"""

libraries = ['matplotlib','numpy','pandas','scikit','seaborn']
for item in libraries:
  cleaned_desc[item] = clean_desc['description'].apply(lambda x: 1 if item in x else 0)

cleaned_desc

#check if any of the libraries are all 0
cleaned_desc[['matplotlib','numpy','pandas','scikit','seaborn']].nunique()

"""#### **Extract Deep Learning Techniques**"""

deep_learning = ['apache','keras','neural','spark','tensorflow']
for item in deep_learning:
  cleaned_desc[item] = clean_desc['description'].apply(lambda x: 1 if item in x else 0)

cleaned_desc

#check if any of the deeep learning techniques are all 0
cleaned_desc[['apache','keras','neural','spark','tensorflow']].nunique()

"""#### **Extract Web Application Platforms**"""

web_apps = ['bottle','cherrypy','django','flask','pyramid','quixote',
            'turbogears','web2py']
for item in web_apps:
  cleaned_desc[item] = clean_desc['description'].apply(lambda x: 1 if item in x else 0)

cleaned_desc

#check if any of the web application platforms are all 0
cleaned_desc[['bottle','cherrypy','django','flask','pyramid','quixote',
            'turbogears','web2py']].nunique()

cleaned_desc.drop(columns=['cherrypy','pyramid','quixote',
            'turbogears','web2py'], inplace=True)

"""### **Skills Visualizations**

#### **Most Popular Years of Experience**
"""

plt.figure(figsize=(10,7))
sns.countplot(y='years_exp', palette='Set3',data=cleaned_desc,
  order=cleaned_desc['years_exp'].value_counts().index)
plt.show()

"""**Interpretation**

Most job postings did not define years of experience, or we didn't account for them accurattely. However if years of experience were detected, it was most commonly 5 or 3 years of experience.

#### **Most Popular Degrees**
"""

#create a sum of each degree column
associates_sum = sum(cleaned_desc['associates'])
bachelors_sum = sum(cleaned_desc['bachelors'])
masters_sum = sum(cleaned_desc['masters'])
doctorate_sum = sum(cleaned_desc['doctorate'])

#make a dictionary of degree sums so we can plot visualization
degree_sums = {'associates':associates_sum, 'bachelors': bachelors_sum, 
               'masters': masters_sum, 'doctorate': doctorate_sum}

plt.figure(figsize=(10,7))
plt.bar(range(len(degree_sums)), list(degree_sums.values()), align='center', color='maroon')
plt.xticks(range(len(degree_sums)), list(degree_sums.keys()))
plt.ylabel('Frequency of Degree')
plt.title('Frequency of Degrees Listed in Job Postings')
plt.xlabel('Language')
plt.show()

"""**Interpretation**

If a degree is mentioned it's most often a bachelors or masters degree.

#### **How Many Senior Job Titles?**
"""

#create a sum of each visualization column
senior_sum = sum(cleaned_desc['senior'])
lead_sum = sum(cleaned_desc['lead'])
principal_sum = sum(cleaned_desc['principal'])
manager_sum = sum(cleaned_desc['manager'])

#make a dictionary of visualization sums so we can plot visualization
title_sums = {'senior':senior_sum, 'lead':lead_sum, 'principal':principal_sum,
              'manager':manager_sum}

plt.figure(figsize=(10,7))
plt.bar(range(len(title_sums)), list(title_sums.values()), align='center', color='teal')
plt.xticks(range(len(title_sums)), list(title_sums.keys()))
plt.ylabel('Frequency of Senior Job Title')
plt.title('Frequency of Senior Job Titles Listed in Job Postings')
plt.xlabel('Senior Job Title')
plt.show()

"""**Interpretation**

About 20% of jobs had senior in the title and much fewer had lead, principal, or manager.

#### **Most Popular Languages**
"""

#create a sum of each language column
bash_sum = sum(cleaned_desc['bash'])
cplusplus_sum = sum(cleaned_desc['cplusplus'])
html_sum = sum(cleaned_desc['html'])
java_sum = sum(cleaned_desc['java'])
javascript_sum = sum(cleaned_desc['javascript'])
julia_sum = sum(cleaned_desc['julia'])
matlab_sum = sum(cleaned_desc['matlab'])
oracle_sum = sum(cleaned_desc['oracle'])
python_sum = sum(cleaned_desc['python'])
r_sum = sum(cleaned_desc[' r '])
ruby_sum = sum(cleaned_desc['ruby'])
sas_sum = sum(cleaned_desc['sas'])
scala_sum = sum(cleaned_desc['scala'])
sql_sum = sum(cleaned_desc['sql'])

#make a dictionary of language sums so we can plot visualization
language_sums = {'bash':bash_sum,'cplusplus': cplusplus_sum,'html':html_sum,
'java':java_sum,'javascript':javascript_sum,'julia':julia_sum,'matlab':matlab_sum,
'oracle': oracle_sum,'python':python_sum,'r':r_sum, 'ruby':ruby_sum, 
'sas':sas_sum,'scala':scala_sum,'sql':sql_sum}

plt.figure(figsize=(12,7))
plt.bar(range(len(language_sums)), list(language_sums.values()), align='center', color='orange')
plt.xticks(range(len(language_sums)), list(language_sums.keys()))
plt.ylabel('Frequency of Language')
plt.title('Frequency of Languages Listed in Job Postings')
plt.xlabel('Language')
plt.show()

"""**Interpretation**

The most important languages to have are sql and python but java, r, sas, and scala were mentioned somewhat frequently as well.

#### **Most Popular Coding Interfaces**
"""

#create a sum of each coding interface column
command_line_sum = sum(cleaned_desc['command line'])
git_sum = sum(cleaned_desc['git '])
github_sum = sum(cleaned_desc['github'])
jupyter_sum = sum(cleaned_desc['jupyter'])
notebook_sum = sum(cleaned_desc['notebook'])

#make a dictionary of coding interface sums so we can plot visualization
vis_sums = {'command_line':command_line_sum, 'git':git_sum, 
'github': github_sum, 'jupyter': jupyter_sum,'notebook': notebook_sum}

plt.figure(figsize=(10,7))
plt.bar(range(len(vis_sums)), list(vis_sums.values()), align='center', color='turquoise')
plt.xticks(range(len(vis_sums)), list(vis_sums.keys()))
plt.ylabel('Frequency of Coding Interface Tool')
plt.title('Frequency of Coding Interface Tools Listed in Job Postings')
plt.xlabel('Coding Interface Tool')
plt.show()

"""**Interpretation**

Coding interfaces are mentioned infrequently.

#### **Most Popular Visualization Tools**
"""

#create a sum of each visualization column
dashboard_sum = sum(cleaned_desc['dashboard'])
data_studio_sum = sum(cleaned_desc['data studio'])
looker_sum = sum(cleaned_desc['looker'])
power_bi_sum = sum(cleaned_desc['power bi'])
sisense_sum = sum(cleaned_desc['sisense'])
tableau_sum = sum(cleaned_desc['tableau'])

#make a dictionary of visualization sums so we can plot visualization
vis_sums = {'dashboard': dashboard_sum,'data_studio': data_studio_sum,
'looker': looker_sum,'power_bi': power_bi_sum,'sisense': sisense_sum,
'tableau': tableau_sum}

plt.figure(figsize=(10,7))
plt.bar(range(len(vis_sums)), list(vis_sums.values()), align='center', color='turquoise')
plt.xticks(range(len(vis_sums)), list(vis_sums.keys()))
plt.ylabel('Frequency of Visualization Tool')
plt.title('Frequency of Visualization Tools Listed in Job Postings')
plt.xlabel('Visualization Tool')
plt.show()

"""**Interpretation**

If a dashboarding tool is mentioned, tableau is most often the tool of choice followed by dashboard in general.

#### **Most Popular Big Data Skills**
"""

#create a sum of each big data column
airflow_sum = sum(cleaned_desc['airflow'])
big_data_sum = sum(cleaned_desc['big data'])
bigquery_sum = sum(cleaned_desc['bigquery'])
cassandra_sum = sum(cleaned_desc['cassandra'])
docker_sum = sum(cleaned_desc['docker'])
etl_sum = sum(cleaned_desc['etl'])
hadoop_sum = sum(cleaned_desc['hadoop'])
hbase_sum = sum(cleaned_desc['hbase'])
hdfs_sum = sum(cleaned_desc['hdfs']) 
hive_sum = sum(cleaned_desc['hive']) 
impala_sum = sum(cleaned_desc['impala'])
kafka_sum = sum(cleaned_desc['kafka'])
kubeflow_sum = sum(cleaned_desc['kubeflow'])
kubernetes_sum = sum(cleaned_desc['kubernetes'])

#make a dictionary of big data sums so we can plot visualization
big_data_sums = {'airflow': airflow_sum,'big_data': big_data_sum,
'bigquery': bigquery_sum,'cassandra': cassandra_sum, 'docer':docker_sum,'etl': etl_sum,
'hadoop': hadoop_sum,'hbase': hbase_sum,'hdfs': hdfs_sum,'hive': hive_sum,
'impala': impala_sum,'kafka': kafka_sum,'kubeflow': kubeflow_sum,
'kubernetes': kubernetes_sum}

plt.figure(figsize=(20,7))
plt.bar(range(len(big_data_sums)), list(big_data_sums.values()), align='center', color='blue')
plt.xticks(range(len(big_data_sums)), list(big_data_sums.keys()))
plt.ylabel('Frequency of Big Data Tool')
plt.title('Frequency of Big Data Tools Listed in Job Postings')
plt.xlabel('Big Data Tool')
plt.show()

"""**Interpretation**

Big data skills aren't mentioned in most jobs but if they are, big data and etl are in general is mentioned, and most popular specific tool is hadoop.

#### **Most Popular Cloud Skills**
"""

#create a sum of each cloud column
aws_sum = sum(cleaned_desc['aws'])
azure_sum = sum(cleaned_desc['azure'])
dynamodb_sum = sum(cleaned_desc['dynamodb'])
ec2_sum = sum(cleaned_desc['ec2'])
gcp_sum = sum(cleaned_desc['gcp'])
google_cloud_sum = sum(cleaned_desc['google cloud'])
lambda_sum = sum(cleaned_desc['lambda']) 
mongodb_sum = sum(cleaned_desc['mongodb'])
redshift_sum = sum(cleaned_desc['redshift'])
s3_sum = sum(cleaned_desc['s3'])
saas_sum = sum(cleaned_desc['saas'])
snowflake_sum = sum(cleaned_desc['snowflake'])

#make a dictionary of cloud sums so we can plot visualization
cloud_sums = {'aws': aws_sum,'azure': azure_sum,'dynamodb': dynamodb_sum,
'ec2': ec2_sum,'gcp': gcp_sum,'google_cloud': google_cloud_sum,
'lambda': lambda_sum,'mongodb': mongodb_sum,'redshift': redshift_sum,
's3': s3_sum, 'saas': saas_sum, 'snowflake':snowflake_sum}

plt.figure(figsize=(20,7))
plt.bar(range(len(cloud_sums)), list(cloud_sums.values()), align='center', color='indigo')
plt.xticks(range(len(cloud_sums)), list(cloud_sums.keys()))
plt.ylabel('Frequency of Cloud Tool')
plt.title('Frequency of Cloud Tools Listed in Job Postings')
plt.xlabel('Cloud Tool')
plt.show()

"""**Interpretation**

The most frequently mentioned cloud platform is aws.

#### **Most Popular Outside Platform Tools**
"""

#create a sum of each outside platform tool
google_analytics_sum = sum(cleaned_desc['google analytics'])
google_sheets_sum = sum(cleaned_desc['google sheets'])
linux_sum = sum(cleaned_desc['linux'])
microsoft_sum = sum(cleaned_desc['microsoft'])
microsoft_excel_sum = sum(cleaned_desc['microsoft excel'])
powerpoint_sum = sum(cleaned_desc['powerpoint'])
salesforce_sum = sum(cleaned_desc['salesforce'])

#make a dictionary of miscellaneous outside platform sums so we can plot visualization
platform_sums = {'google_analytics':google_analytics_sum,
'google_sheets':google_sheets_sum,'linux':linux_sum,'microsoft':microsoft_sum,
'microsoft_excel':microsoft_excel_sum, 'powerpoint':powerpoint_sum,
'salesforce':salesforce_sum}

plt.figure(figsize=(12,7))
plt.bar(range(len(platform_sums)), list(platform_sums.values()), align='center', color='violet')
plt.xticks(range(len(platform_sums)), list(platform_sums.keys()), rotation = 90, fontsize=12)
plt.yticks(fontsize = 12)
plt.ylabel('Frequency of Outside Platform', fontsize=15)
plt.title('Frequency of Outside Platform Tools Listed in Job Postings',fontsize=15)
plt.xlabel('Outside Platform Tool', fontsize=15)
plt.show()

"""**Interpretation**

The most frequently mentioned outside platform is microsoft.

#### **Most Popular Data Science Concepts**
"""

#create a sum of each ds concept
ai_sum = sum(cleaned_desc[' ai '])
ajax_sum = sum(cleaned_desc['ajax'])
api_sum = sum(cleaned_desc['api'])
etl_sum = sum(cleaned_desc['etl'])
forecasting_sum = sum(cleaned_desc['forecasting'])
hypothesis_testing_sum = sum(cleaned_desc['hypothesis testing'])
json_sum = sum(cleaned_desc['json'])
kpi_sum = sum(cleaned_desc['kpi'])
ml_sum = sum(cleaned_desc[' ml '])
nlp_sum = sum(cleaned_desc['nlp'])
pipeline_sum = sum(cleaned_desc['pipeline'])
predictive_sum = sum(cleaned_desc['predictive'])
regression_sum = sum(cleaned_desc['regression'])
statistical_sum = sum(cleaned_desc['statistical'])
time_series_sum = sum(cleaned_desc['time series'])

#make a dictionary of ds concept sums so we can plot visualization
platform_sums = {'ai':ai_sum,'ajax':ajax_sum,'api':api_sum,'etl':etl_sum,
'forecasting': forecasting_sum,'hypothesis_testing': hypothesis_testing_sum,
'json': json_sum,'kpi': kpi_sum,'ml': ml_sum,'nlp': nlp_sum,'pipeline': pipeline_sum,
'predictive': predictive_sum,'regression': regression_sum,
'statistical': statistical_sum,'time_series': time_series_sum}

plt.figure(figsize=(24, 12))
plt.bar(range(len(platform_sums)), list(platform_sums.values()), align='center', color='peachpuff')
plt.xticks(range(len(platform_sums)), list(platform_sums.keys()), rotation = 90, fontsize=12)
plt.yticks(fontsize = 12)
plt.ylabel('Frequency of Outside Platform', fontsize=15)
plt.title('Frequency of Outside Platform Tools Listed in Job Postings',fontsize=15)
plt.xlabel('Outside Platform Tool', fontsize=15)
plt.show()

"""**Interpretation**

The most frequently mentioned skills are api, etl, ml (machine learning), pipeline, predictive, and statistics.

#### **Most Popular Libraries**
"""

#create a sum of each library
matplotlib_sum = sum(cleaned_desc['matplotlib'])
numpy_sum = sum(cleaned_desc['numpy'])
pandas_sum = sum(cleaned_desc['pandas'])
scikit_sum = sum(cleaned_desc['scikit'])
seaborn_sum = sum(cleaned_desc['seaborn'])

#make a dictionary of libraries so we can plot visualization
library_sums = {'matplotlib':matplotlib_sum,'numpy':numpy_sum,
'pandas': pandas_sum,'scikit': scikit_sum,'seaborn': seaborn_sum}

plt.figure(figsize=(12,7))
plt.bar(range(len(library_sums)), list(library_sums.values()), align='center', color='crimson')
plt.xticks(range(len(library_sums)), list(library_sums.keys()), rotation = 90, fontsize=12)
plt.yticks(fontsize = 12)
plt.ylabel('Frequency of Libraries', fontsize=15)
plt.title('Frequency of Libraries Listed in Job Postings',fontsize=12)
plt.xlabel('Libraries', fontsize=15)
plt.show()

"""**Interpretation**

The most frequently mentioned libraries are pandas and scikit. However it's a very small amount of job postings that mentioned libraries.

#### **Most Popular Deep Learning Techniques**
"""

#create a sum of each deep learning technique
apache_sum = sum(cleaned_desc['apache'])
keras_sum = sum(cleaned_desc['keras'])
neural_sum = sum(cleaned_desc['neural'])
spark_sum = sum(cleaned_desc['spark'])
tensorflow_sum = sum(cleaned_desc['tensorflow'])

#make a dictionary of deep learning techniques so we can plot visualization
deep_learning_sums = {'apache': apache_sum,'keras':keras_sum,'neural': neural_sum,
'spark':spark_sum,'tensorflow': tensorflow_sum}

plt.figure(figsize=(12,7))
plt.bar(range(len(deep_learning_sums)), list(deep_learning_sums.values()), align='center', color='tomato')
plt.xticks(range(len(deep_learning_sums)), list(deep_learning_sums.keys()), rotation = 90, fontsize=12)
plt.yticks(fontsize = 12)
plt.ylabel('Frequency of Deep Learning Technique', fontsize=15)
plt.title('Frequency of Deep Learning Techniques Listed in Job Postings',fontsize=15)
plt.xlabel('Deep Learning Technique', fontsize=15)
plt.show()

"""**Interpretation**

The most frequently mentioned deep learning technique is spark.

#### **Most Popular Web Application Platforms**
"""

#create a sum of each web application platform
bottle_sum = sum(cleaned_desc['bottle'])
django_sum = sum(cleaned_desc['django'])
flask_sum = sum(cleaned_desc['flask'])

#make a dictionary of web apps so we can plot visualization
web_app_sums = {'bottle':bottle_sum,'django':django_sum,'flask':flask_sum}

plt.figure(figsize=(12,7))
plt.bar(range(len(web_app_sums)), list(web_app_sums.values()), align='center', color='limegreen')
plt.xticks(range(len(web_app_sums)), list(web_app_sums.keys()), rotation = 90, fontsize=12)
plt.yticks(fontsize = 12)
plt.ylabel('Frequency of Web App Platform', fontsize=15)
plt.title('Frequency of Web App Platforms Listed in Job Postings',fontsize=15)
plt.xlabel('Web App Platform', fontsize=15)
plt.show()

"""**Interpretation**

Web application platforms were rarely mentioned.

### **Dataframe Building**

#### **DF for All Numeric Data**
"""

#list of columns to add to jobs_df. Purposefully exclude coding platforms,
#libraries, and web application platforms since they weren't mentioned often
add_cols = ['years_exp', 'associates', 'bachelors',
       'masters', 'doctorate', 'senior', 'lead', 'principal', 'manager',
       'bash', 'cplusplus', 'html', 'java', 'javascript', 'julia', 'matlab',
       'oracle', 'python', ' r ', 'ruby', 'sas', 'scala', 'sql', 'dashboard',
       'data studio', 'looker', 'power bi', 'sisense', 'tableau', 'airflow',
       'big data', 'bigquery', 'cassandra', 'docker', 'etl', 'hadoop', 'hbase',
       'hdfs', 'hive', 'impala', 'kafka', 'kubeflow', 'kubernetes', 'aws',
       'azure', 'dynamodb', 'ec2', 'gcp', 'google cloud', 'lambda', 'mongodb',
       'redshift', 's3', 'saas', 'snowflake', 'google analytics',
       'google sheets', 'linux', 'microsoft', 'microsoft excel', 'powerpoint',
       'salesforce', ' ai ', 'ajax', 'api', 'forecasting',
       'hypothesis testing', 'json', 'kpi', ' ml ', 'nlp', 'pipeline',
       'predictive', 'regression', 'statistical', 'time series', 'matplotlib',
       'numpy', 'pandas', 'scikit', 'seaborn', 'apache', 'keras', 'neural',
       'spark', 'tensorflow']
#add numeric skill data to jobs_df
for col in add_cols:
  jobs_df[col] = cleaned_desc[col]

jobs_df

#create a df of only numeric data to be evaluated
all_jobs_df = jobs_df[['industry', 'rank',
       'search_city', 'search_job', 'min_sal', 'max_sal',
       'avg_sal', 'years_exp', 'associates', 'bachelors',
       'masters', 'doctorate', 'senior', 'lead', 'principal', 'manager',
       'bash', 'cplusplus', 'html', 'java', 'javascript', 'julia', 'matlab',
       'oracle', 'python', ' r ', 'ruby', 'sas', 'scala', 'sql', 'dashboard',
       'data studio', 'looker', 'power bi', 'sisense', 'tableau', 'airflow',
       'big data', 'bigquery', 'cassandra', 'docker', 'etl', 'hadoop', 'hbase',
       'hdfs', 'hive', 'impala', 'kafka', 'kubeflow', 'kubernetes', 'aws',
       'azure', 'dynamodb', 'ec2', 'gcp', 'google cloud', 'lambda', 'mongodb',
       'redshift', 's3', 'saas', 'snowflake', 'google analytics',
       'google sheets', 'linux', 'microsoft', 'microsoft excel', 'powerpoint',
       'salesforce', ' ai ', 'ajax', 'api', 'forecasting',
       'hypothesis testing', 'json', 'kpi', ' ml ', 'nlp', 'pipeline',
       'predictive', 'regression', 'statistical', 'time series', 'apache', 
       'keras', 'neural','spark', 'tensorflow']]
all_jobs_df['industry'] = all_jobs_df['industry'].str.lower()
all_jobs_df

"""##### **Label Encode Industry, Search City, & Search Job into Numeric Data**"""

le=LabelEncoder()
all_jobs_df['industry']=le.fit_transform(all_jobs_df['industry'])
all_jobs_df['search_city']=le.fit_transform(all_jobs_df['search_city'])
all_jobs_df['search_job']=le.fit_transform(all_jobs_df['search_job'])

all_jobs_df

"""##### **Correlations**

###### **Heatmap**
"""

#investigate correlations between avg_sal and remaining varaiables
plt.figure(figsize=(10, 45))
sns.heatmap(pd.DataFrame(pd.DataFrame(all_jobs_df.corr()).avg_sal), annot=True, cmap='viridis')
sns.set(font_scale=1)
plt.title('All Jobs Heatmap')
plt.yticks()
plt.xticks()
plt.show()

"""###### **Top 20 Correlations**"""

corr= pd.DataFrame(all_jobs_df.corr())
corr['avg_sal'].sort_values(ascending=False)[:20]

corr['avg_sal'].sort_values(ascending=False)[-20:]

"""###### **Interpretation**

Logically, it makes sense that min_sal & max_sal are highly correlated to avg_sal since they were the columns combined to make avg_sal. Since they are so highly correlated, it's best practice to exclude them from any models tested.

It's interesting that ds, data science search, is more positively correlated to avg_sal than the other job searches. Moreover it's intereting that data science & data engineering were positiviely correlated while data analyst and business analyst were negatively correlated.

#### **DF's with Simplified Category Values**

With intentions of building an app for model deoployment, expecting a user to enter yes/no for all of the generated features is unrealistic. Let's create some df's that give one score for each of the categories we crated previously.
"""

#create a new df
categories_df = pd.DataFrame()

#import certain cols from all_jobs_df
cols_to_add = ['industry', 'rank', 'search_city', 'search_job',
               'avg_sal', 'years_exp']

#add colsdata to categories
for col in cols_to_add:
  categories_df[col] = all_jobs_df[col]

categories_df

#create a feature for degrees
categories_df['degrees_total'] = all_jobs_df.iloc[:,8:12].sum(axis=1)

#create a feature for senior job titles
categories_df['sen_job_titles_total'] = all_jobs_df.iloc[:, 12:16].sum(axis=1)

#create a feature for languages
categories_df['languages_total'] = all_jobs_df.iloc[:, 16:30].sum(axis=1)

#create a feature for visualization totals
categories_df['visualization_total'] = all_jobs_df.iloc[:, 30:36].sum(axis=1)

#create a feature for big data tools
categories_df['big_data_total'] = all_jobs_df.iloc[:, 36:50].sum(axis=1)

#create a feature for cloud tools
categories_df['cloud_total'] = all_jobs_df.iloc[:, 50:62].sum(axis=1)

#create a feature for platform tools
categories_df['platform_total'] = all_jobs_df.iloc[:, 62:69].sum(axis=1)

#create a feature for data science concepts
categories_df['ds_concepts_total'] = all_jobs_df.iloc[:, 69:83].sum(axis=1)

#create a feature for deep learning concepts
categories_df['deep_learning_totals'] = all_jobs_df.iloc[:, 83:88].sum(axis=1)

#create a feature for log of avg_sal, will be tested for performance in models
categories_df['log_avg_sal'] = np.log1p(categories_df['avg_sal'])

categories_df

"""#### **DF for Jobs With Years of Experience**"""

yrs_exp = categories_df['years_exp'] != -1
yrs_exp_df = categories_df.iloc[yrs_exp.values]
yrs_exp_df

"""#### **DF for Categories Outliers Removed**

Since we know our target variable isn't perfectly normally distributed, we can create another dataframe for model testing that drops the outliers.

We will keep the 1.5 times +/- interquartile range salaries to see if that helps model performance later on. 
"""

#calculate interquartile range of avg_sal
cat_q25, cat_q75 = np.percentile(categories_df['avg_sal'], 25),np.percentile(categories_df['avg_sal'], 75)
cat_iqr = cat_q75 - cat_q25

#calculate the outlier cutoff
cat_cut_off = cat_iqr * 1.5

#calculate upper and lower outlier limits
cat_lower, cat_upper = cat_q25 - cat_cut_off, cat_q75 + cat_cut_off

#select all salaries in between the lower and upper limits
cat_in_range = (categories_df['avg_sal'] > cat_lower) & (categories_df['avg_sal'] < cat_upper)

#create a new df with the salaries that are in between the lower and upper limites
cat_no_outliers_df = categories_df.iloc[cat_in_range.values]

cat_no_outliers_df

cat_shape_diff = categories_df.shape[0] - cat_no_outliers_df.shape[0]
print('The categories dataframe had {} outliers dropped.'.format(cat_shape_diff))

"""#### **DF for Years of Experience Outliers Removed**

Since we know our target variable isn't perfectly normally distributed, we can create another dataframe for model testing that drops the outliers.

We will keep the 1.5 times +/- interquartile range salaries to see if that helps model performance later on. 
"""

#calculate interquartile range of avg_sal
yrs_q25, yrs_q75 = np.percentile(yrs_exp_df['avg_sal'], 25),np.percentile(yrs_exp_df['avg_sal'], 75)
yrs_iqr = yrs_q75 - yrs_q25

#calculate the outlier cutoff
yrs_cut_off = yrs_iqr * 1.5

#calculate upper and lower outlier limits
yrs_lower, yrs_upper = yrs_q25 - yrs_cut_off, yrs_q75 + yrs_cut_off

#select all salaries in between the lower and upper limits
yrs_in_range = (yrs_exp_df['avg_sal'] > yrs_lower) & (yrs_exp_df['avg_sal'] < yrs_upper)

#create a new df with the salaries that are in between the lower and upper limites
yrs_exp_no_outliers_df = yrs_exp_df.iloc[yrs_in_range.values]

yrs_exp_no_outliers_df

yrs_exp_shape_diff = yrs_exp_df.shape[0] - yrs_exp_no_outliers_df.shape[0]
print('The yrs_exp dataframe had {} outliers dropped.'.format(cat_shape_diff))

"""#### **Correlations**

##### **Heatmaps**
"""

#investigate correlations between avg_sal and remaining varaiables

#categories_df correlations
plt.figure(figsize=(24,1.5))
sns.heatmap(pd.DataFrame(pd.DataFrame(categories_df.corr()).avg_sal).T, annot=True, cmap='viridis')
sns.set(font_scale=1)
plt.yticks(rotation=0)
plt.xticks(rotation=45)
plt.title('Categories All Jobs Heatmap')

#yrs_exp_df correlations
plt.figure(figsize=(24,1.5))
sns.heatmap(pd.DataFrame(pd.DataFrame(yrs_exp_df.corr()).avg_sal).T, annot=True, cmap='viridis')
sns.set(font_scale=1)
plt.yticks(rotation=0)
plt.xticks(rotation=45)
plt.title('Categories for Jobs with Years of Experience Heatmap')
plt.show()

#cat_no_outliers_df correlations
plt.figure(figsize=(24,1.5))
sns.heatmap(pd.DataFrame(pd.DataFrame(cat_no_outliers_df.corr()).avg_sal).T, annot=True, cmap='viridis')
sns.set(font_scale=1)
plt.yticks(rotation=0)
plt.xticks(rotation=45)
plt.title('Categories w/o Outliers Jobs Heatmap')

#yrs_exp_no_outliers_df correlations
plt.figure(figsize=(24,1.5))
sns.heatmap(pd.DataFrame(pd.DataFrame(yrs_exp_no_outliers_df.corr()).avg_sal).T, annot=True, cmap='viridis')
sns.set(font_scale=1)
plt.yticks(rotation=0)
plt.xticks(rotation=45)
plt.title('Categories for Jobs with Years of Experience w/o Outliers Heatmap')
plt.show()

"""##### **Ordered Correlations**"""

#categories_df correlations
cat_corr= pd.DataFrame(categories_df.corr())
cat_corr_sorted = cat_corr['avg_sal'].sort_values(ascending=False)
print('***Categories Ordered Correlations***')
print(cat_corr_sorted)
print()

#yrs_experience correlations
yrs_corr= pd.DataFrame(yrs_exp_df.corr())
yrs_corr_sorted = yrs_corr['avg_sal'].sort_values(ascending=False)
print('***Years Experience Ordered Correlations***')
print(yrs_corr_sorted)

#cat_no_outliers_df correlations
cat_no_outliers_corr= pd.DataFrame(cat_no_outliers_df.corr())
cat_no_outliers_corr_sorted = cat_no_outliers_corr['avg_sal'].sort_values(ascending=False)
print('***Categories w/o Outliers Ordered Correlations***')
print(cat_no_outliers_corr_sorted)
print()

#yrs_exp_no_outliers_df correlations
yrs_no_outliers_corr= pd.DataFrame(yrs_exp_no_outliers_df.corr())
yrs_no_outliers_corr_sorted = yrs_no_outliers_corr['avg_sal'].sort_values(ascending=False)
print('***Years Experience w/o Outliers Ordered Correlations***')
print(yrs_no_outliers_corr_sorted)

"""##### **Interpretation**

**Categories Ordered Heatmap**

Now that our categories are combined into sums, it's interesting that the search_job is the highest positively correlated feature when before two of the jobs were positively correlated and two slightly negatively correlated. 

**Comparison of Categories vs Yrs Experience**

When only jobs that had years of experience were considered, the first two highly correlated features, search_jobs and sen_job_title, were the same in order but features were different from there. Features correlation coefficients were similar in each but order changed.

Years of Experience had nearly twice the positive correlation with avg_sal than it did in the categories only df.

## **Hypothesis Testing**

### **Target Variable**

#### **Is average salary variable normally distributed?**

*  $H_o:$ Distribution is Normal
*   $H_a:$ Distribution is Not Normal
"""

#Is the avg_salary distribution normal?
t,p=stats.shapiro(yrs_exp_no_outliers_df['avg_sal'])

#set up text wrapper for explanation of t-test because the explanation is long
wrapper = textwrap.TextWrapper(width=80)

#explanation of t-testing
if np.round(p, decimals=4) < 0.05:
  ttest_less = 'The p-value of {} is less than 0.05, so we reject the Null Hypothesis. There is sufficient evidence to support the avg_sal variable is not normally distributed.'.format(p, decimals=4)
  ttest_less_str = wrapper.fill(text=ttest_less)
  print(ttest_less_str)
elif np.round(p, decimals=4) > 0.05:
  ttest_more = 'The p-value of {} is greather than 0.05, so we fail to reject the Null Hypothesis. There is not sufficient evidence to suuport the avg_sal varaible is not normally distributed.'.format(p, decimals=4)
  ttest_more_str = wrapper.fill(text=ttest_more)
  print(ttest_more_str)

"""While non-normality was expected, it's important to also look at skewness and kurtosis. """

#investigate skewness of avg_sal variable
avg_sal_k = np.round(yrs_exp_no_outliers_df['avg_sal'].kurtosis(), decimals = 4)
#print explanation of skewness
if avg_sal_k < 0:
  print('The Kurtosis value of {} tells us the avg_sal varaible is skewed left.'.format(avg_sal_k))
elif avg_sal_k > 0:
  print('The Kurtosis value of {} tells us the avg_sal variable is skewed right.'.format(avg_sal_k))

"""##### **Conclusion**

Since the Kurtosis value of the avg_sal variable is between the loose -3 to 3 range, it's safe to proceed with parametric testing.

### **Is there a significant difference in avg_sal by search job?**

#### **Is average salary variable normally distributed per search_job?**

*  $H_o:$ Distribution is Normal
*   $H_a:$ Distribution is Not Normal
"""

#segregate data by city & search
is_aus = yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_city'] == 0]
is_bos = yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_city'] == 1]
is_chi = yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_city'] == 2]
is_co =  yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_city'] == 3]
is_la =  yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_city'] == 4]
is_ny =  yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_city'] == 5]
is_sea = yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_city'] == 6]
is_sf =  yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_city'] == 7]

is_da = yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_job'] == 1]
is_de = yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_job'] == 2]
is_ba = yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_job'] == 0]
is_ds = yrs_exp_no_outliers_df[yrs_exp_no_outliers_df['search_job'] == 3]

search_jobs = {'Data Analyst': is_da, 'Data Engineer':is_de, 
               'Business Analyst':is_ba, 'Data Scientist':is_ds}

for title, job in search_jobs.items():
  #Is the avg_salary distribution normal?
  t,p=stats.shapiro(job['avg_sal'])
  
  #explanation of t-testing
  if np.round(p, decimals=4) < 0.05:
    ttest_less = 'The p-value for {}, of {}, is less than 0.05, so we reject the Null Hypothesis. There is sufficient evidence to support the avg_sal variable is not normally distributed.'.format(title, p, decimals=4)
    ttest_less_str = wrapper.fill(text=ttest_less)
    print(ttest_less_str)
    print()
  elif np.round(p, decimals=4) > 0.05:
    ttest_more = 'The p-value for {}, of {}, is greather than 0.05, so we fail to reject the Null Hypothesis. There is not sufficient evidence to suuport the avg_sal varaible is not normally distributed.'.format(title, p, decimals=4)
    ttest_more_str = wrapper.fill(text=ttest_more)
    print(ttest_more_str)
    print()

"""While non-normality was expected, it's important to also look at skewness and kurtosis. """

for title, job in search_jobs.items():
  #investigate skewness of avg_sal variable
  avg_sal_k = np.round(job['avg_sal'].kurtosis(), decimals = 4)
  #print explanation of skewness
  if avg_sal_k < 0:
    print('The Kurtosis value for {}, of {}, tells us the avg_sal varaible is skewed left.'.format(title, avg_sal_k))
    print()
  elif avg_sal_k > 0:
    print('The Kurtosis value for {}, of {}, tells us the avg_sal variable is skewed right.'.format(title, avg_sal_k))
    print()

"""##### **Conclusion**

Since the Kurtosis values of each search job avg_sal variable is between the loose -3 to 3 range, it's safe to proceed with parametric testing.

#### **One-Way ANOVA**
Identifying if any one search job differs in avg_sal from the others
"""

job_anova = stats.f_oneway(is_da['avg_sal'],is_de['avg_sal'],is_ba['avg_sal'],is_ds['avg_sal'])

#explanation of anova-testing
if np.round(job_anova[1], decimals=4) < 0.05:
  anova_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support at least one search job has an average salary that is significantly different than the others.'.format(job_anova[1], decimals=4)
  anova_less_str = wrapper.fill(text=anova_less)
  print(anova_less_str)
elif np.round(job_anova[1], decimals=4) > 0.05:
  anova_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support at least one search job has an average salary that is significantly different than the others.'.format(job_anova[1], decimals=4)
  anova_more_str = wrapper.fill(text=anova_more)
  print(anova_more_str)

#visualize difference
plt.figure(figsize=(10,7))
avg_sal_diff = sns.pointplot(data= [is_da['avg_sal'],is_de['avg_sal'],is_ba['avg_sal'],is_ds['avg_sal']],
                    join= False)
avg_sal_diff.set(xticklabels = ['Data Analyst', 'Data Engineer', 'Business Analyst', 'Data Scientist'])
avg_sal_diff.set(ylabel='Average Salary')
plt.title('Average Salary by Scraped Job Title')
plt.show()

job_anova = stats.f_oneway(is_da['avg_sal'],is_de['avg_sal'],is_ba['avg_sal'],is_ds['avg_sal'])

#explanation of anova-testing
if np.round(job_anova[1], decimals=4) < 0.05:
  anova_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support at least one search job has an average salary that is significantly different than the others.'.format(job_anova[1], decimals=4)
  anova_less_str = wrapper.fill(text=anova_less)
  print(anova_less_str)
elif np.round(job_anova[1], decimals=4) > 0.05:
  anova_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support at least one search job has an average salary that is significantly different than the others.'.format(job_anova[1], decimals=4)
  anova_more_str = wrapper.fill(text=anova_more)
  print(anova_more_str)

"""##### **Conclusion**

It appears that Data Analyst & Business Analyst positions have significantly lower average salaires than Data Engineer and Data Scientist Positions. Let's see if these two groups have same averages with t-testing. Then, we can compare Data Engineer and Business Analyst to confirm this thouhgt.

#### **T-Testing**
"""

#data analyst & business analyst t-test
da_ba_ttest = stats.ttest_ind(is_da['avg_sal'], is_ba['avg_sal'])

#explanation of t-testing
if np.round(da_ba_ttest[1], decimals=4) < 0.05:
  da_ba_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support data analyst & business analyst positions have different average salaries.'.format(da_ba_ttest[1], decimals=4)
  da_ba_less_str = wrapper.fill(text=da_ba_less)
  print(da_ba_less_str)
  print()
elif np.round(da_ba_ttest[1], decimals=4) > 0.05:
  da_ba_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support data analyst & business analyst positions have different average salaries.'.format(da_ba_ttest[1], decimals=4)
  da_ba_more_str = wrapper.fill(text=da_ba_more)
  print(da_ba_more_str)
  print()

#data engineer & data scientist t-test
de_ds_ttest = stats.ttest_ind(is_de['avg_sal'], is_ds['avg_sal'])

#explanation of t-testing
if np.round(de_ds_ttest[1], decimals=4) < 0.05:
  de_ds_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support data engineer & data scientist positions have different average salaries.'.format(de_ds_ttest[1], decimals=4)
  de_ds_less_str = wrapper.fill(text=de_ds_less)
  print(de_ds_less_str)
  print()
elif np.round(de_ds_ttest[1], decimals=4) > 0.05:
  de_ds_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support data engineer & data scientist positions have different average salaries.'.format(de_ds_ttest[1], decimals=4)
  de_ds_more_str = wrapper.fill(text=de_ds_more)
  print(de_ds_more_str)
  print()

#data engineer & businnes analyst t-test
de_ba_ttest = stats.ttest_ind(is_de['avg_sal'], is_ba['avg_sal'])

#explanation of t-testing
if np.round(de_ba_ttest[1], decimals=4) < 0.05:
  de_ba_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support data engineer & businnes analyst positions have different average salaries.'.format(de_ba_ttest[1], decimals=4)
  de_ba_less_str = wrapper.fill(text=de_ba_less)
  print(de_ba_less_str)
  print()
elif np.round(de_ba_ttest[1], decimals=4) > 0.05:
  de_ba_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support data engineer & businnes analyst positions have different average salaries.'.format(de_ba_ttest[1], decimals=4)
  de_ba_more_str = wrapper.fill(text=de_ba_more)
  print(de_ba_more_str)
  print()

"""##### **Conclusion**

It is confirmed that Data Analyst & Business Analyst positions have significantly lower average salaires than Data Engineer and Data Scientist Positions. However, the average salaries between the group of Data Analyst and Data Engineer compared to Business Analyst and Data Scientist positions are significantly different.

### **Is there a significant difference in avg_sal by search city?**

#### **Is average salary variable normally distributed per search_city?**

*  $H_o:$ Distribution is Normal
*   $H_a:$ Distribution is Not Normal
"""

search_cities = {'Austin': is_aus, 'Boston':is_bos, 'Chicago':is_chi,
                 'Colorado':is_co, 'Los Angeles':is_la, 'New York': is_ny, 
                 'Seattle':is_sea, 'San Francisco':is_sf}

for city, df in search_cities.items():
  #Is the avg_salary distribution normal?
  t,p=stats.shapiro(df['avg_sal'])
  
  #explanation of t-testing
  if np.round(p, decimals=4) < 0.05:
    ttest_less = 'The p-value for {}, of {}, is less than 0.05, so we reject the Null Hypothesis. There is sufficient evidence to support the avg_sal variable is not normally distributed.'.format(city, p, decimals=4)
    ttest_less_str = wrapper.fill(text=ttest_less)
    print(ttest_less_str)
    print()
  elif np.round(p, decimals=4) > 0.05:
    ttest_more = 'The p-value for {}, of {}, is greather than 0.05, so we fail to reject the Null Hypothesis. There is not sufficient evidence to suuport the avg_sal varaible is not normally distributed.'.format(city, p, decimals=4)
    ttest_more_str = wrapper.fill(text=ttest_more)
    print(ttest_more_str)
    print()

"""While non-normality was expected, it's important to also look at skewness and kurtosis. """

for city, df in search_cities.items():
  #investigate skewness of avg_sal variable
  avg_sal_k = np.round(df['avg_sal'].kurtosis(), decimals = 4)
  #print explanation of skewness
  if avg_sal_k < 0:
    print('The Kurtosis value for {}, of {}, tells us the avg_sal varaible is skewed left.'.format(city, avg_sal_k))
    print()
  elif avg_sal_k > 0:
    print('The Kurtosis value for {}, of {}, tells us the avg_sal variable is skewed right.'.format(city, avg_sal_k))
    print()

"""##### **Conclusion**

Since the Kurtosis values of each search city, excluding Boston which has some outliers affecting Kurtosis, avg_sal variable is between the loose -3 to 3 range, it's safe to proceed with parametric testing.

#### **One-Way ANOVA**
Identifying if any one search job differs in avg_sal from the others
"""

city_anova = stats.f_oneway(is_aus['avg_sal'],is_bos['avg_sal'],is_chi['avg_sal'],
                            is_co['avg_sal'],is_la['avg_sal'], is_ny['avg_sal'],
                            is_sea['avg_sal'],is_sf['avg_sal'])

#explanation of anova-testing
if np.round(city_anova[1], decimals=4) < 0.05:
  anova_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_less_str = wrapper.fill(text=anova_less)
  print(anova_less_str)
elif np.round(city_anova[1], decimals=4) > 0.05:
  anova_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_more_str = wrapper.fill(text=anova_more)
  print(anova_more_str)

#visualize difference
plt.figure(figsize=(10,7))
avg_sal_diff = sns.pointplot(data= [is_aus['avg_sal'],is_bos['avg_sal'],is_chi['avg_sal'],
                            is_co['avg_sal'],is_la['avg_sal'], is_ny['avg_sal'],
                            is_sea['avg_sal'],is_sf['avg_sal']], join= False)

avg_sal_diff.set(xticklabels = ['Austin', 'Boston', 'Chicago', 'Colorado', 
                                'Los Angeles', 'New York', 'Seattle','San Francisco',])
avg_sal_diff.set(ylabel='Average Salary')
plt.title('Average Salary by Scraped Job City')
plt.show()

"""##### **Conclusion**

It appears that San Francisco has a significantly higher avg_salary than the other cities and there might be groups of others that are similar. For Example, Boston, Chicago, Colorado seem smilar while Austin, LA, New York, and Seattle seem similar to each other. Let's do some more ANOVA testing to confirm.

#### **Grouped ANOVA Testing**
"""

aus_bos_chi_co_anova = stats.f_oneway(is_aus['avg_sal'],is_bos['avg_sal'],is_chi['avg_sal'],
                            is_co['avg_sal'])

#explanation of anova-testing
if np.round(aus_bos_chi_co_anova[1], decimals=4) < 0.05:
  anova_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_less_str = wrapper.fill(text=anova_less)
  print(anova_less_str)
elif np.round(aus_bos_chi_co_anova[1], decimals=4) > 0.05:
  anova_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_more_str = wrapper.fill(text=anova_more)
  print(anova_more_str)

bos_chi_co_anova = stats.f_oneway(is_bos['avg_sal'],is_chi['avg_sal'],
                            is_co['avg_sal'])

#explanation of anova-testing
if np.round(bos_chi_co_anova[1], decimals=4) < 0.05:
  anova_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_less_str = wrapper.fill(text=anova_less)
  print(anova_less_str)
elif np.round(bos_chi_co_anova[1], decimals=4) > 0.05:
  anova_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_more_str = wrapper.fill(text=anova_more)
  print(anova_more_str)

aus_la_ny_sea_anova = stats.f_oneway(is_aus['avg_sal'], is_la['avg_sal'],is_ny['avg_sal'],is_sea['avg_sal'])

#explanation of anova-testing
if np.round(aus_la_ny_sea_anova[1], decimals=4) < 0.05:
  anova_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_less_str = wrapper.fill(text=anova_less)
  print(anova_less_str)
elif np.round(aus_la_ny_sea_anova[1], decimals=4) > 0.05:
  anova_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_more_str = wrapper.fill(text=anova_more)
  print(anova_more_str)

aus_la_ny_anova = stats.f_oneway(is_aus['avg_sal'], is_la['avg_sal'],is_ny['avg_sal'])

#explanation of anova-testing
if np.round(aus_la_ny_anova[1], decimals=4) < 0.05:
  anova_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_less_str = wrapper.fill(text=anova_less)
  print(anova_less_str)
elif np.round(aus_la_ny_anova[1], decimals=4) > 0.05:
  anova_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_more_str = wrapper.fill(text=anova_more)
  print(anova_more_str)

la_ny_sea_anova = stats.f_oneway(is_la['avg_sal'],is_ny['avg_sal'],is_sea['avg_sal'])

#explanation of anova-testing
if np.round(la_ny_sea_anova[1], decimals=4) < 0.05:
  anova_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_less_str = wrapper.fill(text=anova_less)
  print(anova_less_str)
elif np.round(la_ny_sea_anova[1], decimals=4) > 0.05:
  anova_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_more_str = wrapper.fill(text=anova_more)
  print(anova_more_str)

la_ny_sea_sf_anova = stats.f_oneway(is_la['avg_sal'],is_ny['avg_sal'],is_sea['avg_sal'], is_sf['avg_sal'])

#explanation of anova-testing
if np.round(la_ny_sea_sf_anova[1], decimals=4) < 0.05:
  anova_less = 'The p-value of {}, is less than 0.05. There is sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_less_str = wrapper.fill(text=anova_less)
  print(anova_less_str)
elif np.round(la_ny_sea_sf_anova[1], decimals=4) > 0.05:
  anova_more = 'The p-value of {}, is more than 0.05. There is not sufficient evidence to support at least one search city has an average salary that is significantly different than the others.'.format(city_anova[1], decimals=4)
  anova_more_str = wrapper.fill(text=anova_more)
  print(anova_more_str)

"""##### **Conclusion**

It is confirmed that Austin positions have a significanlty larger salary than Boston, Chicago, and Colorado positions. However, when grouped with LA, NY, and Seattle, Austin positions have similar average salaires to LA and NY but significantly lower than Seattle. Finally, San Francisco had a significantly higher average salary than all other locations.

# **Model Building & Testing**

We now have 4 Dataframe's to test models on.
*   **categories_df:** numeric data of combined sums of the categorie we extrapolated from the job descriptions. 2012 rows × 16 columns
*   **yrs_exp_df**: categories excluding jobs that did not have years of experience listed. 1711 rows × 16 columns
*   **cat_no_outliers_df:** numeric data of combined sums of the categorie we extrapolated from the job descriptions, outliers removed. 2012 rows × 16 columns
*   **yrs_exp_no_outliers_df**: categories excluding jobs that did not have years of experience listed, outliers removed. 1711 rows × 16 columns

For performance comparison, each will be tested with target variable as:
*    **Average Salary**
*    **Log of Average Salary**

### **Visualization of Target Variable**
"""

#visualize Categories Dataframe Average Salary
plt.figure(figsize = (20,40))
plt.subplot(421)
sns.distplot(categories_df['avg_sal'], kde = False)
plt.xlabel('Categories Dataframe Average Salary')
plt.ylabel('Frequency')
plt.title('Frequency of Categories Dataframe Average Salary')
plt.xticks(rotation = 45)

plt.subplot(422)
#visualize Log of Categories Dataframe Average Salary
sns.distplot(categories_df['log_avg_sal'], kde = False)
plt.xlabel('Log of Categories Dataframe Average Salary')
plt.ylabel('Frequency')
plt.title('Frequency of Log of Average Salary')
plt.xticks(rotation = 45)

#visualize Years Experience Dataframe Average Salary
plt.subplot(423)
sns.distplot(yrs_exp_df['avg_sal'], kde = False)
plt.xlabel('Years Experience Dataframe Average Salary')
plt.ylabel('Frequency')
plt.title('Frequency of Yrs Exp Dataframe Average Salary')
plt.xticks(rotation = 45)

#visualize Log of Years Experience Dataframe Average Salary
plt.subplot(424)
sns.distplot(yrs_exp_df['log_avg_sal'], kde = False)
plt.xlabel('Log of Years Experience Dataframe Average Salary')
plt.ylabel('Frequency')
plt.title('Frequency of Log of Yrs Exp Dataframe Average Salary')
plt.xticks(rotation = 45)

#visualize Categories w/o Outliers Dataframe Average Salary
plt.subplot(425)
sns.distplot(cat_no_outliers_df['avg_sal'], kde = False)
plt.xlabel('Categories Dataframe Average Salary')
plt.ylabel('Frequency')
plt.title('Frequency of Categories w/o Outliers Dataframe Average Salary')
plt.xticks(rotation = 45)

#visualize Log of Categories w/o Outliers Dataframe Average Salary
plt.subplot(426)
sns.distplot(cat_no_outliers_df['log_avg_sal'], kde = False)
plt.xlabel('Log of Categories w/o Outliers Dataframe Average Salary')
plt.ylabel('Frequency')
plt.title('Frequency of Log of Average Salary')
plt.xticks(rotation = 45)

#visualize Years Experience w/o Outliers Dataframe Average Salary
plt.subplot(427)
sns.distplot(yrs_exp_no_outliers_df['avg_sal'], kde = False)
plt.xlabel('Years Experience Dataframe Average Salary')
plt.ylabel('Frequency')
plt.title('Frequency of Years of Experience w/o Outliers Dataframe Average Salary')
plt.xticks(rotation = 45)

#visualize Log of Years Experience w/o Outliers Dataframe Average Salary
plt.subplot(428)
sns.distplot(yrs_exp_no_outliers_df['log_avg_sal'], kde = False)
plt.xlabel('Log of Years Experience w/o Outliers Dataframe Average Salary')
plt.ylabel('Frequency')
plt.title('Frequency of Lof of Years of Experience Dataframe Average Salary')
plt.xticks(rotation = 45)
plt.show()

"""## **Linear Regression**

### **categories_df**
"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#create instance of model
categories_lr = LinearRegression()

#fit model
categories_lr.fit(X_train, y_train)

#cross validate
categories_lr_cv_scores = cross_val_score(categories_lr, X_train, y_train, cv = 5)

#get scores
print('Accuracy Scores for the 5 folds: ', categories_lr_cv_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(categories_lr_cv_scores)))

#generate predictions
categories_lr_preds = categories_lr.predict(X_test)

#get performance metrics
categories_lr_rsquared_train = categories_lr.score(X_train, y_train)
categories_lr_rsquared_test = categories_lr.score(X_test, y_test)
categories_lr_mae = mean_absolute_error(y_test, categories_lr_preds)
categories_lr_mse = mse(y_test, categories_lr_preds)
categories_lr_rmse = rmse(y_test, categories_lr_preds)
categories_lr_mape = np.mean(np.abs((y_test - categories_lr_preds)/y_test)*100)
categories_lr_rmspe = (np.sqrt(np.mean(np.square((y_test - categories_lr_preds) / y_test)))) * 100
categories_lr_model_name = 'Categories Linear Regression'

#create easily readable display of performance metrics
categories_lr_dictionary = {'Model': categories_lr_model_name,
  'Training Set R Squared': categories_lr_rsquared_train, 
  'Test Set R Squared': categories_lr_rsquared_test, 'Mean Absolute Error': categories_lr_mae, 
  'Mean Sqaured Error': categories_lr_mse, 'Root Mean Squared Error': categories_lr_rmse,
  'Root Mean Squared Percentage Error': categories_lr_rmspe,
  'Mean Absolute Percentage Error': categories_lr_mape}

categories_lr_df = pd.DataFrame(categories_lr_dictionary, index=[0])

categories_lr_df

"""### **yrs_exp_df**"""

#set features & target
X= yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#create instance of model
yrs_exp_lr = LinearRegression()

#fit model
yrs_exp_lr.fit(X_train, y_train)

#cross validate
yrs_exp_lr_cv_scores = cross_val_score(yrs_exp_lr, X_train, y_train, cv = 5)

#get scores
print('Accuracy Scores for the 5 folds: ', yrs_exp_lr_cv_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_lr_cv_scores)))

#generate predictions
yrs_exp_lr_preds = yrs_exp_lr.predict(X_test)

#get performance metrics
yrs_exp_lr_rsquared_train = yrs_exp_lr.score(X_train, y_train)
yrs_exp_lr_rsquared_test = yrs_exp_lr.score(X_test, y_test)
yrs_exp_lr_mae = mean_absolute_error(y_test, yrs_exp_lr_preds)
yrs_exp_lr_mse = mse(y_test, yrs_exp_lr_preds)
yrs_exp_lr_rmse = rmse(y_test, yrs_exp_lr_preds)
yrs_exp_lr_mape = np.mean(np.abs((y_test - yrs_exp_lr_preds)/y_test)*100)
yrs_exp_lr_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_lr_preds) / y_test)))) * 100
yrs_exp_model_name = 'Years of Experience Linear Regression'

#create easily readable display of performance metrics
yrs_exp_lr_dictionary = {'Model': yrs_exp_model_name,
  'Training Set R Squared': yrs_exp_lr_rsquared_train, 
  'Test Set R Squared': yrs_exp_lr_rsquared_test, 'Mean Absolute Error': yrs_exp_lr_mae, 
  'Mean Sqaured Error': yrs_exp_lr_mse, 'Root Mean Squared Error': yrs_exp_lr_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_lr_rmspe,  
  'Mean Absolute Percentage Error': yrs_exp_lr_mape}

yrs_exp_lr_df = pd.DataFrame(yrs_exp_lr_dictionary, index=[0])

yrs_exp_lr_df

"""### **categories_df log(y)**"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#create instance of model
log_categories_lr = LinearRegression()

#fit model
log_categories_lr.fit(X_train, y_train)

#cross validate
log_categories_lr_cv_scores = cross_val_score(log_categories_lr, X_train, y_train, cv = 5)

#get scores
print('Accuracy Scores for the 5 folds: ', log_categories_lr_cv_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_categories_lr_cv_scores)))

#generate predictions
log_categories_lr_preds = log_categories_lr.predict(X_test)

#get performance metrics
log_categories_lr_rsquared_train = log_categories_lr.score(X_train, y_train)
log_categories_lr_rsquared_test = log_categories_lr.score(X_test, y_test)
log_categories_lr_mae = mean_absolute_error(y_test, log_categories_lr_preds)
log_categories_lr_mse = mse(y_test, log_categories_lr_preds)
log_categories_lr_rmse = rmse(y_test, log_categories_lr_preds)
log_categories_lr_mape = np.mean(np.abs((y_test - log_categories_lr_preds)/y_test)*100)
log_categories_lr_rmspe = (np.sqrt(np.mean(np.square((y_test - log_categories_lr_preds) / y_test)))) * 100
log_categories_lr_model_name = 'Categories Linear Regression Log(y)'

#create easily readable display of performance metrics
log_categories_lr_dictionary = {'Model': log_categories_lr_model_name,
  'Training Set R Squared': log_categories_lr_rsquared_train, 
  'Test Set R Squared': log_categories_lr_rsquared_test, 'Mean Absolute Error': log_categories_lr_mae, 
  'Mean Sqaured Error': log_categories_lr_mse, 'Root Mean Squared Error': log_categories_lr_rmse,
  'Root Mean Squared Percentage Error': log_categories_lr_rmspe,
  'Mean Absolute Percentage Error': log_categories_lr_mape}

log_categories_lr_df = pd.DataFrame(log_categories_lr_dictionary, index=[0])

log_categories_lr_df

"""### **yrs_exp_df log(y)**"""

#set features & target
X= yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#create instance of model
log_yrs_exp_lr = LinearRegression()

#fit model
log_yrs_exp_lr.fit(X_train, y_train)

#cross validate
log_yrs_exp_lr_cv_scores = cross_val_score(log_yrs_exp_lr, X_train, y_train, cv = 5)

#get scores
print('Accuracy Scores for the 5 folds: ', log_yrs_exp_lr_cv_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_lr_cv_scores)))

#generate predictions
log_yrs_exp_lr_preds = log_yrs_exp_lr.predict(X_test)

#get performance metrics
log_yrs_exp_lr_rsquared_train = log_yrs_exp_lr.score(X_train, y_train)
log_yrs_exp_lr_rsquared_test = log_yrs_exp_lr.score(X_test, y_test)
log_yrs_exp_lr_mae = mean_absolute_error(y_test, log_yrs_exp_lr_preds)
log_yrs_exp_lr_mse = mse(y_test, log_yrs_exp_lr_preds)
log_yrs_exp_lr_rmse = rmse(y_test, log_yrs_exp_lr_preds)
log_yrs_exp_lr_mape = np.mean(np.abs((y_test - log_yrs_exp_lr_preds)/y_test)*100)
log_yrs_exp_lr_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_lr_preds) / y_test)))) * 100
yrs_exp_model_name = 'Years of Experience Linear Regression Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_lr_dictionary = {'Model': yrs_exp_model_name,
  'Training Set R Squared': log_yrs_exp_lr_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_lr_rsquared_test, 'Mean Absolute Error': log_yrs_exp_lr_mae, 
  'Mean Sqaured Error': log_yrs_exp_lr_mse, 'Root Mean Squared Error': log_yrs_exp_lr_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_lr_rmspe,  
  'Mean Absolute Percentage Error': log_yrs_exp_lr_mape}

log_yrs_exp_lr_df = pd.DataFrame(log_yrs_exp_lr_dictionary, index=[0])

log_yrs_exp_lr_df

"""### **cat_no_outliers_df**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#create instance of model
cat_no_outliers_lr = LinearRegression()

#fit model
cat_no_outliers_lr.fit(X_train, y_train)

#cross validate
cat_no_outliers_lr_cv_scores = cross_val_score(cat_no_outliers_lr, X_train, y_train, cv = 5)

#get scores
print('Accuracy Scores for the 5 folds: ', cat_no_outliers_lr_cv_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(cat_no_outliers_lr_cv_scores)))

#generate predictions
cat_no_outliers_lr_preds = cat_no_outliers_lr.predict(X_test)

#get performance metrics
cat_no_outliers_lr_rsquared_train = cat_no_outliers_lr.score(X_train, y_train)
cat_no_outliers_lr_rsquared_test = cat_no_outliers_lr.score(X_test, y_test)
cat_no_outliers_lr_mae = mean_absolute_error(y_test, cat_no_outliers_lr_preds)
cat_no_outliers_lr_mse = mse(y_test, cat_no_outliers_lr_preds)
cat_no_outliers_lr_rmse = rmse(y_test, cat_no_outliers_lr_preds)
cat_no_outliers_lr_mape = np.mean(np.abs((y_test - cat_no_outliers_lr_preds)/y_test)*100)
cat_no_outliers_lr_rmspe = (np.sqrt(np.mean(np.square((y_test - cat_no_outliers_lr_preds) / y_test)))) * 100
cat_no_outliers_lr_model_name = 'Categories w/o Outliers Linear Regression'

#create easily readable display of performance metrics
cat_no_outliers_lr_dictionary = {'Model': cat_no_outliers_lr_model_name,
  'Training Set R Squared': cat_no_outliers_lr_rsquared_train, 
  'Test Set R Squared': cat_no_outliers_lr_rsquared_test, 'Mean Absolute Error': cat_no_outliers_lr_mae, 
  'Mean Sqaured Error': cat_no_outliers_lr_mse, 'Root Mean Squared Error': cat_no_outliers_lr_rmse,
  'Root Mean Squared Percentage Error': cat_no_outliers_lr_rmspe,
  'Mean Absolute Percentage Error': cat_no_outliers_lr_mape}

cat_no_outliers_lr_df = pd.DataFrame(cat_no_outliers_lr_dictionary, index=[0])

cat_no_outliers_lr_df

"""### **yrs_exp_no_outliers_df**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#create instance of model
yrs_exp_no_outliers_lr = LinearRegression()

#fit model
yrs_exp_no_outliers_lr.fit(X_train, y_train)

#cross validate
yrs_exp_no_outliers_lr_cv_scores = cross_val_score(yrs_exp_no_outliers_lr, X_train, y_train, cv = 5)

#get scores
print('Accuracy Scores for the 5 folds: ', yrs_exp_no_outliers_lr_cv_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_no_outliers_lr_cv_scores)))

#generate predictions
yrs_exp_no_outliers_lr_preds = yrs_exp_no_outliers_lr.predict(X_test)

#get performance metrics
yrs_exp_no_outliers_lr_rsquared_train = yrs_exp_no_outliers_lr.score(X_train, y_train)
yrs_exp_no_outliers_lr_rsquared_test = yrs_exp_no_outliers_lr.score(X_test, y_test)
yrs_exp_no_outliers_lr_mae = mean_absolute_error(y_test, yrs_exp_no_outliers_lr_preds)
yrs_exp_no_outliers_lr_mse = mse(y_test, yrs_exp_no_outliers_lr_preds)
yrs_exp_no_outliers_lr_rmse = rmse(y_test, yrs_exp_no_outliers_lr_preds)
yrs_exp_no_outliers_lr_mape = np.mean(np.abs((y_test - yrs_exp_no_outliers_lr_preds)/y_test)*100)
yrs_exp_no_outliers_lr_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_no_outliers_lr_preds) / y_test)))) * 100
yrs_exp_no_outliers_lr_model_name = 'Years of Experience w/o Outliers Linear Regression'

#create easily readable display of performance metrics
yrs_exp_no_outliers_lr_dictionary = {'Model': yrs_exp_no_outliers_lr_model_name,
  'Training Set R Squared': yrs_exp_no_outliers_lr_rsquared_train, 
  'Test Set R Squared': yrs_exp_no_outliers_lr_rsquared_test, 'Mean Absolute Error': yrs_exp_no_outliers_lr_mae, 
  'Mean Sqaured Error': yrs_exp_no_outliers_lr_mse, 'Root Mean Squared Error': yrs_exp_no_outliers_lr_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_no_outliers_lr_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_no_outliers_lr_mape}

yrs_exp_no_outliers_lr_df = pd.DataFrame(yrs_exp_no_outliers_lr_dictionary, index=[0])

yrs_exp_no_outliers_lr_df

"""### **cat_no_outliers_df log(y)**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#create instance of model
log_cat_no_outliers_lr = LinearRegression()

#fit model
log_cat_no_outliers_lr.fit(X_train, y_train)

#cross validate
log_cat_no_outliers_lr_cv_scores = cross_val_score(log_cat_no_outliers_lr, X_train, y_train, cv = 5)

#get scores
print('Accuracy Scores for the 5 folds: ', log_cat_no_outliers_lr_cv_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_cat_no_outliers_lr_cv_scores)))

#generate predictions
log_cat_no_outliers_lr_preds = log_cat_no_outliers_lr.predict(X_test)

#get performance metrics
log_cat_no_outliers_lr_rsquared_train = log_cat_no_outliers_lr.score(X_train, y_train)
log_cat_no_outliers_lr_rsquared_test = log_cat_no_outliers_lr.score(X_test, y_test)
log_cat_no_outliers_lr_mae = mean_absolute_error(y_test, log_cat_no_outliers_lr_preds)
log_cat_no_outliers_lr_mse = mse(y_test, log_cat_no_outliers_lr_preds)
log_cat_no_outliers_lr_rmse = rmse(y_test, log_cat_no_outliers_lr_preds)
log_cat_no_outliers_lr_mape = np.mean(np.abs((y_test - log_cat_no_outliers_lr_preds)/y_test)*100)
log_cat_no_outliers_lr_rmspe = (np.sqrt(np.mean(np.square((y_test - log_cat_no_outliers_lr_preds) / y_test)))) * 100
log_cat_no_outliers_lr_model_name = 'Categories w/o Outliers Linear Regression Log(y)'

#create easily readable display of performance metrics
log_cat_no_outliers_lr_dictionary = {'Model': log_cat_no_outliers_lr_model_name,
  'Training Set R Squared': log_cat_no_outliers_lr_rsquared_train, 
  'Test Set R Squared': log_cat_no_outliers_lr_rsquared_test, 'Mean Absolute Error': log_cat_no_outliers_lr_mae, 
  'Mean Sqaured Error': log_cat_no_outliers_lr_mse, 'Root Mean Squared Error': log_cat_no_outliers_lr_rmse,
  'Root Mean Squared Percentage Error': log_cat_no_outliers_lr_rmspe,
  'Mean Absolute Percentage Error': log_cat_no_outliers_lr_mape}

log_cat_no_outliers_lr_df = pd.DataFrame(log_cat_no_outliers_lr_dictionary, index=[0])

log_cat_no_outliers_lr_df

"""### **yrs_exp_no_outliers_df**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#create instance of model
log_yrs_exp_no_outliers_lr = LinearRegression()

#fit model
log_yrs_exp_no_outliers_lr.fit(X_train, y_train)

#cross validate
log_yrs_exp_no_outliers_lr_cv_scores = cross_val_score(log_yrs_exp_no_outliers_lr, X_train, y_train, cv = 5)

#get scores
print('Accuracy Scores for the 5 folds: ', log_yrs_exp_no_outliers_lr_cv_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_no_outliers_lr_cv_scores)))

#generate predictions
log_yrs_exp_no_outliers_lr_preds = log_yrs_exp_no_outliers_lr.predict(X_test)

#get performance metrics
log_yrs_exp_no_outliers_lr_rsquared_train = log_yrs_exp_no_outliers_lr.score(X_train, y_train)
log_yrs_exp_no_outliers_lr_rsquared_test = log_yrs_exp_no_outliers_lr.score(X_test, y_test)
log_yrs_exp_no_outliers_lr_mae = mean_absolute_error(y_test, log_yrs_exp_no_outliers_lr_preds)
log_yrs_exp_no_outliers_lr_mse = mse(y_test, log_yrs_exp_no_outliers_lr_preds)
log_yrs_exp_no_outliers_lr_rmse = rmse(y_test, log_yrs_exp_no_outliers_lr_preds)
log_yrs_exp_no_outliers_lr_mape = np.mean(np.abs((y_test - log_yrs_exp_no_outliers_lr_preds)/y_test)*100)
log_yrs_exp_no_outliers_lr_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_no_outliers_lr_preds) / y_test)))) * 100
log_yrs_exp_no_outliers_lr_model_name = 'Years of Experience w/o Outliers Linear Regression Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_no_outliers_lr_dictionary = {'Model': log_yrs_exp_no_outliers_lr_model_name,
  'Training Set R Squared': log_yrs_exp_no_outliers_lr_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_no_outliers_lr_rsquared_test, 'Mean Absolute Error': log_yrs_exp_no_outliers_lr_mae, 
  'Mean Sqaured Error': log_yrs_exp_no_outliers_lr_mse, 'Root Mean Squared Error': log_yrs_exp_no_outliers_lr_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_no_outliers_lr_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_no_outliers_lr_mape}

log_yrs_exp_no_outliers_lr_df = pd.DataFrame(log_yrs_exp_no_outliers_lr_dictionary, index=[0])

log_yrs_exp_no_outliers_lr_df

"""## **Ridge Regression**

### **categories_df**
"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

categories_ridge_grid = GridSearchCV(Ridge(), param_grid)

# fitting the model for grid search 
categories_ridge_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(categories_ridge_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(categories_ridge_grid.best_estimator_)

#set model with ideal hyperparameters from above
categories_ridge = Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,
      random_state=None, solver='auto', tol=0.001)

categories_ridge.fit(X_train, y_train)

#cross validate
categories_ridge_scores = cross_val_score(categories_ridge, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', categories_ridge_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(categories_ridge_scores)))

#generate predictions
categories_ridge_preds = categories_ridge.predict(X_test)

#more performance metrics
categories_ridge_rsquared_train = categories_ridge.score(X_train, y_train)
categories_ridge_rsquared_test = categories_ridge.score(X_test, y_test)
categories_ridge_mae = mean_absolute_error(y_test, categories_ridge_preds)
categories_ridge_mse = mse(y_test, categories_ridge_preds)
categories_ridge_rmse = rmse(y_test, categories_ridge_preds)
categories_ridge_mape = np.mean(np.abs((y_test - categories_ridge_preds)/y_test)*100)
categories_ridge_rmspe = (np.sqrt(np.mean(np.square((y_test - categories_ridge_preds) / y_test)))) * 100
categories_ridge_model_name = 'Categories Ridge Regression'

#create easily readable display of performance metrics
categories_ridge_dictionary = {'Model': categories_ridge_model_name,
  'Training Set R Squared': categories_ridge_rsquared_train, 
  'Test Set R Squared': categories_ridge_rsquared_test, 'Mean Absolute Error': categories_ridge_mae, 
  'Mean Sqaured Error': categories_ridge_mse, 'Root Mean Squared Error': categories_ridge_rmse,
  'Root Mean Squared Percentage Error': categories_ridge_rmspe,
  'Mean Absolute Percentage Error': categories_ridge_mape}

categories_ridge_df = pd.DataFrame(categories_ridge_dictionary, index=[0])

categories_ridge_df

"""### **yrs_exp_df**"""

#set features & target
X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

yrs_exp_ridge_grid = GridSearchCV(Ridge(), param_grid)

# fitting the model for grid search 
yrs_exp_ridge_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(yrs_exp_ridge_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(yrs_exp_ridge_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_ridge = Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,
      random_state=None, solver='auto', tol=0.001)

yrs_exp_ridge.fit(X_train, y_train)

#cross validate
yrs_exp_ridge_scores = cross_val_score(yrs_exp_ridge, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_ridge_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_ridge_scores)))

#generate predictions
yrs_exp_ridge_preds = yrs_exp_ridge.predict(X_test)

#more performance metrics
yrs_exp_ridge_rsquared_train = yrs_exp_ridge.score(X_train, y_train)
yrs_exp_ridge_rsquared_test = yrs_exp_ridge.score(X_test, y_test)
yrs_exp_ridge_mae = mean_absolute_error(y_test, yrs_exp_ridge_preds)
yrs_exp_ridge_mse = mse(y_test, yrs_exp_ridge_preds)
yrs_exp_ridge_rmse = rmse(y_test, yrs_exp_ridge_preds)
yrs_exp_ridge_mape = np.mean(np.abs((y_test - yrs_exp_ridge_preds)/y_test)*100)
yrs_exp_ridge_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_ridge_preds) / y_test)))) * 100
yrs_exp_model_name = 'Years of Experience Ridge Regression'

#create easily readable display of performance metrics
yrs_exp_ridge_dictionary = {'Model': yrs_exp_model_name, 
  'Training Set R Squared': yrs_exp_ridge_rsquared_train, 
  'Test Set R Squared': yrs_exp_ridge_rsquared_test, 'Mean Absolute Error': yrs_exp_ridge_mae, 
  'Mean Sqaured Error': yrs_exp_ridge_mse, 'Root Mean Squared Error': yrs_exp_ridge_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_ridge_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_ridge_mape}

yrs_exp_ridge_df = pd.DataFrame(yrs_exp_ridge_dictionary, index=[0])

yrs_exp_ridge_df

"""### **categories_df log(y)**"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

log_categories_ridge_grid = GridSearchCV(Ridge(), param_grid)

# fitting the model for grid search 
log_categories_ridge_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_categories_ridge_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_categories_ridge_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_categories_ridge = Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,
      random_state=None, solver='auto', tol=0.001)

log_categories_ridge.fit(X_train, y_train)

#cross validate
log_categories_ridge_scores = cross_val_score(log_categories_ridge, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_categories_ridge_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_categories_ridge_scores)))

#generate predictions
log_categories_ridge_preds = log_categories_ridge.predict(X_test)

#more performance metrics
log_categories_ridge_rsquared_train = log_categories_ridge.score(X_train, y_train)
log_categories_ridge_rsquared_test = log_categories_ridge.score(X_test, y_test)
log_categories_ridge_mae = mean_absolute_error(y_test, log_categories_ridge_preds)
log_categories_ridge_mse = mse(y_test, log_categories_ridge_preds)
log_categories_ridge_rmse = rmse(y_test, log_categories_ridge_preds)
log_categories_ridge_mape = np.mean(np.abs((y_test - log_categories_ridge_preds)/y_test)*100)
log_categories_ridge_rmspe = (np.sqrt(np.mean(np.square((y_test - log_categories_ridge_preds) / y_test)))) * 100
log_categories_ridge_model_name = 'Categories Ridge Regression Log(y)'

#create easily readable display of performance metrics
log_categories_ridge_dictionary = {'Model': log_categories_ridge_model_name,
  'Training Set R Squared': log_categories_ridge_rsquared_train, 
  'Test Set R Squared': log_categories_ridge_rsquared_test, 'Mean Absolute Error': log_categories_ridge_mae, 
  'Mean Sqaured Error': log_categories_ridge_mse, 'Root Mean Squared Error': log_categories_ridge_rmse,
  'Root Mean Squared Percentage Error': log_categories_ridge_rmspe,
  'Mean Absolute Percentage Error': log_categories_ridge_mape}

log_categories_ridge_df = pd.DataFrame(log_categories_ridge_dictionary, index=[0])

log_categories_ridge_df

"""### **yrs_exp_df log(y)**"""

#set features & target
X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

log_yrs_exp_ridge_grid = GridSearchCV(Ridge(), param_grid)

# fitting the model for grid search 
log_yrs_exp_ridge_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_yrs_exp_ridge_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_yrs_exp_ridge_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_ridge = Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,
      random_state=None, solver='auto', tol=0.001)

log_yrs_exp_ridge.fit(X_train, y_train)

#cross validate
log_yrs_exp_ridge_scores = cross_val_score(log_yrs_exp_ridge, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_ridge_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_ridge_scores)))

#generate predictions
log_yrs_exp_ridge_preds = log_yrs_exp_ridge.predict(X_test)

#more performance metrics
log_yrs_exp_ridge_rsquared_train = log_yrs_exp_ridge.score(X_train, y_train)
log_yrs_exp_ridge_rsquared_test = log_yrs_exp_ridge.score(X_test, y_test)
log_yrs_exp_ridge_mae = mean_absolute_error(y_test, log_yrs_exp_ridge_preds)
log_yrs_exp_ridge_mse = mse(y_test, log_yrs_exp_ridge_preds)
log_yrs_exp_ridge_rmse = rmse(y_test, log_yrs_exp_ridge_preds)
log_yrs_exp_ridge_mape = np.mean(np.abs((y_test - log_yrs_exp_ridge_preds)/y_test)*100)
log_yrs_exp_ridge_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_ridge_preds) / y_test)))) * 100
yrs_exp_model_name = 'Years of Experience Ridge Regression Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_ridge_dictionary = {'Model': yrs_exp_model_name, 
  'Training Set R Squared': log_yrs_exp_ridge_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_ridge_rsquared_test, 'Mean Absolute Error': log_yrs_exp_ridge_mae, 
  'Mean Sqaured Error': log_yrs_exp_ridge_mse, 'Root Mean Squared Error': log_yrs_exp_ridge_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_ridge_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_ridge_mape}

log_yrs_exp_ridge_df = pd.DataFrame(log_yrs_exp_ridge_dictionary, index=[0])

log_yrs_exp_ridge_df

"""### **cat_no_outliers_df**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

cat_no_outliers_grid = GridSearchCV(Ridge(), param_grid)

# fitting the model for grid search 
cat_no_outliers_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(cat_no_outliers_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(cat_no_outliers_grid.best_estimator_)

#set model with ideal hyperparameters from above
cat_no_outliers = Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,
      random_state=None, solver='auto', tol=0.001)

cat_no_outliers.fit(X_train, y_train)

#cross validate
cat_no_outliers_scores = cross_val_score(cat_no_outliers, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', cat_no_outliers_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(cat_no_outliers_scores)))

#generate predictions
cat_no_outliers_preds = cat_no_outliers.predict(X_test)

#more performance metrics
cat_no_outliers_rsquared_train = cat_no_outliers.score(X_train, y_train)
cat_no_outliers_rsquared_test = cat_no_outliers.score(X_test, y_test)
cat_no_outliers_mae = mean_absolute_error(y_test, cat_no_outliers_preds)
cat_no_outliers_mse = mse(y_test, cat_no_outliers_preds)
cat_no_outliers_rmse = rmse(y_test, cat_no_outliers_preds)
cat_no_outliers_mape = np.mean(np.abs((y_test - cat_no_outliers_preds)/y_test)*100)
cat_no_outliers_rmspe = (np.sqrt(np.mean(np.square((y_test - cat_no_outliers_preds) / y_test)))) * 100
cat_no_outliers_model_name = 'Categories w/o Outliers Ridge Regression'

#create easily readable display of performance metrics
cat_no_outliers_dictionary = {'Model': cat_no_outliers_model_name,
  'Training Set R Squared': cat_no_outliers_rsquared_train, 
  'Test Set R Squared': cat_no_outliers_rsquared_test, 'Mean Absolute Error': cat_no_outliers_mae, 
  'Mean Sqaured Error': cat_no_outliers_mse, 'Root Mean Squared Error': cat_no_outliers_rmse,
  'Root Mean Squared Percentage Error': cat_no_outliers_rmspe,
  'Mean Absolute Percentage Error': cat_no_outliers_mape}

cat_no_outliers_ridge_df = pd.DataFrame(cat_no_outliers_dictionary, index=[0])

cat_no_outliers_ridge_df

"""### **yrs_exp_no_outliers_df**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

yrs_exp_no_outliers_ridge_grid = GridSearchCV(Ridge(), param_grid)

# fitting the model for grid search 
yrs_exp_no_outliers_ridge_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(yrs_exp_no_outliers_ridge_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(yrs_exp_no_outliers_ridge_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_no_outliers_ridge = Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,
      random_state=None, solver='auto', tol=0.001)

yrs_exp_no_outliers_ridge.fit(X_train, y_train)

#cross validate
yrs_exp_no_outliers_ridge_scores = cross_val_score(yrs_exp_no_outliers_ridge, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_no_outliers_ridge_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_no_outliers_ridge_scores)))

#generate predictions
yrs_exp_no_outliers_ridge_preds = yrs_exp_no_outliers_ridge.predict(X_test)

#more performance metrics
yrs_exp_no_outliers_ridge_rsquared_train = yrs_exp_no_outliers_ridge.score(X_train, y_train)
yrs_exp_no_outliers_ridge_rsquared_test = yrs_exp_no_outliers_ridge.score(X_test, y_test)
yrs_exp_no_outliers_ridge_mae = mean_absolute_error(y_test, yrs_exp_no_outliers_ridge_preds)
yrs_exp_no_outliers_ridge_mse = mse(y_test, yrs_exp_no_outliers_ridge_preds)
yrs_exp_no_outliers_ridge_rmse = rmse(y_test, yrs_exp_no_outliers_ridge_preds)
yrs_exp_no_outliers_ridge_mape = np.mean(np.abs((y_test - yrs_exp_no_outliers_ridge_preds)/y_test)*100)
yrs_exp_no_outliers_ridge_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_no_outliers_ridge_preds) / y_test)))) * 100
yrs_exp_model_name = 'Years of Experience w/o Outliers Ridge Regression'

#create easily readable display of performance metrics
yrs_exp_no_outliers_ridge_dictionary = {'Model': yrs_exp_model_name, 
  'Training Set R Squared': yrs_exp_no_outliers_ridge_rsquared_train, 
  'Test Set R Squared': yrs_exp_no_outliers_ridge_rsquared_test, 'Mean Absolute Error': yrs_exp_no_outliers_ridge_mae, 
  'Mean Sqaured Error': yrs_exp_no_outliers_ridge_mse, 'Root Mean Squared Error': yrs_exp_no_outliers_ridge_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_no_outliers_ridge_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_no_outliers_ridge_mape}

yrs_exp_no_outliers_ridge_df = pd.DataFrame(yrs_exp_no_outliers_ridge_dictionary, index=[0])

yrs_exp_no_outliers_ridge_df

"""### **categories_df log(y)**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

log_cat_no_outliers_grid = GridSearchCV(Ridge(), param_grid)

# fitting the model for grid search 
log_cat_no_outliers_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_cat_no_outliers_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_cat_no_outliers_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_cat_no_outliers = Ridge(alpha=1, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,
      random_state=None, solver='auto', tol=0.001)

log_cat_no_outliers.fit(X_train, y_train)

#cross validate
log_cat_no_outliers_scores = cross_val_score(log_cat_no_outliers, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_cat_no_outliers_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_cat_no_outliers_scores)))

#generate predictions
log_cat_no_outliers_preds = log_cat_no_outliers.predict(X_test)

#more performance metrics
log_cat_no_outliers_rsquared_train = log_cat_no_outliers.score(X_train, y_train)
log_cat_no_outliers_rsquared_test = log_cat_no_outliers.score(X_test, y_test)
log_cat_no_outliers_mae = mean_absolute_error(y_test, log_cat_no_outliers_preds)
log_cat_no_outliers_mse = mse(y_test, log_cat_no_outliers_preds)
log_cat_no_outliers_rmse = rmse(y_test, log_cat_no_outliers_preds)
log_cat_no_outliers_mape = np.mean(np.abs((y_test - log_cat_no_outliers_preds)/y_test)*100)
log_cat_no_outliers_rmspe = (np.sqrt(np.mean(np.square((y_test - log_cat_no_outliers_preds) / y_test)))) * 100
log_cat_no_outliers_model_name = 'Categories w/o Outliers Ridge Regression Log(y)'

#create easily readable display of performance metrics
log_cat_no_outliers_dictionary = {'Model': log_cat_no_outliers_model_name,
  'Training Set R Squared': log_cat_no_outliers_rsquared_train, 
  'Test Set R Squared': log_cat_no_outliers_rsquared_test, 'Mean Absolute Error': log_cat_no_outliers_mae, 
  'Mean Sqaured Error': log_cat_no_outliers_mse, 'Root Mean Squared Error': log_cat_no_outliers_rmse,
  'Root Mean Squared Percentage Error': log_cat_no_outliers_rmspe,
  'Mean Absolute Percentage Error': log_cat_no_outliers_mape}

log_cat_no_outliers_ridge_df = pd.DataFrame(log_cat_no_outliers_dictionary, index=[0])

log_cat_no_outliers_ridge_df

"""### **yrs_exp_no_outliers_df log(y)**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

log_yrs_exp_no_outliers_ridge_grid = GridSearchCV(Ridge(), param_grid)

# fitting the model for grid search 
log_yrs_exp_no_outliers_ridge_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_yrs_exp_no_outliers_ridge_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_yrs_exp_no_outliers_ridge_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_no_outliers_ridge = Ridge(alpha=10, copy_X=True, fit_intercept=True, max_iter=None, normalize=False,
      random_state=None, solver='auto', tol=0.001)

log_yrs_exp_no_outliers_ridge.fit(X_train, y_train)

#cross validate
log_yrs_exp_no_outliers_ridge_scores = cross_val_score(log_yrs_exp_no_outliers_ridge, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_no_outliers_ridge_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_no_outliers_ridge_scores)))

#generate predictions
log_yrs_exp_no_outliers_ridge_preds = log_yrs_exp_no_outliers_ridge.predict(X_test)

#more performance metrics
log_yrs_exp_no_outliers_ridge_rsquared_train = log_yrs_exp_no_outliers_ridge.score(X_train, y_train)
log_yrs_exp_no_outliers_ridge_rsquared_test = log_yrs_exp_no_outliers_ridge.score(X_test, y_test)
log_yrs_exp_no_outliers_ridge_mae = mean_absolute_error(y_test, log_yrs_exp_no_outliers_ridge_preds)
log_yrs_exp_no_outliers_ridge_mse = mse(y_test, log_yrs_exp_no_outliers_ridge_preds)
log_yrs_exp_no_outliers_ridge_rmse = rmse(y_test, log_yrs_exp_no_outliers_ridge_preds)
log_yrs_exp_no_outliers_ridge_mape = np.mean(np.abs((y_test - log_yrs_exp_no_outliers_ridge_preds)/y_test)*100)
log_yrs_exp_no_outliers_ridge_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_no_outliers_ridge_preds) / y_test)))) * 100
yrs_exp_model_name = 'Years of Experience w/o Outliers Ridge Regression Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_no_outliers_ridge_dictionary = {'Model': yrs_exp_model_name, 
  'Training Set R Squared': log_yrs_exp_no_outliers_ridge_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_no_outliers_ridge_rsquared_test, 'Mean Absolute Error': log_yrs_exp_no_outliers_ridge_mae, 
  'Mean Sqaured Error': log_yrs_exp_no_outliers_ridge_mse, 'Root Mean Squared Error': log_yrs_exp_no_outliers_ridge_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_no_outliers_ridge_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_no_outliers_ridge_mape}

log_yrs_exp_no_outliers_ridge_df = pd.DataFrame(log_yrs_exp_no_outliers_ridge_dictionary, index=[0])

log_yrs_exp_no_outliers_ridge_df

"""## **Lasso Regression**

### **categories_df**
"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

categories_lasso_grid = GridSearchCV(Lasso(), param_grid)

# fitting the model for grid search 
categories_lasso_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(categories_lasso_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(categories_lasso_grid.best_estimator_)

#set model with ideal hyperparameters from above
categories_lasso = Lasso(alpha=10, copy_X=True, fit_intercept=True, max_iter=1000,
      normalize=False, positive=False, precompute=False, random_state=None,
      selection='cyclic', tol=0.0001, warm_start=False)

categories_lasso.fit(X_train, y_train)

#cross validate
categories_lasso_scores = cross_val_score(categories_lasso, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', categories_lasso_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(categories_lasso_scores)))

#generate predictions
categories_lasso_preds = categories_lasso.predict(X_test)

#more performance metrics
categories_lasso_rsquared_train = categories_lasso.score(X_train, y_train)
categories_lasso_rsquared_test = categories_lasso.score(X_test, y_test)
categories_lasso_mae = mean_absolute_error(y_test, categories_lasso_preds)
categories_lasso_mse = mse(y_test, categories_lasso_preds)
categories_lasso_rmse = rmse(y_test, categories_lasso_preds)
categories_lasso_mape = np.mean(np.abs((y_test - categories_lasso_preds)/y_test)*100)
categories_lasso_rmspe = (np.sqrt(np.mean(np.square((y_test - categories_lasso_preds) / y_test)))) * 100
categories_lasso_model_name = 'Categories Lasso Regression'

#create easily readable display of performance metrics
categories_lasso_dictionary = {'Model': categories_lasso_model_name,
  'Training Set R Squared': categories_lasso_rsquared_train, 
  'Test Set R Squared': categories_lasso_rsquared_test, 'Mean Absolute Error': categories_lasso_mae, 
  'Mean Sqaured Error': categories_lasso_mse, 'Root Mean Squared Error': categories_lasso_rmse,
  'Root Mean Squared Percentage Error': categories_lasso_rmspe,
  'Mean Absolute Percentage Error': categories_lasso_mape}

categories_lasso_df = pd.DataFrame(categories_lasso_dictionary, index=[0])

categories_lasso_df

"""### **yrs_exp_df**"""

#set features & target
X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

yrs_exp_lasso_grid = GridSearchCV(Lasso(), param_grid)

# fitting the model for grid search 
yrs_exp_lasso_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(yrs_exp_lasso_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(yrs_exp_lasso_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_lasso = Lasso(alpha=100, copy_X=True, fit_intercept=True, max_iter=1000,
      normalize=False, positive=False, precompute=False, random_state=None,
      selection='cyclic', tol=0.0001, warm_start=False)

yrs_exp_lasso.fit(X_train, y_train)

#cross validate
yrs_exp_lasso_scores = cross_val_score(yrs_exp_lasso, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_lasso_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_lasso_scores)))

#generate predictions
yrs_exp_lasso_preds = yrs_exp_lasso.predict(X_test)

#more performance metrics
yrs_exp_lasso_rsquared_train = yrs_exp_lasso.score(X_train, y_train)
yrs_exp_lasso_rsquared_test = yrs_exp_lasso.score(X_test, y_test)
yrs_exp_lasso_mae = mean_absolute_error(y_test, yrs_exp_lasso_preds)
yrs_exp_lasso_mse = mse(y_test, yrs_exp_lasso_preds)
yrs_exp_lasso_rmse = rmse(y_test, yrs_exp_lasso_preds)
yrs_exp_lasso_mape = np.mean(np.abs((y_test - yrs_exp_lasso_preds)/y_test)*100)
yrs_exp_lasso_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_lasso_preds) / y_test)))) * 100
yrs_exp_lasso_model_name = 'Years of Experience Lasso Regression'

#create easily readable display of performance metrics
yrs_exp_lasso_dictionary = {'Model': yrs_exp_lasso_model_name,
  'Training Set R Squared': yrs_exp_lasso_rsquared_train, 
  'Test Set R Squared': yrs_exp_lasso_rsquared_test, 'Mean Absolute Error': yrs_exp_lasso_mae, 
  'Mean Sqaured Error': yrs_exp_lasso_mse, 'Root Mean Squared Error': yrs_exp_lasso_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_lasso_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_lasso_mape}

yrs_exp_lasso_df = pd.DataFrame(yrs_exp_lasso_dictionary, index=[0])

yrs_exp_lasso_df

"""### **categories_df log(y)**"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

log_categories_lasso_grid = GridSearchCV(Lasso(), param_grid)

# fitting the model for grid search 
log_categories_lasso_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_categories_lasso_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_categories_lasso_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_categories_lasso = Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,
      normalize=False, positive=False, precompute=False, random_state=None,
      selection='cyclic', tol=0.0001, warm_start=False)

log_categories_lasso.fit(X_train, y_train)

#cross validate
log_categories_lasso_scores = cross_val_score(log_categories_lasso, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_categories_lasso_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_categories_lasso_scores)))

#generate predictions
log_categories_lasso_preds = log_categories_lasso.predict(X_test)

#more performance metrics
log_categories_lasso_rsquared_train = log_categories_lasso.score(X_train, y_train)
log_categories_lasso_rsquared_test = log_categories_lasso.score(X_test, y_test)
log_categories_lasso_mae = mean_absolute_error(y_test, log_categories_lasso_preds)
log_categories_lasso_mse = mse(y_test, log_categories_lasso_preds)
log_categories_lasso_rmse = rmse(y_test, log_categories_lasso_preds)
log_categories_lasso_mape = np.mean(np.abs((y_test - log_categories_lasso_preds)/y_test)*100)
log_categories_lasso_rmspe = (np.sqrt(np.mean(np.square((y_test - log_categories_lasso_preds) / y_test)))) * 100
log_categories_lasso_model_name = 'Categories Lasso Regression Log(y)'

#create easily readable display of performance metrics
log_categories_lasso_dictionary = {'Model': log_categories_lasso_model_name,
  'Training Set R Squared': log_categories_lasso_rsquared_train, 
  'Test Set R Squared': log_categories_lasso_rsquared_test, 'Mean Absolute Error': log_categories_lasso_mae, 
  'Mean Sqaured Error': log_categories_lasso_mse, 'Root Mean Squared Error': log_categories_lasso_rmse,
  'Root Mean Squared Percentage Error': log_categories_lasso_rmspe,
  'Mean Absolute Percentage Error': log_categories_lasso_mape}

log_categories_lasso_df = pd.DataFrame(log_categories_lasso_dictionary, index=[0])

log_categories_lasso_df

"""### **yrs_exp_df log(y)**"""

#set features & target
X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

log_yrs_exp_lasso_grid = GridSearchCV(Lasso(), param_grid)

# fitting the model for grid search 
log_yrs_exp_lasso_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_yrs_exp_lasso_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_yrs_exp_lasso_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_lasso = Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,
      normalize=False, positive=False, precompute=False, random_state=None,
      selection='cyclic', tol=0.0001, warm_start=False)

log_yrs_exp_lasso.fit(X_train, y_train)

#cross validate
log_yrs_exp_lasso_scores = cross_val_score(log_yrs_exp_lasso, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_lasso_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_lasso_scores)))

#generate predictions
log_yrs_exp_lasso_preds = log_yrs_exp_lasso.predict(X_test)

#more performance metrics
log_yrs_exp_lasso_rsquared_train = log_yrs_exp_lasso.score(X_train, y_train)
log_yrs_exp_lasso_rsquared_test = log_yrs_exp_lasso.score(X_test, y_test)
log_yrs_exp_lasso_mae = mean_absolute_error(y_test, log_yrs_exp_lasso_preds)
log_yrs_exp_lasso_mse = mse(y_test, log_yrs_exp_lasso_preds)
log_yrs_exp_lasso_rmse = rmse(y_test, log_yrs_exp_lasso_preds)
log_yrs_exp_lasso_mape = np.mean(np.abs((y_test - log_yrs_exp_lasso_preds)/y_test)*100)
log_yrs_exp_lasso_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_lasso_preds) / y_test)))) * 100
log_yrs_exp_lasso_model_name = 'Years of Experience Lasso Regression Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_lasso_dictionary = {'Model': log_yrs_exp_lasso_model_name,
  'Training Set R Squared': log_yrs_exp_lasso_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_lasso_rsquared_test, 'Mean Absolute Error': log_yrs_exp_lasso_mae, 
  'Mean Sqaured Error': log_yrs_exp_lasso_mse, 'Root Mean Squared Error': log_yrs_exp_lasso_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_lasso_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_lasso_mape}

log_yrs_exp_lasso_df = pd.DataFrame(log_yrs_exp_lasso_dictionary, index=[0])

log_yrs_exp_lasso_df

"""### **cat_no_outliers_df**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

cat_no_outliers_lasso_grid = GridSearchCV(Lasso(), param_grid)

# fitting the model for grid search 
cat_no_outliers_lasso_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(cat_no_outliers_lasso_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(cat_no_outliers_lasso_grid.best_estimator_)

#set model with ideal hyperparameters from above
cat_no_outliers_lasso = Lasso(alpha=1, copy_X=True, fit_intercept=True, max_iter=1000,
      normalize=False, positive=False, precompute=False, random_state=None,
      selection='cyclic', tol=0.0001, warm_start=False)

cat_no_outliers_lasso.fit(X_train, y_train)

#cross validate
cat_no_outliers_lasso_scores = cross_val_score(cat_no_outliers_lasso, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', cat_no_outliers_lasso_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(cat_no_outliers_lasso_scores)))

#generate predictions
cat_no_outliers_lasso_preds = cat_no_outliers_lasso.predict(X_test)

#more performance metrics
cat_no_outliers_lasso_rsquared_train = cat_no_outliers_lasso.score(X_train, y_train)
cat_no_outliers_lasso_rsquared_test = cat_no_outliers_lasso.score(X_test, y_test)
cat_no_outliers_lasso_mae = mean_absolute_error(y_test, cat_no_outliers_lasso_preds)
cat_no_outliers_lasso_mse = mse(y_test, cat_no_outliers_lasso_preds)
cat_no_outliers_lasso_rmse = rmse(y_test, cat_no_outliers_lasso_preds)
cat_no_outliers_lasso_mape = np.mean(np.abs((y_test - cat_no_outliers_lasso_preds)/y_test)*100)
cat_no_outliers_lasso_rmspe = (np.sqrt(np.mean(np.square((y_test - cat_no_outliers_lasso_preds) / y_test)))) * 100
cat_no_outliers_lasso_model_name = 'Categories Lasso Regression'

#create easily readable display of performance metrics
cat_no_outliers_lasso_dictionary = {'Model': cat_no_outliers_lasso_model_name,
  'Training Set R Squared': cat_no_outliers_lasso_rsquared_train, 
  'Test Set R Squared': cat_no_outliers_lasso_rsquared_test, 'Mean Absolute Error': cat_no_outliers_lasso_mae, 
  'Mean Sqaured Error': cat_no_outliers_lasso_mse, 'Root Mean Squared Error': cat_no_outliers_lasso_rmse,
  'Root Mean Squared Percentage Error': cat_no_outliers_lasso_rmspe,
  'Mean Absolute Percentage Error': cat_no_outliers_lasso_mape}

cat_no_outliers_lasso_df = pd.DataFrame(cat_no_outliers_lasso_dictionary, index=[0])

cat_no_outliers_lasso_df

"""### **yrs_exp_no_outliers_df**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

yrs_exp_lasso_grid = GridSearchCV(Lasso(), param_grid)

# fitting the model for grid search 
yrs_exp_lasso_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(yrs_exp_lasso_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(yrs_exp_lasso_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_lasso = Lasso(alpha=10, copy_X=True, fit_intercept=True, max_iter=1000,
      normalize=False, positive=False, precompute=False, random_state=None,
      selection='cyclic', tol=0.0001, warm_start=False)

yrs_exp_lasso.fit(X_train, y_train)

#cross validate
yrs_exp_lasso_scores = cross_val_score(yrs_exp_lasso, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_lasso_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_lasso_scores)))

#generate predictions
yrs_exp_lasso_preds = yrs_exp_lasso.predict(X_test)

#more performance metrics
yrs_exp_lasso_rsquared_train = yrs_exp_lasso.score(X_train, y_train)
yrs_exp_lasso_rsquared_test = yrs_exp_lasso.score(X_test, y_test)
yrs_exp_lasso_mae = mean_absolute_error(y_test, yrs_exp_lasso_preds)
yrs_exp_lasso_mse = mse(y_test, yrs_exp_lasso_preds)
yrs_exp_lasso_rmse = rmse(y_test, yrs_exp_lasso_preds)
yrs_exp_lasso_mape = np.mean(np.abs((y_test - yrs_exp_lasso_preds)/y_test)*100)
yrs_exp_lasso_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_lasso_preds) / y_test)))) * 100
yrs_exp_lasso_model_name = 'Years of Experience w/o Outliers Lasso Regression'

#create easily readable display of performance metrics
yrs_exp_lasso_dictionary = {'Model': yrs_exp_lasso_model_name,
  'Training Set R Squared': yrs_exp_lasso_rsquared_train, 
  'Test Set R Squared': yrs_exp_lasso_rsquared_test, 'Mean Absolute Error': yrs_exp_lasso_mae, 
  'Mean Sqaured Error': yrs_exp_lasso_mse, 'Root Mean Squared Error': yrs_exp_lasso_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_lasso_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_lasso_mape}

yrs_exp_no_outliers_lasso_df = pd.DataFrame(yrs_exp_lasso_dictionary, index=[0])

yrs_exp_no_outliers_lasso_df

"""### **cat_no_outliers_df  log(y)**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

log_cat_no_outliers_lasso_grid = GridSearchCV(Lasso(), param_grid)

# fitting the model for grid search 
log_cat_no_outliers_lasso_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_cat_no_outliers_lasso_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_cat_no_outliers_lasso_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_cat_no_outliers_lasso = Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,
      normalize=False, positive=False, precompute=False, random_state=None,
      selection='cyclic', tol=0.0001, warm_start=False)

log_cat_no_outliers_lasso.fit(X_train, y_train)

#cross validate
log_cat_no_outliers_lasso_scores = cross_val_score(log_cat_no_outliers_lasso, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_cat_no_outliers_lasso_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_cat_no_outliers_lasso_scores)))

#generate predictions
log_cat_no_outliers_lasso_preds = log_cat_no_outliers_lasso.predict(X_test)

#more performance metrics
log_cat_no_outliers_lasso_rsquared_train = log_cat_no_outliers_lasso.score(X_train, y_train)
log_cat_no_outliers_lasso_rsquared_test = log_cat_no_outliers_lasso.score(X_test, y_test)
log_cat_no_outliers_lasso_mae = mean_absolute_error(y_test, log_cat_no_outliers_lasso_preds)
log_cat_no_outliers_lasso_mse = mse(y_test, log_cat_no_outliers_lasso_preds)
log_cat_no_outliers_lasso_rmse = rmse(y_test, log_cat_no_outliers_lasso_preds)
log_cat_no_outliers_lasso_mape = np.mean(np.abs((y_test - log_cat_no_outliers_lasso_preds)/y_test)*100)
log_cat_no_outliers_lasso_rmspe = (np.sqrt(np.mean(np.square((y_test - log_cat_no_outliers_lasso_preds) / y_test)))) * 100
log_cat_no_outliers_lasso_model_name = 'Categories w/o Outliers Lasso Regression Log(y)'

#create easily readable display of performance metrics
log_cat_no_outliers_lasso_dictionary = {'Model': log_cat_no_outliers_lasso_model_name,
  'Training Set R Squared': log_cat_no_outliers_lasso_rsquared_train, 
  'Test Set R Squared': log_cat_no_outliers_lasso_rsquared_test, 'Mean Absolute Error': log_cat_no_outliers_lasso_mae, 
  'Mean Sqaured Error': log_cat_no_outliers_lasso_mse, 'Root Mean Squared Error': log_cat_no_outliers_lasso_rmse,
  'Root Mean Squared Percentage Error': log_cat_no_outliers_lasso_rmspe,
  'Mean Absolute Percentage Error': log_cat_no_outliers_lasso_mape}

log_cat_no_outliers_lasso_df = pd.DataFrame(log_cat_no_outliers_lasso_dictionary, index=[0])

log_cat_no_outliers_lasso_df

"""### **yrs_exp_no_outliers_df log(y)**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]} 

log_yrs_exp_no_outliers_lasso_grid = GridSearchCV(Lasso(), param_grid)

# fitting the model for grid search 
log_yrs_exp_no_outliers_lasso_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_yrs_exp_no_outliers_lasso_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_yrs_exp_no_outliers_lasso_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_no_outliers_lasso = Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,
      normalize=False, positive=False, precompute=False, random_state=None,
      selection='cyclic', tol=0.0001, warm_start=False)

log_yrs_exp_no_outliers_lasso.fit(X_train, y_train)

#cross validate
log_yrs_exp_no_outliers_lasso_scores = cross_val_score(log_yrs_exp_no_outliers_lasso, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_no_outliers_lasso_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_no_outliers_lasso_scores)))

#generate predictions
log_yrs_exp_no_outliers_lasso_preds = log_yrs_exp_no_outliers_lasso.predict(X_test)

#more performance metrics
log_yrs_exp_no_outliers_lasso_rsquared_train = log_yrs_exp_no_outliers_lasso.score(X_train, y_train)
log_yrs_exp_no_outliers_lasso_rsquared_test = log_yrs_exp_no_outliers_lasso.score(X_test, y_test)
log_yrs_exp_no_outliers_lasso_mae = mean_absolute_error(y_test, log_yrs_exp_no_outliers_lasso_preds)
log_yrs_exp_no_outliers_lasso_mse = mse(y_test, log_yrs_exp_no_outliers_lasso_preds)
log_yrs_exp_no_outliers_lasso_rmse = rmse(y_test, log_yrs_exp_no_outliers_lasso_preds)
log_yrs_exp_no_outliers_lasso_mape = np.mean(np.abs((y_test - log_yrs_exp_no_outliers_lasso_preds)/y_test)*100)
log_yrs_exp_no_outliers_lasso_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_no_outliers_lasso_preds) / y_test)))) * 100
log_yrs_exp_no_outliers_lasso_model_name = 'Years of Experience w/o Outliers Lasso Regression Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_no_outliers_lasso_dictionary = {'Model': log_yrs_exp_no_outliers_lasso_model_name,
  'Training Set R Squared': log_yrs_exp_no_outliers_lasso_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_no_outliers_lasso_rsquared_test, 'Mean Absolute Error': log_yrs_exp_no_outliers_lasso_mae, 
  'Mean Sqaured Error': log_yrs_exp_no_outliers_lasso_mse, 'Root Mean Squared Error': log_yrs_exp_no_outliers_lasso_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_no_outliers_lasso_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_no_outliers_lasso_mape}

log_yrs_exp_no_outliers_lasso_df = pd.DataFrame(log_yrs_exp_no_outliers_lasso_dictionary, index=[0])

log_yrs_exp_no_outliers_lasso_df

"""## **ElasticNet Regression**

### **categories_df**
"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  
              'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]} 

categories_enet_grid = GridSearchCV(ElasticNet(), param_grid, scoring='r2', 
                                    verbose=1, n_jobs=-1)

# fitting the model for grid search 
categories_enet_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(categories_enet_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(categories_enet_grid.best_estimator_)

#set model with ideal hyperparameters from above
categories_enet = ElasticNet(alpha=0.01, copy_X=True, fit_intercept=True, l1_ratio=0.7,
           max_iter=1000, normalize=False, positive=False, precompute=False,
           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)

categories_enet.fit(X_train, y_train)

#cross validate
categories_enet_scores = cross_val_score(categories_enet, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', categories_enet_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(categories_enet_scores)))

#generate predictions
categories_enet_preds = categories_enet.predict(X_test)

#more performance metrics
categories_enet_rsquared_train = categories_enet.score(X_train, y_train)
categories_enet_rsquared_test = categories_enet.score(X_test, y_test)
categories_enet_mae = mean_absolute_error(y_test, categories_enet_preds)
categories_enet_mse = mse(y_test, categories_enet_preds)
categories_enet_rmse = rmse(y_test, categories_enet_preds)
categories_enet_mape = np.mean(np.abs((y_test - categories_enet_preds)/y_test)*100)
categories_enet_rmspe = (np.sqrt(np.mean(np.square((y_test - categories_enet_preds) / y_test)))) * 100
categories_enet_model_name = 'Categories Elastic Net Regression'

#create easily readable display of performance metrics
categories_enet_dictionary = {'Model': categories_enet_model_name,
  'Training Set R Squared': categories_enet_rsquared_train, 
  'Test Set R Squared': categories_enet_rsquared_test, 'Mean Absolute Error': categories_enet_mae, 
  'Mean Sqaured Error': categories_enet_mse, 'Root Mean Squared Error': categories_enet_rmse,
  'Root Mean Squared Percentage Error': categories_enet_rmspe,
  'Mean Absolute Percentage Error': categories_enet_mape}

categories_enet_df = pd.DataFrame(categories_enet_dictionary, index=[0])

categories_enet_df

"""### **yrs_exp_df**"""

#set features & target
X= yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  
              'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]} 

yrs_exp_enet_grid = GridSearchCV(ElasticNet(), param_grid, scoring='r2', 
                                    verbose=1, n_jobs=-1)

# fitting the model for grid search 
yrs_exp_enet_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(yrs_exp_enet_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(yrs_exp_enet_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_enet = ElasticNet(alpha=100, copy_X=True, fit_intercept=True, l1_ratio=1,
           max_iter=1000, normalize=False, positive=False, precompute=False,
           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)

yrs_exp_enet.fit(X_train, y_train)

#cross validate
yrs_exp_enet_scores = cross_val_score(yrs_exp_enet, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_enet_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_enet_scores)))

#generate predictions
yrs_exp_enet_preds = yrs_exp_enet.predict(X_test)

#more performance metrics
yrs_exp_enet_rsquared_train = yrs_exp_enet.score(X_train, y_train)
yrs_exp_enet_rsquared_test = yrs_exp_enet.score(X_test, y_test)
yrs_exp_enet_mae = mean_absolute_error(y_test, yrs_exp_enet_preds)
yrs_exp_enet_mse = mse(y_test, yrs_exp_enet_preds)
yrs_exp_enet_rmse = rmse(y_test, yrs_exp_enet_preds)
yrs_exp_enet_mape = np.mean(np.abs((y_test - yrs_exp_enet_preds)/y_test)*100)
yrs_exp_enet_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_enet_preds) / y_test)))) * 100
yrs_exp_enet_model_name = 'Years of Experience Elastic Net Regression'

#create easily readable display of performance metrics
yrs_exp_enet_dictionary = {'Model': yrs_exp_enet_model_name,
  'Training Set R Squared': yrs_exp_enet_rsquared_train, 
  'Test Set R Squared': yrs_exp_enet_rsquared_test, 'Mean Absolute Error': yrs_exp_enet_mae, 
  'Mean Sqaured Error': yrs_exp_enet_mse, 'Root Mean Squared Error': yrs_exp_enet_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_enet_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_enet_mape}

yrs_exp_enet_df = pd.DataFrame(yrs_exp_enet_dictionary, index=[0])

yrs_exp_enet_df

"""### **categories_df log(y)**"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  
              'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]} 

log_categories_enet_grid = GridSearchCV(ElasticNet(), param_grid, scoring='r2', 
                                    verbose=1, n_jobs=-1)

# fitting the model for grid search 
log_categories_enet_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_categories_enet_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_categories_enet_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_categories_enet = ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.3,
           max_iter=1000, normalize=False, positive=False, precompute=False,
           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)

log_categories_enet.fit(X_train, y_train)

#cross validate
log_categories_enet_scores = cross_val_score(log_categories_enet, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_categories_enet_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_categories_enet_scores)))

#generate predictions
log_categories_enet_preds = log_categories_enet.predict(X_test)

#more performance metrics
log_categories_enet_rsquared_train = log_categories_enet.score(X_train, y_train)
log_categories_enet_rsquared_test = log_categories_enet.score(X_test, y_test)
log_categories_enet_mae = mean_absolute_error(y_test, log_categories_enet_preds)
log_categories_enet_mse = mse(y_test, log_categories_enet_preds)
log_categories_enet_rmse = rmse(y_test, log_categories_enet_preds)
log_categories_enet_mape = np.mean(np.abs((y_test - log_categories_enet_preds)/y_test)*100)
log_categories_enet_rmspe = (np.sqrt(np.mean(np.square((y_test - log_categories_enet_preds) / y_test)))) * 100
log_categories_enet_model_name = 'Categories Elastic Net Regression Log(y)'

#create easily readable display of performance metrics
log_categories_enet_dictionary = {'Model': log_categories_enet_model_name,
  'Training Set R Squared': log_categories_enet_rsquared_train, 
  'Test Set R Squared': log_categories_enet_rsquared_test, 'Mean Absolute Error': log_categories_enet_mae, 
  'Mean Sqaured Error': log_categories_enet_mse, 'Root Mean Squared Error': log_categories_enet_rmse,
  'Root Mean Squared Percentage Error': log_categories_enet_rmspe,
  'Mean Absolute Percentage Error': log_categories_enet_mape}

log_categories_enet_df = pd.DataFrame(log_categories_enet_dictionary, index=[0])

log_categories_enet_df

"""### **yrs_exp_df log(y)**"""

#set features & target
X= yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  
              'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]} 

log_yrs_exp_enet_grid = GridSearchCV(ElasticNet(), param_grid, scoring='r2', 
                                    verbose=1, n_jobs=-1)

# fitting the model for grid search 
log_yrs_exp_enet_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_yrs_exp_enet_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_yrs_exp_enet_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_enet = ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0,
           max_iter=1000, normalize=False, positive=False, precompute=False,
           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)

log_yrs_exp_enet.fit(X_train, y_train)

#cross validate
log_yrs_exp_enet_scores = cross_val_score(log_yrs_exp_enet, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_enet_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_enet_scores)))

#generate predictions
log_yrs_exp_enet_preds = log_yrs_exp_enet.predict(X_test)

#more performance metrics
log_yrs_exp_enet_rsquared_train = log_yrs_exp_enet.score(X_train, y_train)
log_yrs_exp_enet_rsquared_test = log_yrs_exp_enet.score(X_test, y_test)
log_yrs_exp_enet_mae = mean_absolute_error(y_test, log_yrs_exp_enet_preds)
log_yrs_exp_enet_mse = mse(y_test, log_yrs_exp_enet_preds)
log_yrs_exp_enet_rmse = rmse(y_test, log_yrs_exp_enet_preds)
log_yrs_exp_enet_mape = np.mean(np.abs((y_test - log_yrs_exp_enet_preds)/y_test)*100)
log_yrs_exp_enet_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_enet_preds) / y_test)))) * 100
log_yrs_exp_enet_model_name = 'Years of Experience Elastic Net Regression Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_enet_dictionary = {'Model': log_yrs_exp_enet_model_name,
  'Training Set R Squared': log_yrs_exp_enet_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_enet_rsquared_test, 'Mean Absolute Error': log_yrs_exp_enet_mae, 
  'Mean Sqaured Error': log_yrs_exp_enet_mse, 'Root Mean Squared Error': log_yrs_exp_enet_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_enet_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_enet_mape}

log_yrs_exp_enet_df = pd.DataFrame(log_yrs_exp_enet_dictionary, index=[0])

log_yrs_exp_enet_df

"""### **cat_no_outliers_df**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  
              'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]} 

cat_no_outliers_enet_grid = GridSearchCV(ElasticNet(), param_grid, scoring='r2', 
                                    verbose=1, n_jobs=-1)

# fitting the model for grid search 
cat_no_outliers_enet_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(cat_no_outliers_enet_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(cat_no_outliers_enet_grid.best_estimator_)

#set model with ideal hyperparameters from above
cat_no_outliers_enet = ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.1,
           max_iter=1000, normalize=False, positive=False, precompute=False,
           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)

cat_no_outliers_enet.fit(X_train, y_train)

#cross validate
cat_no_outliers_enet_scores = cross_val_score(cat_no_outliers_enet, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', cat_no_outliers_enet_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(cat_no_outliers_enet_scores)))

#generate predictions
cat_no_outliers_enet_preds = cat_no_outliers_enet.predict(X_test)

#more performance metrics
cat_no_outliers_enet_rsquared_train = cat_no_outliers_enet.score(X_train, y_train)
cat_no_outliers_enet_rsquared_test = cat_no_outliers_enet.score(X_test, y_test)
cat_no_outliers_enet_mae = mean_absolute_error(y_test, cat_no_outliers_enet_preds)
cat_no_outliers_enet_mse = mse(y_test, cat_no_outliers_enet_preds)
cat_no_outliers_enet_rmse = rmse(y_test, cat_no_outliers_enet_preds)
cat_no_outliers_enet_mape = np.mean(np.abs((y_test - cat_no_outliers_enet_preds)/y_test)*100)
cat_no_outliers_enet_rmspe = (np.sqrt(np.mean(np.square((y_test - cat_no_outliers_enet_preds) / y_test)))) * 100
cat_no_outliers_enet_model_name = 'Categories w/o Outliers Elastic Net Regression'

#create easily readable display of performance metrics
cat_no_outliers_enet_dictionary = {'Model': cat_no_outliers_enet_model_name,
  'Training Set R Squared': cat_no_outliers_enet_rsquared_train, 
  'Test Set R Squared': cat_no_outliers_enet_rsquared_test, 'Mean Absolute Error': cat_no_outliers_enet_mae, 
  'Mean Sqaured Error': cat_no_outliers_enet_mse, 'Root Mean Squared Error': cat_no_outliers_enet_rmse,
  'Root Mean Squared Percentage Error': cat_no_outliers_enet_rmspe,
  'Mean Absolute Percentage Error': cat_no_outliers_enet_mape}

cat_no_outliers_enet_df = pd.DataFrame(cat_no_outliers_enet_dictionary, index=[0])

cat_no_outliers_enet_df

"""### **yrs_exp_no_outliers_df**"""

#set features & target
X= yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  
              'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]} 

yrs_exp_no_outliers_enet_grid = GridSearchCV(ElasticNet(), param_grid, scoring='r2', 
                                    verbose=1, n_jobs=-1)

# fitting the model for grid search 
yrs_exp_no_outliers_enet_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(yrs_exp_no_outliers_enet_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(yrs_exp_no_outliers_enet_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_no_outliers_enet = ElasticNet(alpha=0.01, copy_X=True, fit_intercept=True, l1_ratio=0.7,
           max_iter=1000, normalize=False, positive=False, precompute=False,
           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)

yrs_exp_no_outliers_enet.fit(X_train, y_train)

#cross validate
yrs_exp_no_outliers_enet_scores = cross_val_score(yrs_exp_no_outliers_enet, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_no_outliers_enet_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_no_outliers_enet_scores)))

#generate predictions
yrs_exp_no_outliers_enet_preds = yrs_exp_no_outliers_enet.predict(X_test)

#more performance metrics
yrs_exp_no_outliers_enet_rsquared_train = yrs_exp_no_outliers_enet.score(X_train, y_train)
yrs_exp_no_outliers_enet_rsquared_test = yrs_exp_no_outliers_enet.score(X_test, y_test)
yrs_exp_no_outliers_enet_mae = mean_absolute_error(y_test, yrs_exp_no_outliers_enet_preds)
yrs_exp_no_outliers_enet_mse = mse(y_test, yrs_exp_no_outliers_enet_preds)
yrs_exp_no_outliers_enet_rmse = rmse(y_test, yrs_exp_no_outliers_enet_preds)
yrs_exp_no_outliers_enet_mape = np.mean(np.abs((y_test - yrs_exp_no_outliers_enet_preds)/y_test)*100)
yrs_exp_no_outliers_enet_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_no_outliers_enet_preds) / y_test)))) * 100
yrs_exp_no_outliers_enet_model_name = 'Years of Experience w/o Outliers Elastic Net Regression'

#create easily readable display of performance metrics
yrs_exp_no_outliers_enet_dictionary = {'Model': yrs_exp_no_outliers_enet_model_name,
  'Training Set R Squared': yrs_exp_no_outliers_enet_rsquared_train, 
  'Test Set R Squared': yrs_exp_no_outliers_enet_rsquared_test, 'Mean Absolute Error': yrs_exp_no_outliers_enet_mae, 
  'Mean Sqaured Error': yrs_exp_no_outliers_enet_mse, 'Root Mean Squared Error': yrs_exp_no_outliers_enet_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_no_outliers_enet_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_no_outliers_enet_mape}

yrs_exp_no_outliers_enet_df = pd.DataFrame(yrs_exp_no_outliers_enet_dictionary, index=[0])

yrs_exp_no_outliers_enet_df

"""### **cat_no_outliers_df log(y)**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  
              'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]} 

log_cat_no_outliers_enet_grid = GridSearchCV(ElasticNet(), param_grid, scoring='r2', 
                                    verbose=1, n_jobs=-1)

# fitting the model for grid search 
log_cat_no_outliers_enet_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_cat_no_outliers_enet_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_cat_no_outliers_enet_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_cat_no_outliers_enet = ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0,
           max_iter=1000, normalize=False, positive=False, precompute=False,
           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)

log_cat_no_outliers_enet.fit(X_train, y_train)

#cross validate
log_cat_no_outliers_enet_scores = cross_val_score(log_cat_no_outliers_enet, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_cat_no_outliers_enet_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_cat_no_outliers_enet_scores)))

#generate predictions
log_cat_no_outliers_enet_preds = log_cat_no_outliers_enet.predict(X_test)

#more performance metrics
log_cat_no_outliers_enet_rsquared_train = log_cat_no_outliers_enet.score(X_train, y_train)
log_cat_no_outliers_enet_rsquared_test = log_cat_no_outliers_enet.score(X_test, y_test)
log_cat_no_outliers_enet_mae = mean_absolute_error(y_test, log_cat_no_outliers_enet_preds)
log_cat_no_outliers_enet_mse = mse(y_test, log_cat_no_outliers_enet_preds)
log_cat_no_outliers_enet_rmse = rmse(y_test, log_cat_no_outliers_enet_preds)
log_cat_no_outliers_enet_mape = np.mean(np.abs((y_test - log_cat_no_outliers_enet_preds)/y_test)*100)
log_cat_no_outliers_enet_rmspe = (np.sqrt(np.mean(np.square((y_test - log_cat_no_outliers_enet_preds) / y_test)))) * 100
log_cat_no_outliers_enet_model_name = 'Categories w/o Outliers Elastic Net Regression Log(y)'

#create easily readable display of performance metrics
log_cat_no_outliers_enet_dictionary = {'Model': log_cat_no_outliers_enet_model_name,
  'Training Set R Squared': log_cat_no_outliers_enet_rsquared_train, 
  'Test Set R Squared': log_cat_no_outliers_enet_rsquared_test, 'Mean Absolute Error': log_cat_no_outliers_enet_mae, 
  'Mean Sqaured Error': log_cat_no_outliers_enet_mse, 'Root Mean Squared Error': log_cat_no_outliers_enet_rmse,
  'Root Mean Squared Percentage Error': log_cat_no_outliers_enet_rmspe,
  'Mean Absolute Percentage Error': log_cat_no_outliers_enet_mape}

log_cat_no_outliers_enet_df = pd.DataFrame(log_cat_no_outliers_enet_dictionary, index=[0])

log_cat_no_outliers_enet_df

"""### **yrs_exp_no_outliers_df log(y)**"""

#set features & target
X= yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000],  
              'l1_ratio': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]} 

log_yrs_exp_no_outliers_enet_grid = GridSearchCV(ElasticNet(), param_grid, scoring='r2', 
                                    verbose=1, n_jobs=-1)

# fitting the model for grid search 
log_yrs_exp_no_outliers_enet_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_yrs_exp_no_outliers_enet_grid.best_params_)

# print how our model looks after hyper-parameter tuning 
print(log_yrs_exp_no_outliers_enet_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_no_outliers_enet = ElasticNet(alpha=0.001, copy_X=True, fit_intercept=True, l1_ratio=0.6,
           max_iter=1000, normalize=False, positive=False, precompute=False,
           random_state=None, selection='cyclic', tol=0.0001, warm_start=False)

log_yrs_exp_no_outliers_enet.fit(X_train, y_train)

#cross validate
log_yrs_exp_no_outliers_enet_scores = cross_val_score(log_yrs_exp_no_outliers_enet, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_no_outliers_enet_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_no_outliers_enet_scores)))

#generate predictions
log_yrs_exp_no_outliers_enet_preds = log_yrs_exp_no_outliers_enet.predict(X_test)

#more performance metrics
log_yrs_exp_no_outliers_enet_rsquared_train = log_yrs_exp_no_outliers_enet.score(X_train, y_train)
log_yrs_exp_no_outliers_enet_rsquared_test = log_yrs_exp_no_outliers_enet.score(X_test, y_test)
log_yrs_exp_no_outliers_enet_mae = mean_absolute_error(y_test, log_yrs_exp_no_outliers_enet_preds)
log_yrs_exp_no_outliers_enet_mse = mse(y_test, log_yrs_exp_no_outliers_enet_preds)
log_yrs_exp_no_outliers_enet_rmse = rmse(y_test, log_yrs_exp_no_outliers_enet_preds)
log_yrs_exp_no_outliers_enet_mape = np.mean(np.abs((y_test - log_yrs_exp_no_outliers_enet_preds)/y_test)*100)
log_yrs_exp_no_outliers_enet_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_no_outliers_enet_preds) / y_test)))) * 100
log_yrs_exp_no_outliers_enet_model_name = 'Years of Experience Elastic Net Regression Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_no_outliers_enet_dictionary = {'Model': log_yrs_exp_no_outliers_enet_model_name,
  'Training Set R Squared': log_yrs_exp_no_outliers_enet_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_no_outliers_enet_rsquared_test, 'Mean Absolute Error': log_yrs_exp_no_outliers_enet_mae, 
  'Mean Sqaured Error': log_yrs_exp_no_outliers_enet_mse, 'Root Mean Squared Error': log_yrs_exp_no_outliers_enet_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_no_outliers_enet_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_no_outliers_enet_mape}

log_yrs_exp_no_outliers_enet_df = pd.DataFrame(log_yrs_exp_no_outliers_enet_dictionary, index=[0])

log_yrs_exp_no_outliers_enet_df

"""## **K Nearest Neighbors**

### **categories_df**
"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'n_neighbors': [2, 4, 6, 8, 10, 12, 15],  
              'weights': ['uniform', 'distance'], 
              'metric': ['euclidean', 'manhattan']}  
  
categories_knn_grid = GridSearchCV(KNeighborsRegressor(), param_grid, refit = True, verbose=1) 
  
# fitting the model for grid search 
categories_knn_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(categories_knn_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(categories_knn_grid.best_estimator_)

#validate best value for k
rmse_val = [] #to store rmse values for different k
for K in range (20):
  K = K+1
  model = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=K, p=2,
                    weights='distance')
  model.fit(X_train, y_train) #fit the model
  preds = model.predict(X_test) #make predictions on test set
  error = rmse(y_test, preds) #calculate rmse
  rmse_val.append(error) #store rmse values
  print('RMSe Value for K= ', K, 'is ', error)

#plotting the rmse values (y) against k values (x)
curve = pd.DataFrame(rmse_val) #elbow curve 
curve.plot()

#set model with ideal hyperparameters from above
categories_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=12, p=2,
                    weights='distance')
categories_knn.fit(X_train, y_train)

#cross validate
categories_knn_scores = cross_val_score(categories_knn, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', categories_knn_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(categories_knn_scores)))

#generate predictions
categories_knn_preds = categories_knn.predict(X_test)

#more performance metrics
categories_knn_rsquared_train = categories_knn.score(X_train, y_train)
categories_knn_rsquared_test = categories_knn.score(X_test, y_test)
categories_knn_mae = mean_absolute_error(y_test, categories_knn_preds)
categories_knn_mse = mse(y_test, categories_knn_preds)
categories_knn_rmse = rmse(y_test, categories_knn_preds)
categories_knn_mape = np.mean(np.abs((y_test - categories_knn_preds)/y_test)*100)
categories_knn_rmspe = (np.sqrt(np.mean(np.square((y_test - categories_knn_preds) / y_test)))) * 100
categories_knn_model_name = 'Categories KNN'

#create easily readable display of performance metrics
categories_knn_dictionary = {'Model': categories_knn_model_name,
  'Training Set R Squared': categories_knn_rsquared_train, 
  'Test Set R Squared': categories_knn_rsquared_test, 'Mean Absolute Error': categories_knn_mae, 
  'Mean Sqaured Error': categories_knn_mse, 'Root Mean Squared Error': categories_knn_rmse,
  'Root Mean Squared Percentage Error': categories_knn_rmspe,
  'Mean Absolute Percentage Error': categories_knn_mape}

categories_knn_df = pd.DataFrame(categories_knn_dictionary, index=[0])

categories_knn_df

"""### **yrs_exp_df**"""

#set features & target
X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'n_neighbors': [2, 4, 6, 8, 10, 12, 15],  
              'weights': ['uniform', 'distance'], 
              'metric': ['euclidean', 'manhattan']}  
  
years_exp_knn_grid = GridSearchCV(KNeighborsRegressor(), param_grid, refit = True, verbose=1) 
  
# fitting the model for grid search 
years_exp_knn_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(years_exp_knn_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(years_exp_knn_grid.best_estimator_)

#validate best value for k
rmse_val = [] #to store rmse values for different k
for K in range (20):
  K = K+1
  model = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=K, p=2,
                    weights='distance')
  model.fit(X_train, y_train) #fit the model
  preds = model.predict(X_test) #make predictions on test set
  error = rmse(y_test, preds) #calculate rmse
  rmse_val.append(error) #store rmse values
  print('RMSe Value for K= ', K, 'is ', error)

#plotting the rmse values (y) against k values (x)
curve = pd.DataFrame(rmse_val) #elbow curve 
curve.plot()

#set model with ideal hyperparameters from above
years_exp_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=15, p=2,
                    weights='distance')
years_exp_knn.fit(X_train, y_train)

#cross validate
years_exp_knn_scores = cross_val_score(years_exp_knn, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', years_exp_knn_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(years_exp_knn_scores)))

#generate predictions
years_exp_knn_preds = years_exp_knn.predict(X_test)

#more performance metrics
years_exp_knn_rsquared_train = years_exp_knn.score(X_train, y_train)
years_exp_knn_rsquared_test = years_exp_knn.score(X_test, y_test)
years_exp_knn_mae = mean_absolute_error(y_test, years_exp_knn_preds)
years_exp_knn_mse = mse(y_test, years_exp_knn_preds)
years_exp_knn_rmse = rmse(y_test, years_exp_knn_preds)
years_exp_knn_mape = np.mean(np.abs((y_test - years_exp_knn_preds)/y_test)*100)
years_exp_knn_rmspe = (np.sqrt(np.mean(np.square((y_test - years_exp_knn_preds) / y_test)))) * 100
years_exp_knn_model_name = 'Years of Experience KNN'

#create easily readable display of performance metrics
years_exp_knn_dictionary = {'Model': years_exp_knn_model_name,
  'Training Set R Squared': years_exp_knn_rsquared_train, 
  'Test Set R Squared': years_exp_knn_rsquared_test, 'Mean Absolute Error': years_exp_knn_mae, 
  'Mean Sqaured Error': years_exp_knn_mse, 'Root Mean Squared Error': years_exp_knn_rmse,
  'Root Mean Squared Percentage Error': years_exp_knn_rmspe,
  'Mean Absolute Percentage Error': years_exp_knn_mape}

years_exp_knn_df = pd.DataFrame(years_exp_knn_dictionary, index=[0])

years_exp_knn_df

"""### **categories_df log(y)**"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'n_neighbors': [2, 4, 6, 8, 10, 12, 15],  
              'weights': ['uniform', 'distance'], 
              'metric': ['euclidean', 'manhattan']}  
  
log_categories_knn_grid = GridSearchCV(KNeighborsRegressor(), param_grid, refit = True, verbose=1) 
  
# fitting the model for grid search 
log_categories_knn_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_categories_knn_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_categories_knn_grid.best_estimator_)

#validate best value for k
rmse_val = [] #to store rmse values for different k
for K in range (20):
  K = K+1
  model = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=K, p=2,
                    weights='distance')
  model.fit(X_train, y_train) #fit the model
  preds = model.predict(X_test) #make predictions on test set
  error = rmse(y_test, preds) #calculate rmse
  rmse_val.append(error) #store rmse values
  print('RMSe Value for K= ', K, 'is ', error)

#plotting the rmse values (y) against k values (x)
curve = pd.DataFrame(rmse_val) #elbow curve 
curve.plot()

#set model with ideal hyperparameters from above
log_categories_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=15, p=2,
                    weights='distance')
log_categories_knn.fit(X_train, y_train)

#cross validate
log_categories_knn_scores = cross_val_score(log_categories_knn, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_categories_knn_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_categories_knn_scores)))

#generate predictions
log_categories_knn_preds = log_categories_knn.predict(X_test)

#more performance metrics
log_categories_knn_rsquared_train = log_categories_knn.score(X_train, y_train)
log_categories_knn_rsquared_test = log_categories_knn.score(X_test, y_test)
log_categories_knn_mae = mean_absolute_error(y_test, log_categories_knn_preds)
log_categories_knn_mse = mse(y_test, log_categories_knn_preds)
log_categories_knn_rmse = rmse(y_test, log_categories_knn_preds)
log_categories_knn_mape = np.mean(np.abs((y_test - log_categories_knn_preds)/y_test)*100)
log_categories_knn_rmspe = (np.sqrt(np.mean(np.square((y_test - log_categories_knn_preds) / y_test)))) * 100
log_categories_knn_model_name = 'Categories KNN Log(y)'

#create easily readable display of performance metrics
log_categories_knn_dictionary = {'Model': log_categories_knn_model_name,
  'Training Set R Squared': log_categories_knn_rsquared_train, 
  'Test Set R Squared': log_categories_knn_rsquared_test, 'Mean Absolute Error': log_categories_knn_mae, 
  'Mean Sqaured Error': log_categories_knn_mse, 'Root Mean Squared Error': log_categories_knn_rmse,
  'Root Mean Squared Percentage Error': log_categories_knn_rmspe,
  'Mean Absolute Percentage Error': log_categories_knn_mape}

log_categories_knn_df = pd.DataFrame(log_categories_knn_dictionary, index=[0])

log_categories_knn_df

"""### **yrs_exp_df log(y)**"""

#set features & target
X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'n_neighbors': [2, 4, 6, 8, 10, 12, 15],  
              'weights': ['uniform', 'distance'], 
              'metric': ['euclidean', 'manhattan']}  
  
log_years_exp_knn_grid = GridSearchCV(KNeighborsRegressor(), param_grid, refit = True, verbose=1) 
  
# fitting the model for grid search 
log_years_exp_knn_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_years_exp_knn_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_years_exp_knn_grid.best_estimator_)

#validate best value for k
rmse_val = [] #to store rmse values for different k
for K in range (20):
  K = K+1
  model = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=K, p=2,
                    weights='distance')
  model.fit(X_train, y_train) #fit the model
  preds = model.predict(X_test) #make predictions on test set
  error = rmse(y_test, preds) #calculate rmse
  rmse_val.append(error) #store rmse values
  print('RMSe Value for K= ', K, 'is ', error)

#plotting the rmse values (y) against k values (x)
curve = pd.DataFrame(rmse_val) #elbow curve 
curve.plot()

#set model with ideal hyperparameters from above
log_years_exp_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=15, p=2,
                    weights='distance')
log_years_exp_knn.fit(X_train, y_train)

#cross validate
log_years_exp_knn_scores = cross_val_score(log_years_exp_knn, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_years_exp_knn_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_years_exp_knn_scores)))

#generate predictions
log_years_exp_knn_preds = log_years_exp_knn.predict(X_test)

#more performance metrics
log_years_exp_knn_rsquared_train = log_years_exp_knn.score(X_train, y_train)
log_years_exp_knn_rsquared_test = log_years_exp_knn.score(X_test, y_test)
log_years_exp_knn_mae = mean_absolute_error(y_test, log_years_exp_knn_preds)
log_years_exp_knn_mse = mse(y_test, log_years_exp_knn_preds)
log_years_exp_knn_rmse = rmse(y_test, log_years_exp_knn_preds)
log_years_exp_knn_mape = np.mean(np.abs((y_test - log_years_exp_knn_preds)/y_test)*100)
log_years_exp_knn_rmspe = (np.sqrt(np.mean(np.square((y_test - log_years_exp_knn_preds) / y_test)))) * 100
log_years_exp_knn_model_name = 'Years of Experience KNN Log(y)'

#create easily readable display of performance metrics
log_years_exp_knn_dictionary = {'Model': log_years_exp_knn_model_name,
  'Training Set R Squared': log_years_exp_knn_rsquared_train, 
  'Test Set R Squared': log_years_exp_knn_rsquared_test, 'Mean Absolute Error': log_years_exp_knn_mae, 
  'Mean Sqaured Error': log_years_exp_knn_mse, 'Root Mean Squared Error': log_years_exp_knn_rmse,
  'Root Mean Squared Percentage Error': log_years_exp_knn_rmspe,
  'Mean Absolute Percentage Error': log_years_exp_knn_mape}

log_years_exp_knn_df = pd.DataFrame(log_years_exp_knn_dictionary, index=[0])

log_years_exp_knn_df

"""### **cat_no_outliers_df**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'n_neighbors': [2, 4, 6, 8, 10, 12, 15],  
              'weights': ['uniform', 'distance'], 
              'metric': ['euclidean', 'manhattan']}  
  
cat_no_outliers_knn_grid = GridSearchCV(KNeighborsRegressor(), param_grid, refit = True, verbose=1) 
  
# fitting the model for grid search 
cat_no_outliers_knn_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(cat_no_outliers_knn_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(cat_no_outliers_knn_grid.best_estimator_)

#validate best value for k
rmse_val = [] #to store rmse values for different k
for K in range (20):
  K = K+1
  model = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=K, p=2,
                    weights='distance')
  model.fit(X_train, y_train) #fit the model
  preds = model.predict(X_test) #make predictions on test set
  error = rmse(y_test, preds) #calculate rmse
  rmse_val.append(error) #store rmse values
  print('RMSe Value for K= ', K, 'is ', error)

#plotting the rmse values (y) against k values (x)
curve = pd.DataFrame(rmse_val) #elbow curve 
curve.plot()

#set model with ideal hyperparameters from above
cat_no_outliers_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=15, p=2,
                    weights='distance')
cat_no_outliers_knn.fit(X_train, y_train)

#cross validate
cat_no_outliers_knn_scores = cross_val_score(cat_no_outliers_knn, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', cat_no_outliers_knn_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(cat_no_outliers_knn_scores)))

#generate predictions
cat_no_outliers_knn_preds = cat_no_outliers_knn.predict(X_test)

#more performance metrics
cat_no_outliers_knn_rsquared_train = cat_no_outliers_knn.score(X_train, y_train)
cat_no_outliers_knn_rsquared_test = cat_no_outliers_knn.score(X_test, y_test)
cat_no_outliers_knn_mae = mean_absolute_error(y_test, cat_no_outliers_knn_preds)
cat_no_outliers_knn_mse = mse(y_test, cat_no_outliers_knn_preds)
cat_no_outliers_knn_rmse = rmse(y_test, cat_no_outliers_knn_preds)
cat_no_outliers_knn_mape = np.mean(np.abs((y_test - cat_no_outliers_knn_preds)/y_test)*100)
cat_no_outliers_knn_rmspe = (np.sqrt(np.mean(np.square((y_test - cat_no_outliers_knn_preds) / y_test)))) * 100
cat_no_outliers_knn_model_name = 'Categories w/o Outliers KNN'

#create easily readable display of performance metrics
cat_no_outliers_knn_dictionary = {'Model': cat_no_outliers_knn_model_name,
  'Training Set R Squared': cat_no_outliers_knn_rsquared_train, 
  'Test Set R Squared': cat_no_outliers_knn_rsquared_test, 'Mean Absolute Error': cat_no_outliers_knn_mae, 
  'Mean Sqaured Error': cat_no_outliers_knn_mse, 'Root Mean Squared Error': cat_no_outliers_knn_rmse,
  'Root Mean Squared Percentage Error': cat_no_outliers_knn_rmspe,
  'Mean Absolute Percentage Error': cat_no_outliers_knn_mape}

cat_no_outliers_knn_df = pd.DataFrame(cat_no_outliers_knn_dictionary, index=[0])

cat_no_outliers_knn_df

"""### **yrs_exp_no_outliers_df**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'n_neighbors': [2, 4, 6, 8, 10, 12, 15],  
              'weights': ['uniform', 'distance'], 
              'metric': ['euclidean', 'manhattan']}  
  
years_exp_no_outliers_knn_grid = GridSearchCV(KNeighborsRegressor(), param_grid, refit = True, verbose=1) 
  
# fitting the model for grid search 
years_exp_no_outliers_knn_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(years_exp_no_outliers_knn_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(years_exp_no_outliers_knn_grid.best_estimator_)

#validate best value for k
rmse_val = [] #to store rmse values for different k
for K in range (20):
  K = K+1
  model = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=K, p=2,
                    weights='distance')
  model.fit(X_train, y_train) #fit the model
  preds = model.predict(X_test) #make predictions on test set
  error = rmse(y_test, preds) #calculate rmse
  rmse_val.append(error) #store rmse values
  print('RMSe Value for K= ', K, 'is ', error)

#plotting the rmse values (y) against k values (x)
curve = pd.DataFrame(rmse_val) #elbow curve 
curve.plot()

#set model with ideal hyperparameters from above
years_exp_no_outliers_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=12, p=2,
                    weights='distance')
years_exp_no_outliers_knn.fit(X_train, y_train)

#cross validate
years_exp_no_outliers_knn_scores = cross_val_score(years_exp_no_outliers_knn, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', years_exp_no_outliers_knn_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(years_exp_no_outliers_knn_scores)))

#generate predictions
years_exp_no_outliers_knn_preds = years_exp_no_outliers_knn.predict(X_test)

#more performance metrics
years_exp_no_outliers_knn_rsquared_train = years_exp_no_outliers_knn.score(X_train, y_train)
years_exp_no_outliers_knn_rsquared_test = years_exp_no_outliers_knn.score(X_test, y_test)
years_exp_no_outliers_knn_mae = mean_absolute_error(y_test, years_exp_no_outliers_knn_preds)
years_exp_no_outliers_knn_mse = mse(y_test, years_exp_no_outliers_knn_preds)
years_exp_no_outliers_knn_rmse = rmse(y_test, years_exp_no_outliers_knn_preds)
years_exp_no_outliers_knn_mape = np.mean(np.abs((y_test - years_exp_no_outliers_knn_preds)/y_test)*100)
years_exp_no_outliers_knn_rmspe = (np.sqrt(np.mean(np.square((y_test - years_exp_no_outliers_knn_preds) / y_test)))) * 100
years_exp_no_outliers_knn_model_name = 'Years of Experience w/o Outliers KNN'

#create easily readable display of performance metrics
years_exp_no_outliers_knn_dictionary = {'Model': years_exp_no_outliers_knn_model_name,
  'Training Set R Squared': years_exp_no_outliers_knn_rsquared_train, 
  'Test Set R Squared': years_exp_no_outliers_knn_rsquared_test, 'Mean Absolute Error': years_exp_no_outliers_knn_mae, 
  'Mean Sqaured Error': years_exp_no_outliers_knn_mse, 'Root Mean Squared Error': years_exp_no_outliers_knn_rmse,
  'Root Mean Squared Percentage Error': years_exp_no_outliers_knn_rmspe,
  'Mean Absolute Percentage Error': years_exp_no_outliers_knn_mape}

years_exp_no_outliers_knn_df = pd.DataFrame(years_exp_no_outliers_knn_dictionary, index=[0])

years_exp_no_outliers_knn_df

"""### **cat_no_outliers_df log(y)**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'n_neighbors': [2, 4, 6, 8, 10, 12, 15],  
              'weights': ['uniform', 'distance'], 
              'metric': ['euclidean', 'manhattan']}  
  
log_cat_no_outliers_knn_grid = GridSearchCV(KNeighborsRegressor(), param_grid, refit = True, verbose=1) 
  
# fitting the model for grid search 
log_cat_no_outliers_knn_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_cat_no_outliers_knn_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_cat_no_outliers_knn_grid.best_estimator_)

#validate best value for k
rmse_val = [] #to store rmse values for different k
for K in range (20):
  K = K+1
  model = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=K, p=2,
                    weights='distance')
  model.fit(X_train, y_train) #fit the model
  preds = model.predict(X_test) #make predictions on test set
  error = rmse(y_test, preds) #calculate rmse
  rmse_val.append(error) #store rmse values
  print('RMSe Value for K= ', K, 'is ', error)

#plotting the rmse values (y) against k values (x)
curve = pd.DataFrame(rmse_val) #elbow curve 
curve.plot()

#set model with ideal hyperparameters from above
log_cat_no_outliers_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=12, p=2,
                    weights='distance')
log_cat_no_outliers_knn.fit(X_train, y_train)

#cross validate
log_cat_no_outliers_knn_scores = cross_val_score(log_cat_no_outliers_knn, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_cat_no_outliers_knn_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_cat_no_outliers_knn_scores)))

#generate predictions
log_cat_no_outliers_knn_preds = log_cat_no_outliers_knn.predict(X_test)

#more performance metrics
log_cat_no_outliers_knn_rsquared_train = log_cat_no_outliers_knn.score(X_train, y_train)
log_cat_no_outliers_knn_rsquared_test = log_cat_no_outliers_knn.score(X_test, y_test)
log_cat_no_outliers_knn_mae = mean_absolute_error(y_test, log_cat_no_outliers_knn_preds)
log_cat_no_outliers_knn_mse = mse(y_test, log_cat_no_outliers_knn_preds)
log_cat_no_outliers_knn_rmse = rmse(y_test, log_cat_no_outliers_knn_preds)
log_cat_no_outliers_knn_mape = np.mean(np.abs((y_test - log_cat_no_outliers_knn_preds)/y_test)*100)
log_cat_no_outliers_knn_rmspe = (np.sqrt(np.mean(np.square((y_test - log_cat_no_outliers_knn_preds) / y_test)))) * 100
log_cat_no_outliers_knn_model_name = 'Categories w/o Outliers KNN Log(y)'

#create easily readable display of performance metrics
log_cat_no_outliers_knn_dictionary = {'Model': log_cat_no_outliers_knn_model_name,
  'Training Set R Squared': log_cat_no_outliers_knn_rsquared_train, 
  'Test Set R Squared': log_cat_no_outliers_knn_rsquared_test, 'Mean Absolute Error': log_cat_no_outliers_knn_mae, 
  'Mean Sqaured Error': log_cat_no_outliers_knn_mse, 'Root Mean Squared Error': log_cat_no_outliers_knn_rmse,
  'Root Mean Squared Percentage Error': log_cat_no_outliers_knn_rmspe,
  'Mean Absolute Percentage Error': log_cat_no_outliers_knn_mape}

log_cat_no_outliers_knn_df = pd.DataFrame(log_cat_no_outliers_knn_dictionary, index=[0])

log_cat_no_outliers_knn_df

"""### **yrs_exp_no_outliers_df log(y)**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'n_neighbors': [2, 4, 6, 8, 10, 12, 15],  
              'weights': ['uniform', 'distance'], 
              'metric': ['euclidean', 'manhattan']}  
  
log_years_exp_no_outliers_knn_grid = GridSearchCV(KNeighborsRegressor(), param_grid, refit = True, verbose=1) 
  
# fitting the model for grid search 
log_years_exp_no_outliers_knn_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_years_exp_no_outliers_knn_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_years_exp_no_outliers_knn_grid.best_estimator_)

#validate best value for k
rmse_val = [] #to store rmse values for different k
for K in range (20):
  K = K+1
  model = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=K, p=2,
                    weights='distance')
  model.fit(X_train, y_train) #fit the model
  preds = model.predict(X_test) #make predictions on test set
  error = rmse(y_test, preds) #calculate rmse
  rmse_val.append(error) #store rmse values
  print('RMSe Value for K= ', K, 'is ', error)

#plotting the rmse values (y) against k values (x)
curve = pd.DataFrame(rmse_val) #elbow curve 
curve.plot()

#set model with ideal hyperparameters from above
log_years_exp_no_outliers_knn = KNeighborsRegressor(algorithm='auto', leaf_size=30, metric='manhattan',
                    metric_params=None, n_jobs=None, n_neighbors=12, p=2,
                    weights='distance')
log_years_exp_no_outliers_knn.fit(X_train, y_train)

#cross validate
log_years_exp_no_outliers_knn_scores = cross_val_score(log_years_exp_no_outliers_knn, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_years_exp_no_outliers_knn_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_years_exp_no_outliers_knn_scores)))

#generate predictions
log_years_exp_no_outliers_knn_preds = log_years_exp_no_outliers_knn.predict(X_test)

#more performance metrics
log_years_exp_no_outliers_knn_rsquared_train = log_years_exp_no_outliers_knn.score(X_train, y_train)
log_years_exp_no_outliers_knn_rsquared_test = log_years_exp_no_outliers_knn.score(X_test, y_test)
log_years_exp_no_outliers_knn_mae = mean_absolute_error(y_test, log_years_exp_no_outliers_knn_preds)
log_years_exp_no_outliers_knn_mse = mse(y_test, log_years_exp_no_outliers_knn_preds)
log_years_exp_no_outliers_knn_rmse = rmse(y_test, log_years_exp_no_outliers_knn_preds)
log_years_exp_no_outliers_knn_mape = np.mean(np.abs((y_test - log_years_exp_no_outliers_knn_preds)/y_test)*100)
log_years_exp_no_outliers_knn_rmspe = (np.sqrt(np.mean(np.square((y_test - log_years_exp_no_outliers_knn_preds) / y_test)))) * 100
log_years_exp_no_outliers_knn_model_name = 'Years of Experience w/o Outliers KNN Log(y)'

#create easily readable display of performance metrics
log_years_exp_no_outliers_knn_dictionary = {'Model': log_years_exp_no_outliers_knn_model_name,
  'Training Set R Squared': log_years_exp_no_outliers_knn_rsquared_train, 
  'Test Set R Squared': log_years_exp_no_outliers_knn_rsquared_test, 'Mean Absolute Error': log_years_exp_no_outliers_knn_mae, 
  'Mean Sqaured Error': log_years_exp_no_outliers_knn_mse, 'Root Mean Squared Error': log_years_exp_no_outliers_knn_rmse,
  'Root Mean Squared Percentage Error': log_years_exp_no_outliers_knn_rmspe,
  'Mean Absolute Percentage Error': log_years_exp_no_outliers_knn_mape}

log_years_exp_no_outliers_knn_df = pd.DataFrame(log_years_exp_no_outliers_knn_dictionary, index=[0])

log_years_exp_no_outliers_knn_df

"""## **Random Forest**

### **categories_df**
"""

# #set features & target
# X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
# y = categories_df['avg_sal']

# #split data into training & test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

# #Use grid search to find best hyperparameters
# # defining parameter range 
# param_grid = {'bootstrap': [True, False],
#  'max_depth': [10, 20, 30, 40, 50, None],
#  'max_features': ['auto', 'sqrt'],
#  'min_samples_leaf': [1, 2, 4],
#  'min_samples_split': [2, 5, 10],
#  'n_estimators': [0,20,40,60,80,100,120,140,160,180,200],
#  'random_state': [43]}  
  
# categories_rf_grid = GridSearchCV(RandomForestRegressor(), param_grid, refit = True, verbose=1)
# # fitting the model for grid search 
# categories_rf_grid.fit(X_train, y_train) 
# # print best parameter after tuning 
# print(categories_rf_grid.best_params_) 
  
# # print how our model looks after hyper-parameter tuning 
# print(categories_rf_grid.best_estimator_)

#set model with ideal hyperparameters from above
categories_rf = RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',
                      max_depth=20, max_features='sqrt', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      n_estimators=200, n_jobs=None, oob_score=False,
                      random_state=43, verbose=0, warm_start=False)

categories_rf.fit(X_train, y_train)

#cross validate
categories_rf_scores = cross_val_score(categories_rf, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', categories_rf_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(categories_rf_scores)))

#generate predictions
categories_rf_preds = categories_rf.predict(X_test)

#more performance metrics
categories_rf_rsquared_train = categories_rf.score(X_train, y_train)
categories_rf_rsquared_test = categories_rf.score(X_test, y_test)
categories_rf_mae = mean_absolute_error(y_test, categories_rf_preds)
categories_rf_mse = mse(y_test, categories_rf_preds)
categories_rf_rmse = rmse(y_test, categories_rf_preds)
categories_rf_mape = np.mean(np.abs((y_test - categories_rf_preds)/y_test)*100)
categories_rf_rmspe = (np.sqrt(np.mean(np.square((y_test - categories_rf_preds) / y_test)))) * 100
categories_rf_model_name = 'Categories Random Forest'

#create easily readable display of performance metrics
categories_rf_dictionary = {'Model': categories_rf_model_name,
  'Training Set R Squared': categories_rf_rsquared_train, 
  'Test Set R Squared': categories_rf_rsquared_test, 'Mean Absolute Error': categories_rf_mae, 
  'Mean Sqaured Error': categories_rf_mse, 'Root Mean Squared Error': categories_rf_rmse,
  'Root Mean Squared Percentage Error': categories_rf_rmspe,
  'Mean Absolute Percentage Error': categories_rf_mape}

categories_rf_df = pd.DataFrame(categories_rf_dictionary, index=[0])

categories_rf_df

"""### **yrs_exp_df**"""

# #set features & target
# X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
# y = yrs_exp_df['avg_sal']

# #split data into training & test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

# #Use grid search to find best hyperparameters
# # defining parameter range 
# param_grid = {'bootstrap': [True, False],
#  'max_depth': [10, 20, 30, 40, 50, None],
#  'max_features': ['auto', 'sqrt'],
#  'min_samples_leaf': [1, 2, 4],
#  'min_samples_split': [2, 5, 10],
#  'n_estimators': [0,20,40,60,80,100,120,140,160,180,200],
#  'random_state': [52]}  
  
# yrs_exp_rf_grid = GridSearchCV(RandomForestRegressor(), param_grid, refit = True, verbose=1)
# # fitting the model for grid search 
# yrs_exp_rf_grid.fit(X_train, y_train) 
# # print best parameter after tuning 
# print(yrs_exp_rf_grid.best_params_) 
  
# # print how our model looks after hyper-parameter tuning 
# print(yrs_exp_rf_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_rf = RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',
                      max_depth=20, max_features='sqrt', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      n_estimators=200, n_jobs=None, oob_score=False,
                      random_state=52, verbose=0, warm_start=False)
yrs_exp_rf.fit(X_train, y_train)

#cross validate
yrs_exp_rf_scores = cross_val_score(yrs_exp_rf, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_rf_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_rf_scores)))

#generate predictions
yrs_exp_rf_preds = yrs_exp_rf.predict(X_test)

#more performance metrics
yrs_exp_rf_rsquared_train = yrs_exp_rf.score(X_train, y_train)
yrs_exp_rf_rsquared_test = yrs_exp_rf.score(X_test, y_test)
yrs_exp_rf_mae = mean_absolute_error(y_test, yrs_exp_rf_preds)
yrs_exp_rf_mse = mse(y_test, yrs_exp_rf_preds)
yrs_exp_rf_rmse = rmse(y_test, yrs_exp_rf_preds)
yrs_exp_rf_mape = np.mean(np.abs((y_test - yrs_exp_rf_preds)/y_test)*100)
yrs_exp_rf_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_rf_preds) / y_test)))) * 100
yrs_exp_rf_model_name = 'Years of Experience Random Forest'

#create easily readable display of performance metrics
yrs_exp_rf_dictionary = {'Model': yrs_exp_rf_model_name,
  'Training Set R Squared': yrs_exp_rf_rsquared_train, 
  'Test Set R Squared': yrs_exp_rf_rsquared_test, 'Mean Absolute Error': yrs_exp_rf_mae, 
  'Mean Sqaured Error': yrs_exp_rf_mse, 'Root Mean Squared Error': yrs_exp_rf_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_rf_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_rf_mape}

yrs_exp_rf_df = pd.DataFrame(yrs_exp_rf_dictionary, index=[0])

yrs_exp_rf_df

"""### **categories_df log(y)**"""

# #set features & target
# X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
# y = categories_df['log_avg_sal']

# #split data into training & test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

# #Use grid search to find best hyperparameters
# # defining parameter range 
# param_grid = {'bootstrap': [True, False],
#  'max_depth': [10, 20, 30, 40, 50, None],
#  'max_features': ['auto', 'sqrt'],
#  'min_samples_leaf': [1, 2, 4],
#  'min_samples_split': [2, 5, 10],
#  'n_estimators': [0,20,40,60,80,100,120,140,160,180,200],
#  'random_state': [52]}  
  
# log_categories_rf_grid = GridSearchCV(RandomForestRegressor(), param_grid, refit = True, verbose=1)
# # fitting the model for grid search 
# log_categories_rf_grid.fit(X_train, y_train) 
# # print best parameter after tuning 
# print(log_categories_rf_grid.best_params_) 
  
# # print how our model looks after hyper-parameter tuning 
# print(log_categories_rf_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_categories_rf = RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',
                      max_depth=30, max_features='sqrt', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=2,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      n_estimators=200, n_jobs=None, oob_score=False,
                      random_state=52, verbose=0, warm_start=False)

log_categories_rf.fit(X_train, y_train)

#cross validate
log_categories_rf_scores = cross_val_score(log_categories_rf, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_categories_rf_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_categories_rf_scores)))

#generate predictions
log_categories_rf_preds = log_categories_rf.predict(X_test)

#more performance metrics
log_categories_rf_rsquared_train = log_categories_rf.score(X_train, y_train)
log_categories_rf_rsquared_test = log_categories_rf.score(X_test, y_test)
log_categories_rf_mae = mean_absolute_error(y_test, log_categories_rf_preds)
log_categories_rf_mse = mse(y_test, log_categories_rf_preds)
log_categories_rf_rmse = rmse(y_test, log_categories_rf_preds)
log_categories_rf_mape = np.mean(np.abs((y_test - log_categories_rf_preds)/y_test)*100)
log_categories_rf_rmspe = (np.sqrt(np.mean(np.square((y_test - log_categories_rf_preds) / y_test)))) * 100
log_categories_rf_model_name = 'Categories Random Forest Log(y)'

#create easily readable display of performance metrics
log_categories_rf_dictionary = {'Model': log_categories_rf_model_name,
  'Training Set R Squared': log_categories_rf_rsquared_train, 
  'Test Set R Squared': log_categories_rf_rsquared_test, 'Mean Absolute Error': log_categories_rf_mae, 
  'Mean Sqaured Error': log_categories_rf_mse, 'Root Mean Squared Error': log_categories_rf_rmse,
  'Root Mean Squared Percentage Error': log_categories_rf_rmspe,
  'Mean Absolute Percentage Error': log_categories_rf_mape}

log_categories_rf_df = pd.DataFrame(log_categories_rf_dictionary, index=[0])

log_categories_rf_df

"""### **yrs_exp_df log(y)**"""

# #set features & target
# X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
# y = yrs_exp_df['log_avg_sal']

# #split data into training & test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

# #Use grid search to find best hyperparameters
# # defining parameter range 
# param_grid = {'bootstrap': [True, False],
#  'max_depth': [10, 20, 30, 40, 50, None],
#  'max_features': ['auto', 'sqrt'],
#  'min_samples_leaf': [1, 2, 4],
#  'min_samples_split': [2, 5, 10],
#  'n_estimators': [0,20,40,60,80,100,120,140,160,180,200],
#  'random_state': [52]}  
  
# log_yrs_exp_rf_grid = GridSearchCV(RandomForestRegressor(), param_grid, refit = True, verbose=1)
# # fitting the model for grid search 
# log_yrs_exp_rf_grid.fit(X_train, y_train) 
# # print best parameter after tuning 
# print(log_yrs_exp_rf_grid.best_params_) 
  
# # print how our model looks after hyper-parameter tuning 
# print(log_yrs_exp_rf_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_rf = RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',
                      max_depth=30, max_features='sqrt', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=1,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      n_estimators=200, n_jobs=None, oob_score=False,
                      random_state=52, verbose=0, warm_start=False)
log_yrs_exp_rf.fit(X_train, y_train)

#cross validate
log_yrs_exp_rf_scores = cross_val_score(log_yrs_exp_rf, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_rf_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_rf_scores)))

#generate predictions
log_yrs_exp_rf_preds = log_yrs_exp_rf.predict(X_test)

#more performance metrics
log_yrs_exp_rf_rsquared_train = log_yrs_exp_rf.score(X_train, y_train)
log_yrs_exp_rf_rsquared_test = log_yrs_exp_rf.score(X_test, y_test)
log_yrs_exp_rf_mae = mean_absolute_error(y_test, log_yrs_exp_rf_preds)
log_yrs_exp_rf_mse = mse(y_test, log_yrs_exp_rf_preds)
log_yrs_exp_rf_rmse = rmse(y_test, log_yrs_exp_rf_preds)
log_yrs_exp_rf_mape = np.mean(np.abs((y_test - log_yrs_exp_rf_preds)/y_test)*100)
log_yrs_exp_rf_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_rf_preds) / y_test)))) * 100
log_yrs_exp_rf_model_name = 'Years of Experience Random Forest Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_rf_dictionary = {'Model': log_yrs_exp_rf_model_name,
  'Training Set R Squared': log_yrs_exp_rf_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_rf_rsquared_test, 'Mean Absolute Error': log_yrs_exp_rf_mae, 
  'Mean Sqaured Error': log_yrs_exp_rf_mse, 'Root Mean Squared Error': log_yrs_exp_rf_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_rf_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_rf_mape}

log_yrs_exp_rf_df = pd.DataFrame(log_yrs_exp_rf_dictionary, index=[0])

log_yrs_exp_rf_df

"""### **cat_no_outliers_df**"""

# #set features & target
# X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
# y = cat_no_outliers_df['avg_sal']

# #split data into training & test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

# #Use grid search to find best hyperparameters
# # defining parameter range 
# param_grid = {'bootstrap': [True, False],
#  'max_depth': [10, 20, 30, 40, 50, None],
#  'max_features': ['auto', 'sqrt'],
#  'min_samples_leaf': [1, 2, 4],
#  'min_samples_split': [2, 5, 10],
#  'n_estimators': [0,20,40,60,80,100,120,140,160,180,200],
#  'random_state': [43]}  
  
# cat_no_outliers_rf_grid = GridSearchCV(RandomForestRegressor(), param_grid, refit = True, verbose=1)
# # fitting the model for grid search 
# cat_no_outliers_rf_grid.fit(X_train, y_train) 
# # print best parameter after tuning 
# print(cat_no_outliers_rf_grid.best_params_) 
  
# # print how our model looks after hyper-parameter tuning 
# print(cat_no_outliers_rf_grid.best_estimator_)

#set model with ideal hyperparameters from above
cat_no_outliers_rf = RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',
                      max_depth=30, max_features='sqrt', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=1,
                      min_samples_split=5, min_weight_fraction_leaf=0.0,
                      n_estimators=140, n_jobs=None, oob_score=False,
                      random_state=43, verbose=0, warm_start=False)

cat_no_outliers_rf.fit(X_train, y_train)

#cross validate
cat_no_outliers_rf_scores = cross_val_score(cat_no_outliers_rf, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', cat_no_outliers_rf_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(cat_no_outliers_rf_scores)))

#generate predictions
cat_no_outliers_rf_preds = cat_no_outliers_rf.predict(X_test)

#more performance metrics
cat_no_outliers_rf_rsquared_train = cat_no_outliers_rf.score(X_train, y_train)
cat_no_outliers_rf_rsquared_test = cat_no_outliers_rf.score(X_test, y_test)
cat_no_outliers_rf_mae = mean_absolute_error(y_test, cat_no_outliers_rf_preds)
cat_no_outliers_rf_mse = mse(y_test, cat_no_outliers_rf_preds)
cat_no_outliers_rf_rmse = rmse(y_test, cat_no_outliers_rf_preds)
cat_no_outliers_rf_mape = np.mean(np.abs((y_test - cat_no_outliers_rf_preds)/y_test)*100)
cat_no_outliers_rf_rmspe = (np.sqrt(np.mean(np.square((y_test - cat_no_outliers_rf_preds) / y_test)))) * 100
cat_no_outliers_rf_model_name = 'Categories w/o Outliers Random Forest'

#create easily readable display of performance metrics
cat_no_outliers_rf_dictionary = {'Model': cat_no_outliers_rf_model_name,
  'Training Set R Squared': cat_no_outliers_rf_rsquared_train, 
  'Test Set R Squared': cat_no_outliers_rf_rsquared_test, 'Mean Absolute Error': cat_no_outliers_rf_mae, 
  'Mean Sqaured Error': cat_no_outliers_rf_mse, 'Root Mean Squared Error': cat_no_outliers_rf_rmse,
  'Root Mean Squared Percentage Error': cat_no_outliers_rf_rmspe,
  'Mean Absolute Percentage Error': cat_no_outliers_rf_mape}

cat_no_outliers_rf_df = pd.DataFrame(cat_no_outliers_rf_dictionary, index=[0])

cat_no_outliers_rf_df

"""### **yrs_exp_no_outliers_df**"""

# #set features & target
# X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
# y = yrs_exp_no_outliers_df['avg_sal']

# #split data into training & test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

# #Use grid search to find best hyperparameters
# # defining parameter range 
# param_grid = {'bootstrap': [True, False],
#  'max_depth': [10, 20, 30, 40, 50, None],
#  'max_features': ['auto', 'sqrt'],
#  'min_samples_leaf': [1, 2, 4],
#  'min_samples_split': [2, 5, 10],
#  'n_estimators': [0,20,40,60,80,100,120,140,160,180,200],
#  'random_state': [52]}  
  
# yrs_exp_no_outliers_rf_grid = GridSearchCV(RandomForestRegressor(), param_grid, refit = True, verbose=1)
# # fitting the model for grid search 
# yrs_exp_no_outliers_rf_grid.fit(X_train, y_train) 
# # print best parameter after tuning 
# print(yrs_exp_no_outliers_rf_grid.best_params_) 
  
# # print how our model looks after hyper-parameter tuning 
# print(yrs_exp_no_outliers_rf_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_no_outliers_rf = RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',
                      max_depth=20, max_features='sqrt', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=1,
                      min_samples_split=5, min_weight_fraction_leaf=0.0,
                      n_estimators=160, n_jobs=None, oob_score=False,
                      random_state=52, verbose=0, warm_start=False)
yrs_exp_no_outliers_rf.fit(X_train, y_train)

#cross validate
yrs_exp_no_outliers_rf_scores = cross_val_score(yrs_exp_no_outliers_rf, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_no_outliers_rf_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_no_outliers_rf_scores)))

#generate predictions
yrs_exp_no_outliers_rf_preds = yrs_exp_no_outliers_rf.predict(X_test)

#more performance metrics
yrs_exp_no_outliers_rf_rsquared_train = yrs_exp_no_outliers_rf.score(X_train, y_train)
yrs_exp_no_outliers_rf_rsquared_test = yrs_exp_no_outliers_rf.score(X_test, y_test)
yrs_exp_no_outliers_rf_mae = mean_absolute_error(y_test, yrs_exp_no_outliers_rf_preds)
yrs_exp_no_outliers_rf_mse = mse(y_test, yrs_exp_no_outliers_rf_preds)
yrs_exp_no_outliers_rf_rmse = rmse(y_test, yrs_exp_no_outliers_rf_preds)
yrs_exp_no_outliers_rf_mape = np.mean(np.abs((y_test - yrs_exp_no_outliers_rf_preds)/y_test)*100)
yrs_exp_no_outliers_rf_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_no_outliers_rf_preds) / y_test)))) * 100
yrs_exp_no_outliers_rf_model_name = 'Years of Experience w/o Outliers Random Forest'

#create easily readable display of performance metrics
yrs_exp_no_outliers_rf_dictionary = {'Model': yrs_exp_no_outliers_rf_model_name,
  'Training Set R Squared': yrs_exp_no_outliers_rf_rsquared_train, 
  'Test Set R Squared': yrs_exp_no_outliers_rf_rsquared_test, 'Mean Absolute Error': yrs_exp_no_outliers_rf_mae, 
  'Mean Sqaured Error': yrs_exp_no_outliers_rf_mse, 'Root Mean Squared Error': yrs_exp_no_outliers_rf_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_no_outliers_rf_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_no_outliers_rf_mape}

yrs_exp_no_outliers_rf_df = pd.DataFrame(yrs_exp_no_outliers_rf_dictionary, index=[0])

yrs_exp_no_outliers_rf_df

"""### **cat_no_outliers_df log(y)**"""

# #set features & target
# X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
# y = cat_no_outliers_df['log_avg_sal']

# #split data into training & test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

# #Use grid search to find best hyperparameters
# # defining parameter range 
# param_grid = {'bootstrap': [True, False],
#  'max_depth': [10, 20, 30, 40, 50, None],
#  'max_features': ['auto', 'sqrt'],
#  'min_samples_leaf': [1, 2, 4],
#  'min_samples_split': [2, 5, 10],
#  'n_estimators': [0,20,40,60,80,100,120,140,160,180,200],
#  'random_state': [52]}  
  
# log_cat_no_outliers_rf_grid = GridSearchCV(RandomForestRegressor(), param_grid, refit = True, verbose=1)
# # fitting the model for grid search 
# log_cat_no_outliers_rf_grid.fit(X_train, y_train) 
# # print best parameter after tuning 
# print(log_cat_no_outliers_rf_grid.best_params_) 
  
# # print how our model looks after hyper-parameter tuning 
# print(log_cat_no_outliers_rf_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_cat_no_outliers_rf = RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',
                      max_depth=30, max_features='sqrt', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=2,
                      min_samples_split=2, min_weight_fraction_leaf=0.0,
                      n_estimators=200, n_jobs=None, oob_score=False,
                      random_state=52, verbose=0, warm_start=False)

log_cat_no_outliers_rf.fit(X_train, y_train)

#cross validate
log_cat_no_outliers_rf_scores = cross_val_score(log_cat_no_outliers_rf, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_cat_no_outliers_rf_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_cat_no_outliers_rf_scores)))

#generate predictions
log_cat_no_outliers_rf_preds = log_cat_no_outliers_rf.predict(X_test)

#more performance metrics
log_cat_no_outliers_rf_rsquared_train = log_cat_no_outliers_rf.score(X_train, y_train)
log_cat_no_outliers_rf_rsquared_test = log_cat_no_outliers_rf.score(X_test, y_test)
log_cat_no_outliers_rf_mae = mean_absolute_error(y_test, log_cat_no_outliers_rf_preds)
log_cat_no_outliers_rf_mse = mse(y_test, log_cat_no_outliers_rf_preds)
log_cat_no_outliers_rf_rmse = rmse(y_test, log_cat_no_outliers_rf_preds)
log_cat_no_outliers_rf_mape = np.mean(np.abs((y_test - log_cat_no_outliers_rf_preds)/y_test)*100)
log_cat_no_outliers_rf_rmspe = (np.sqrt(np.mean(np.square((y_test - log_cat_no_outliers_rf_preds) / y_test)))) * 100
log_cat_no_outliers_rf_model_name = 'Categories w/o Outliers Random Forest Log(y)'

#create easily readable display of performance metrics
log_cat_no_outliers_rf_dictionary = {'Model': log_cat_no_outliers_rf_model_name,
  'Training Set R Squared': log_cat_no_outliers_rf_rsquared_train, 
  'Test Set R Squared': log_cat_no_outliers_rf_rsquared_test, 'Mean Absolute Error': log_cat_no_outliers_rf_mae, 
  'Mean Sqaured Error': log_cat_no_outliers_rf_mse, 'Root Mean Squared Error': log_cat_no_outliers_rf_rmse,
  'Root Mean Squared Percentage Error': log_cat_no_outliers_rf_rmspe,
  'Mean Absolute Percentage Error': log_cat_no_outliers_rf_mape}

log_cat_no_outliers_rf_df = pd.DataFrame(log_cat_no_outliers_rf_dictionary, index=[0])

log_cat_no_outliers_rf_df

"""### **yrs_exp_no_outliers_df log(y)**"""

# #set features & target
# X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
# y = yrs_exp_no_outliers_df['log_avg_sal']

# #split data into training & test sets
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

# #Use grid search to find best hyperparameters
# # defining parameter range 
# param_grid = {'bootstrap': [True, False],
#  'max_depth': [10, 20, 30, 40, 50, None],
#  'max_features': ['auto', 'sqrt'],
#  'min_samples_leaf': [1, 2, 4],
#  'min_samples_split': [2, 5, 10],
#  'n_estimators': [0,20,40,60,80,100,120,140,160,180,200],
#  'random_state': [52]}  
  
# log_yrs_exp_no_outliers_rf_grid = GridSearchCV(RandomForestRegressor(), param_grid, refit = True, verbose=1)
# # fitting the model for grid search 
# log_yrs_exp_no_outliers_rf_grid.fit(X_train, y_train) 
# # print best parameter after tuning 
# print(log_yrs_exp_no_outliers_rf_grid.best_params_) 
  
# # print how our model looks after hyper-parameter tuning 
# print(log_yrs_exp_no_outliers_rf_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_no_outliers_rf = RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',
                      max_depth=30, max_features='sqrt', max_leaf_nodes=None,
                      max_samples=None, min_impurity_decrease=0.0,
                      min_impurity_split=None, min_samples_leaf=2,
                      min_samples_split=5, min_weight_fraction_leaf=0.0,
                      n_estimators=200, n_jobs=None, oob_score=False,
                      random_state=52, verbose=0, warm_start=False)
log_yrs_exp_no_outliers_rf.fit(X_train, y_train)

#cross validate
log_yrs_exp_no_outliers_rf_scores = cross_val_score(log_yrs_exp_no_outliers_rf, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_no_outliers_rf_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_no_outliers_rf_scores)))

#generate predictions
log_yrs_exp_no_outliers_rf_preds = log_yrs_exp_no_outliers_rf.predict(X_test)

#more performance metrics
log_yrs_exp_no_outliers_rf_rsquared_train = log_yrs_exp_no_outliers_rf.score(X_train, y_train)
log_yrs_exp_no_outliers_rf_rsquared_test = log_yrs_exp_no_outliers_rf.score(X_test, y_test)
log_yrs_exp_no_outliers_rf_mae = mean_absolute_error(y_test, log_yrs_exp_no_outliers_rf_preds)
log_yrs_exp_no_outliers_rf_mse = mse(y_test, log_yrs_exp_no_outliers_rf_preds)
log_yrs_exp_no_outliers_rf_rmse = rmse(y_test, log_yrs_exp_no_outliers_rf_preds)
log_yrs_exp_no_outliers_rf_mape = np.mean(np.abs((y_test - log_yrs_exp_no_outliers_rf_preds)/y_test)*100)
log_yrs_exp_no_outliers_rf_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_no_outliers_rf_preds) / y_test)))) * 100
log_yrs_exp_no_outliers_rf_model_name = 'Years of Experience w/o Outliers Random Forest Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_no_outliers_rf_dictionary = {'Model': log_yrs_exp_no_outliers_rf_model_name,
  'Training Set R Squared': log_yrs_exp_no_outliers_rf_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_no_outliers_rf_rsquared_test, 'Mean Absolute Error': log_yrs_exp_no_outliers_rf_mae, 
  'Mean Sqaured Error': log_yrs_exp_no_outliers_rf_mse, 'Root Mean Squared Error': log_yrs_exp_no_outliers_rf_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_no_outliers_rf_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_no_outliers_rf_mape}

log_yrs_exp_no_outliers_rf_df = pd.DataFrame(log_yrs_exp_no_outliers_rf_dictionary, index=[0])

log_yrs_exp_no_outliers_rf_df

"""## **Support Vector Machine**

### **categories_df**
"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10],
 'gamma': [0.001, 0.01, 0.1, 1]
} 
  
categories_svm_grid = GridSearchCV(svm.SVR(kernel='rbf'), param_grid)
# fitting the model for grid search 
categories_svm_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(categories_svm_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(categories_svm_grid.best_estimator_)

#set model with ideal hyperparameters from above
categories_svm = svm.SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.01,
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)

categories_svm.fit(X_train, y_train)

#cross validate
categories_svm_scores = cross_val_score(categories_svm, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', categories_svm_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(categories_svm_scores)))

#generate predictions
categories_svm_preds = categories_svm.predict(X_test)

#more performance metrics
categories_svm_rsquared_train = categories_svm.score(X_train, y_train)
categories_svm_rsquared_test = categories_svm.score(X_test, y_test)
categories_svm_mae = mean_absolute_error(y_test, categories_svm_preds)
categories_svm_mse = mse(y_test, categories_svm_preds)
categories_svm_rmse = rmse(y_test, categories_svm_preds)
categories_svm_mape = np.mean(np.abs((y_test - categories_svm_preds)/y_test)*100)
categories_svm_rmspe = (np.sqrt(np.mean(np.square((y_test - categories_svm_preds) / y_test)))) * 100
categories_svm_model_name = 'Categories SVM'

#create easily readable display of performance metrics
categories_svm_dictionary = {'Model': categories_svm_model_name,
  'Training Set R Squared': categories_svm_rsquared_train, 
  'Test Set R Squared': categories_svm_rsquared_test, 'Mean Absolute Error': categories_svm_mae, 
  'Mean Sqaured Error': categories_svm_mse, 'Root Mean Squared Error': categories_svm_rmse,
  'Root Mean Squared Percentage Error': categories_svm_rmspe,
  'Mean Absolute Percentage Error': categories_svm_mape}

categories_svm_df = pd.DataFrame(categories_svm_dictionary, index=[0])

categories_svm_df

"""### **yrs_exp_df**"""

#set features & target
X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10],
 'gamma': [0.001, 0.01, 0.1, 1]
} 
  
yrs_exp_svm_grid = GridSearchCV(svm.SVR(kernel='rbf'), param_grid)
# fitting the model for grid search 
yrs_exp_svm_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(yrs_exp_svm_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(yrs_exp_svm_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_svm = svm.SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, 
      gamma=0.01, kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, 
      verbose=False)
yrs_exp_svm.fit(X_train, y_train)

#cross validate
yrs_exp_svm_scores = cross_val_score(yrs_exp_svm, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_svm_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_svm_scores)))

#generate predictions
yrs_exp_svm_preds = yrs_exp_svm.predict(X_test)

#more performance metrics
yrs_exp_svm_rsquared_train = yrs_exp_svm.score(X_train, y_train)
yrs_exp_svm_rsquared_test = yrs_exp_svm.score(X_test, y_test)
yrs_exp_svm_mae = mean_absolute_error(y_test, yrs_exp_svm_preds)
yrs_exp_svm_mse = mse(y_test, yrs_exp_svm_preds)
yrs_exp_svm_rmse = rmse(y_test, yrs_exp_svm_preds)
yrs_exp_svm_mape = np.mean(np.abs((y_test - yrs_exp_svm_preds)/y_test)*100)
yrs_exp_svm_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_svm_preds) / y_test)))) * 100
yrs_exp_svm_model_name = 'Years of Experience SVM'

#create easily readable display of performance metrics
yrs_exp_svm_dictionary = {'Model': yrs_exp_svm_model_name,
  'Training Set R Squared': yrs_exp_svm_rsquared_train, 
  'Test Set R Squared': yrs_exp_svm_rsquared_test, 'Mean Absolute Error': yrs_exp_svm_mae, 
  'Mean Sqaured Error': yrs_exp_svm_mse, 'Root Mean Squared Error': yrs_exp_svm_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_svm_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_svm_mape}

yrs_exp_svm_df = pd.DataFrame(yrs_exp_svm_dictionary, index=[0])

yrs_exp_svm_df

"""### **categories_df log(y)**"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10],
 'gamma': [0.001, 0.01, 0.1, 1]
} 
  
log_categories_svm_grid = GridSearchCV(svm.SVR(kernel='rbf'), param_grid)
# fitting the model for grid search 
log_categories_svm_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_categories_svm_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_categories_svm_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_categories_svm = svm.SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.01,
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)

log_categories_svm.fit(X_train, y_train)

#cross validate
log_categories_svm_scores = cross_val_score(log_categories_svm, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_categories_svm_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_categories_svm_scores)))

#generate predictions
log_categories_svm_preds = log_categories_svm.predict(X_test)

#more performance metrics
log_categories_svm_rsquared_train = log_categories_svm.score(X_train, y_train)
log_categories_svm_rsquared_test = log_categories_svm.score(X_test, y_test)
log_categories_svm_mae = mean_absolute_error(y_test, log_categories_svm_preds)
log_categories_svm_mse = mse(y_test, log_categories_svm_preds)
log_categories_svm_rmse = rmse(y_test, log_categories_svm_preds)
log_categories_svm_mape = np.mean(np.abs((y_test - log_categories_svm_preds)/y_test)*100)
log_categories_svm_rmspe = (np.sqrt(np.mean(np.square((y_test - log_categories_svm_preds) / y_test)))) * 100
log_categories_svm_model_name = 'Categories SVM Log(y)'

#create easily readable display of performance metrics
log_categories_svm_dictionary = {'Model': log_categories_svm_model_name,
  'Training Set R Squared': log_categories_svm_rsquared_train, 
  'Test Set R Squared': log_categories_svm_rsquared_test, 'Mean Absolute Error': log_categories_svm_mae, 
  'Mean Sqaured Error': log_categories_svm_mse, 'Root Mean Squared Error': log_categories_svm_rmse,
  'Root Mean Squared Percentage Error': log_categories_svm_rmspe,
  'Mean Absolute Percentage Error': log_categories_svm_mape}

log_categories_svm_df = pd.DataFrame(log_categories_svm_dictionary, index=[0])

log_categories_svm_df

"""### **yrs_exp_df log(y)**"""

#set features & target
X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10],
 'gamma': [0.001, 0.01, 0.1, 1]
} 
  
log_yrs_exp_svm_grid = GridSearchCV(svm.SVR(kernel='rbf'), param_grid)
# fitting the model for grid search 
log_yrs_exp_svm_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_yrs_exp_svm_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_yrs_exp_svm_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_svm = svm.SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, 
      gamma=0.01, kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, 
      verbose=False)
log_yrs_exp_svm.fit(X_train, y_train)

#cross validate
log_yrs_exp_svm_scores = cross_val_score(log_yrs_exp_svm, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_svm_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_svm_scores)))

#generate predictions
log_yrs_exp_svm_preds = log_yrs_exp_svm.predict(X_test)

#more performance metrics
log_yrs_exp_svm_rsquared_train = log_yrs_exp_svm.score(X_train, y_train)
log_yrs_exp_svm_rsquared_test = log_yrs_exp_svm.score(X_test, y_test)
log_yrs_exp_svm_mae = mean_absolute_error(y_test, log_yrs_exp_svm_preds)
log_yrs_exp_svm_mse = mse(y_test, log_yrs_exp_svm_preds)
log_yrs_exp_svm_rmse = rmse(y_test, log_yrs_exp_svm_preds)
log_yrs_exp_svm_mape = np.mean(np.abs((y_test - log_yrs_exp_svm_preds)/y_test)*100)
log_yrs_exp_svm_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_svm_preds) / y_test)))) * 100
log_yrs_exp_svm_model_name = 'Years of Experience SVM Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_svm_dictionary = {'Model': log_yrs_exp_svm_model_name,
  'Training Set R Squared': log_yrs_exp_svm_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_svm_rsquared_test, 'Mean Absolute Error': log_yrs_exp_svm_mae, 
  'Mean Sqaured Error': log_yrs_exp_svm_mse, 'Root Mean Squared Error': log_yrs_exp_svm_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_svm_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_svm_mape}

log_yrs_exp_svm_df = pd.DataFrame(log_yrs_exp_svm_dictionary, index=[0])

log_yrs_exp_svm_df

"""### **cat_no_outliers_df**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10],
 'gamma': [0.001, 0.01, 0.1, 1]
} 
  
cat_no_outliers_svm_grid = GridSearchCV(svm.SVR(kernel='rbf'), param_grid)
# fitting the model for grid search 
cat_no_outliers_svm_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(cat_no_outliers_svm_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(cat_no_outliers_svm_grid.best_estimator_)

#set model with ideal hyperparameters from above
cat_no_outliers_svm = svm.SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.01,
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)

cat_no_outliers_svm.fit(X_train, y_train)

#cross validate
cat_no_outliers_svm_scores = cross_val_score(cat_no_outliers_svm, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', cat_no_outliers_svm_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(cat_no_outliers_svm_scores)))

#generate predictions
cat_no_outliers_svm_preds = cat_no_outliers_svm.predict(X_test)

#more performance metrics
cat_no_outliers_svm_rsquared_train = cat_no_outliers_svm.score(X_train, y_train)
cat_no_outliers_svm_rsquared_test = cat_no_outliers_svm.score(X_test, y_test)
cat_no_outliers_svm_mae = mean_absolute_error(y_test, cat_no_outliers_svm_preds)
cat_no_outliers_svm_mse = mse(y_test, cat_no_outliers_svm_preds)
cat_no_outliers_svm_rmse = rmse(y_test, cat_no_outliers_svm_preds)
cat_no_outliers_svm_mape = np.mean(np.abs((y_test - cat_no_outliers_svm_preds)/y_test)*100)
cat_no_outliers_svm_rmspe = (np.sqrt(np.mean(np.square((y_test - cat_no_outliers_svm_preds) / y_test)))) * 100
cat_no_outliers_svm_model_name = 'Categories w/o Outliers SVM'

#create easily readable display of performance metrics
cat_no_outliers_svm_dictionary = {'Model': cat_no_outliers_svm_model_name,
  'Training Set R Squared': cat_no_outliers_svm_rsquared_train, 
  'Test Set R Squared': cat_no_outliers_svm_rsquared_test, 'Mean Absolute Error': cat_no_outliers_svm_mae, 
  'Mean Sqaured Error': cat_no_outliers_svm_mse, 'Root Mean Squared Error': cat_no_outliers_svm_rmse,
  'Root Mean Squared Percentage Error': cat_no_outliers_svm_rmspe,
  'Mean Absolute Percentage Error': cat_no_outliers_svm_mape}

cat_no_outliers_svm_df = pd.DataFrame(cat_no_outliers_svm_dictionary, index=[0])

cat_no_outliers_svm_df

"""### **yrs_exp_no_outliers_df**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10],
 'gamma': [0.001, 0.01, 0.1, 1]
} 
  
yrs_exp_no_outliers_svm_grid = GridSearchCV(svm.SVR(kernel='rbf'), param_grid)
# fitting the model for grid search 
yrs_exp_no_outliers_svm_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(yrs_exp_no_outliers_svm_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(yrs_exp_no_outliers_svm_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_no_outliers_svm = svm.SVR(C=10, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, 
      gamma=0.01, kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, 
      verbose=False)
yrs_exp_no_outliers_svm.fit(X_train, y_train)

#cross validate
yrs_exp_no_outliers_svm_scores = cross_val_score(yrs_exp_no_outliers_svm, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_no_outliers_svm_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_no_outliers_svm_scores)))

#generate predictions
yrs_exp_no_outliers_svm_preds = yrs_exp_no_outliers_svm.predict(X_test)

#more performance metrics
yrs_exp_no_outliers_svm_rsquared_train = yrs_exp_no_outliers_svm.score(X_train, y_train)
yrs_exp_no_outliers_svm_rsquared_test = yrs_exp_no_outliers_svm.score(X_test, y_test)
yrs_exp_no_outliers_svm_mae = mean_absolute_error(y_test, yrs_exp_no_outliers_svm_preds)
yrs_exp_no_outliers_svm_mse = mse(y_test, yrs_exp_no_outliers_svm_preds)
yrs_exp_no_outliers_svm_rmse = rmse(y_test, yrs_exp_no_outliers_svm_preds)
yrs_exp_no_outliers_svm_mape = np.mean(np.abs((y_test - yrs_exp_no_outliers_svm_preds)/y_test)*100)
yrs_exp_no_outliers_svm_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_no_outliers_svm_preds) / y_test)))) * 100
yrs_exp_no_outliers_svm_model_name = 'Years of Experience w/o Outliers SVM'

#create easily readable display of performance metrics
yrs_exp_no_outliers_svm_dictionary = {'Model': yrs_exp_no_outliers_svm_model_name,
  'Training Set R Squared': yrs_exp_no_outliers_svm_rsquared_train, 
  'Test Set R Squared': yrs_exp_no_outliers_svm_rsquared_test, 'Mean Absolute Error': yrs_exp_no_outliers_svm_mae, 
  'Mean Sqaured Error': yrs_exp_no_outliers_svm_mse, 'Root Mean Squared Error': yrs_exp_no_outliers_svm_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_no_outliers_svm_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_no_outliers_svm_mape}

yrs_exp_no_outliers_svm_df = pd.DataFrame(yrs_exp_no_outliers_svm_dictionary, index=[0])

yrs_exp_no_outliers_svm_df

"""### **cat_no_outliers_df log(y)**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10],
 'gamma': [0.001, 0.01, 0.1, 1]
} 
  
log_cat_no_outliers_svm_grid = GridSearchCV(svm.SVR(kernel='rbf'), param_grid)
# fitting the model for grid search 
log_cat_no_outliers_svm_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_cat_no_outliers_svm_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_cat_no_outliers_svm_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_cat_no_outliers_svm = svm.SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma=0.01,
    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)

log_cat_no_outliers_svm.fit(X_train, y_train)

#cross validate
log_cat_no_outliers_svm_scores = cross_val_score(log_cat_no_outliers_svm, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_cat_no_outliers_svm_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_cat_no_outliers_svm_scores)))

#generate predictions
log_cat_no_outliers_svm_preds = log_cat_no_outliers_svm.predict(X_test)

#more performance metrics
log_cat_no_outliers_svm_rsquared_train = log_cat_no_outliers_svm.score(X_train, y_train)
log_cat_no_outliers_svm_rsquared_test = log_cat_no_outliers_svm.score(X_test, y_test)
log_cat_no_outliers_svm_mae = mean_absolute_error(y_test, log_cat_no_outliers_svm_preds)
log_cat_no_outliers_svm_mse = mse(y_test, log_cat_no_outliers_svm_preds)
log_cat_no_outliers_svm_rmse = rmse(y_test, log_cat_no_outliers_svm_preds)
log_cat_no_outliers_svm_mape = np.mean(np.abs((y_test - log_cat_no_outliers_svm_preds)/y_test)*100)
log_cat_no_outliers_svm_rmspe = (np.sqrt(np.mean(np.square((y_test - log_cat_no_outliers_svm_preds) / y_test)))) * 100
log_cat_no_outliers_svm_model_name = 'Categories w/o OUtliers SVM Log(y)'

#create easily readable display of performance metrics
log_cat_no_outliers_svm_dictionary = {'Model': log_cat_no_outliers_svm_model_name,
  'Training Set R Squared': log_cat_no_outliers_svm_rsquared_train, 
  'Test Set R Squared': log_cat_no_outliers_svm_rsquared_test, 'Mean Absolute Error': log_cat_no_outliers_svm_mae, 
  'Mean Sqaured Error': log_cat_no_outliers_svm_mse, 'Root Mean Squared Error': log_cat_no_outliers_svm_rmse,
  'Root Mean Squared Percentage Error': log_cat_no_outliers_svm_rmspe,
  'Mean Absolute Percentage Error': log_cat_no_outliers_svm_mape}

log_cat_no_outliers_svm_df = pd.DataFrame(log_cat_no_outliers_svm_dictionary, index=[0])

log_cat_no_outliers_svm_df

"""### **yrs_exp_no_outliers_df log(y)**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'C': [0.001, 0.01, 0.1, 1, 10],
 'gamma': [0.001, 0.01, 0.1, 1]
} 
  
log_yrs_exp_no_outliers_svm_grid = GridSearchCV(svm.SVR(kernel='rbf'), param_grid)
# fitting the model for grid search 
log_yrs_exp_no_outliers_svm_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_yrs_exp_no_outliers_svm_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_yrs_exp_no_outliers_svm_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_no_outliers_svm = svm.SVR(C=1, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, 
      gamma=0.01, kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, 
      verbose=False)
log_yrs_exp_no_outliers_svm.fit(X_train, y_train)

#cross validate
log_yrs_exp_no_outliers_svm_scores = cross_val_score(log_yrs_exp_no_outliers_svm, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_no_outliers_svm_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_no_outliers_svm_scores)))

#generate predictions
log_yrs_exp_no_outliers_svm_preds = log_yrs_exp_no_outliers_svm.predict(X_test)

#more performance metrics
log_yrs_exp_no_outliers_svm_rsquared_train = log_yrs_exp_no_outliers_svm.score(X_train, y_train)
log_yrs_exp_no_outliers_svm_rsquared_test = log_yrs_exp_no_outliers_svm.score(X_test, y_test)
log_yrs_exp_no_outliers_svm_mae = mean_absolute_error(y_test, log_yrs_exp_no_outliers_svm_preds)
log_yrs_exp_no_outliers_svm_mse = mse(y_test, log_yrs_exp_no_outliers_svm_preds)
log_yrs_exp_no_outliers_svm_rmse = rmse(y_test, log_yrs_exp_no_outliers_svm_preds)
log_yrs_exp_no_outliers_svm_mape = np.mean(np.abs((y_test - log_yrs_exp_no_outliers_svm_preds)/y_test)*100)
log_yrs_exp_no_outliers_svm_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_no_outliers_svm_preds) / y_test)))) * 100
log_yrs_exp_no_outliers_svm_model_name = 'Years of Experience w/o Outliers SVM Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_no_outliers_svm_dictionary = {'Model': log_yrs_exp_no_outliers_svm_model_name,
  'Training Set R Squared': log_yrs_exp_no_outliers_svm_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_no_outliers_svm_rsquared_test, 'Mean Absolute Error': log_yrs_exp_no_outliers_svm_mae, 
  'Mean Sqaured Error': log_yrs_exp_no_outliers_svm_mse, 'Root Mean Squared Error': log_yrs_exp_no_outliers_svm_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_no_outliers_svm_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_no_outliers_svm_mape}

log_yrs_exp_no_outliers_svm_df = pd.DataFrame(log_yrs_exp_no_outliers_svm_dictionary, index=[0])

log_yrs_exp_no_outliers_svm_df

"""## **XGBoost**

### **categories_df**
"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7],
              'min_child_weight': [4],
              'silent': [1],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [500]}
  
categories_xgb_grid = GridSearchCV(XGBRegressor(), param_grid,  cv = 5,
                        n_jobs = 5, verbose=True)
# fitting the model for grid search 
categories_xgb_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(categories_xgb_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(categories_xgb_grid.best_estimator_)

#set model with ideal hyperparameters from above
categories_xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=0.7, gamma=0,
             importance_type='gain', learning_rate=0.03, max_delta_step=0,
             max_depth=6, min_child_weight=4, missing=None, n_estimators=500,
             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,
             subsample=0.7, verbosity=1)

categories_xgb.fit(X_train, y_train)

#cross validate
categories_xgb_scores = cross_val_score(categories_xgb, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', categories_xgb_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(categories_xgb_scores)))

#generate predictions
categories_xgb_preds = categories_xgb.predict(X_test)

#more performance metrics
categories_xgb_rsquared_train = categories_xgb.score(X_train, y_train)
categories_xgb_rsquared_test = categories_xgb.score(X_test, y_test)
categories_xgb_mae = mean_absolute_error(y_test, categories_xgb_preds)
categories_xgb_mse = mse(y_test, categories_xgb_preds)
categories_xgb_rmse = rmse(y_test, categories_xgb_preds)
categories_xgb_mape = np.mean(np.abs((y_test - categories_xgb_preds)/y_test)*100)
categories_xgb_rmspe = (np.sqrt(np.mean(np.square((y_test - categories_xgb_preds) / y_test)))) * 100
categories_xgb_model_name = 'Categories XGBoost'

#create easily readable display of performance metrics
categories_xgb_dictionary = {'Model': categories_xgb_model_name,
  'Training Set R Squared': categories_xgb_rsquared_train, 
  'Test Set R Squared': categories_xgb_rsquared_test, 'Mean Absolute Error': categories_xgb_mae, 
  'Mean Sqaured Error': categories_xgb_mse, 'Root Mean Squared Error': categories_xgb_rmse,
  'Root Mean Squared Percentage Error': categories_xgb_rmspe,
  'Mean Absolute Percentage Error': categories_xgb_mape}

categories_xgb_df = pd.DataFrame(categories_xgb_dictionary, index=[0])

categories_xgb_df

"""### **yrs_exp_df**"""

#set features & target
X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7],
              'min_child_weight': [4],
              'silent': [1],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [500]}
  
yrs_exp_xgb_grid = GridSearchCV(XGBRegressor(), param_grid,  cv = 5,
                        n_jobs = 5, verbose=True)
# fitting the model for grid search 
yrs_exp_xgb_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(yrs_exp_xgb_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(yrs_exp_xgb_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=0.7, gamma=0,
             importance_type='gain', learning_rate=0.03, max_delta_step=0,
             max_depth=6, min_child_weight=4, missing=None, n_estimators=500,
             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,
             subsample=0.7, verbosity=1)

yrs_exp_xgb.fit(X_train, y_train)

#cross validate
yrs_exp_xgb_scores = cross_val_score(yrs_exp_xgb, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_xgb_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_xgb_scores)))

#generate predictions
yrs_exp_xgb_preds = yrs_exp_xgb.predict(X_test)

#more performance metrics
yrs_exp_xgb_rsquared_train = yrs_exp_xgb.score(X_train, y_train)
yrs_exp_xgb_rsquared_test = yrs_exp_xgb.score(X_test, y_test)
yrs_exp_xgb_mae = mean_absolute_error(y_test, yrs_exp_xgb_preds)
yrs_exp_xgb_mse = mse(y_test, yrs_exp_xgb_preds)
yrs_exp_xgb_rmse = rmse(y_test, yrs_exp_xgb_preds)
yrs_exp_xgb_mape = np.mean(np.abs((y_test - yrs_exp_xgb_preds)/y_test)*100)
yrs_exp_xgb_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_xgb_preds) / y_test)))) * 100
yrs_exp_xgb_model_name = 'Years of Experience XGBoost'

#create easily readable display of performance metrics
yrs_exp_xgb_dictionary = {'Model': yrs_exp_xgb_model_name,
  'Training Set R Squared': yrs_exp_xgb_rsquared_train, 
  'Test Set R Squared': yrs_exp_xgb_rsquared_test, 'Mean Absolute Error': yrs_exp_xgb_mae, 
  'Mean Sqaured Error': yrs_exp_xgb_mse, 'Root Mean Squared Error': yrs_exp_xgb_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_xgb_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_xgb_mape}

yrs_exp_xgb_df = pd.DataFrame(yrs_exp_xgb_dictionary, index=[0])

yrs_exp_xgb_df

"""### **categories_df log(y)**"""

#set features & target
X = categories_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = categories_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7],
              'min_child_weight': [4],
              'silent': [1],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [500]}
  
log_categories_xgb_grid = GridSearchCV(XGBRegressor(), param_grid,  cv = 5,
                        n_jobs = 5, verbose=True)
# fitting the model for grid search 
log_categories_xgb_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_categories_xgb_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_categories_xgb_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_categories_xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=0.7, gamma=0,
             importance_type='gain', learning_rate=0.03, max_delta_step=0,
             max_depth=7, min_child_weight=4, missing=None, n_estimators=500,
             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,
             subsample=0.7, verbosity=1)

log_categories_xgb.fit(X_train, y_train)

#cross validate
log_categories_xgb_scores = cross_val_score(log_categories_xgb, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_categories_xgb_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_categories_xgb_scores)))

#generate predictions
log_categories_xgb_preds = log_categories_xgb.predict(X_test)

#more performance metrics
log_categories_xgb_rsquared_train = log_categories_xgb.score(X_train, y_train)
log_categories_xgb_rsquared_test = log_categories_xgb.score(X_test, y_test)
log_categories_xgb_mae = mean_absolute_error(y_test, log_categories_xgb_preds)
log_categories_xgb_mse = mse(y_test, log_categories_xgb_preds)
log_categories_xgb_rmse = rmse(y_test, log_categories_xgb_preds)
log_categories_xgb_mape = np.mean(np.abs((y_test - log_categories_xgb_preds)/y_test)*100)
log_categories_xgb_rmspe = (np.sqrt(np.mean(np.square((y_test - log_categories_xgb_preds) / y_test)))) * 100
log_categories_xgb_model_name = 'Categories XGBoost Log(y)'

#create easily readable display of performance metrics
log_categories_xgb_dictionary = {'Model': log_categories_xgb_model_name,
  'Training Set R Squared': log_categories_xgb_rsquared_train, 
  'Test Set R Squared': log_categories_xgb_rsquared_test, 'Mean Absolute Error': log_categories_xgb_mae, 
  'Mean Sqaured Error': log_categories_xgb_mse, 'Root Mean Squared Error': log_categories_xgb_rmse,
  'Root Mean Squared Percentage Error': log_categories_xgb_rmspe,
  'Mean Absolute Percentage Error': log_categories_xgb_mape}

log_categories_xgb_df = pd.DataFrame(log_categories_xgb_dictionary, index=[0])

log_categories_xgb_df

"""### **yrs_exp_df log(y)**"""

#set features & target
X = yrs_exp_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7],
              'min_child_weight': [4],
              'silent': [1],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [500]}
  
log_yrs_exp_xgb_grid = GridSearchCV(XGBRegressor(), param_grid,  cv = 5,
                        n_jobs = 5, verbose=True)
# fitting the model for grid search 
log_yrs_exp_xgb_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_yrs_exp_xgb_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_yrs_exp_xgb_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=0.7, gamma=0,
             importance_type='gain', learning_rate=0.03, max_delta_step=0,
             max_depth=5, min_child_weight=4, missing=None, n_estimators=500,
             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,
             subsample=0.7, verbosity=1)

log_yrs_exp_xgb.fit(X_train, y_train)

#cross validate
log_yrs_exp_xgb_scores = cross_val_score(log_yrs_exp_xgb, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_xgb_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_xgb_scores)))

#generate predictions
log_yrs_exp_xgb_preds = log_yrs_exp_xgb.predict(X_test)

#more performance metrics
log_yrs_exp_xgb_rsquared_train = log_yrs_exp_xgb.score(X_train, y_train)
log_yrs_exp_xgb_rsquared_test = log_yrs_exp_xgb.score(X_test, y_test)
log_yrs_exp_xgb_mae = mean_absolute_error(y_test, log_yrs_exp_xgb_preds)
log_yrs_exp_xgb_mse = mse(y_test, log_yrs_exp_xgb_preds)
log_yrs_exp_xgb_rmse = rmse(y_test, log_yrs_exp_xgb_preds)
log_yrs_exp_xgb_mape = np.mean(np.abs((y_test - log_yrs_exp_xgb_preds)/y_test)*100)
log_yrs_exp_xgb_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_xgb_preds) / y_test)))) * 100
log_yrs_exp_xgb_model_name = 'Years of Experience XGBoost Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_xgb_dictionary = {'Model': log_yrs_exp_xgb_model_name,
  'Training Set R Squared': log_yrs_exp_xgb_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_xgb_rsquared_test, 'Mean Absolute Error': log_yrs_exp_xgb_mae, 
  'Mean Sqaured Error': log_yrs_exp_xgb_mse, 'Root Mean Squared Error': log_yrs_exp_xgb_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_xgb_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_xgb_mape}

log_yrs_exp_xgb_df = pd.DataFrame(log_yrs_exp_xgb_dictionary, index=[0])

log_yrs_exp_xgb_df

"""### **cat_no_outliers_df**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7],
              'min_child_weight': [4],
              'silent': [1],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [500]}
  
cat_no_outliers_xgb_grid = GridSearchCV(XGBRegressor(), param_grid,  cv = 5,
                        n_jobs = 5, verbose=True)
# fitting the model for grid search 
cat_no_outliers_xgb_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(cat_no_outliers_xgb_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(cat_no_outliers_xgb_grid.best_estimator_)

#set model with ideal hyperparameters from above
cat_no_outliers_xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=0.7, gamma=0,
             importance_type='gain', learning_rate=0.03, max_delta_step=0,
             max_depth=6, min_child_weight=4, missing=None, n_estimators=500,
             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,
             subsample=0.7, verbosity=1)

cat_no_outliers_xgb.fit(X_train, y_train)

#cross validate
cat_no_outliers_xgb_scores = cross_val_score(cat_no_outliers_xgb, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', cat_no_outliers_xgb_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(cat_no_outliers_xgb_scores)))

#generate predictions
cat_no_outliers_xgb_preds = cat_no_outliers_xgb.predict(X_test)

#more performance metrics
cat_no_outliers_xgb_rsquared_train = cat_no_outliers_xgb.score(X_train, y_train)
cat_no_outliers_xgb_rsquared_test = cat_no_outliers_xgb.score(X_test, y_test)
cat_no_outliers_xgb_mae = mean_absolute_error(y_test, cat_no_outliers_xgb_preds)
cat_no_outliers_xgb_mse = mse(y_test, cat_no_outliers_xgb_preds)
cat_no_outliers_xgb_rmse = rmse(y_test, cat_no_outliers_xgb_preds)
cat_no_outliers_xgb_mape = np.mean(np.abs((y_test - cat_no_outliers_xgb_preds)/y_test)*100)
cat_no_outliers_xgb_rmspe = (np.sqrt(np.mean(np.square((y_test - cat_no_outliers_xgb_preds) / y_test)))) * 100
cat_no_outliers_xgb_model_name = 'Categories w/o Outliers XGBoost'

#create easily readable display of performance metrics
cat_no_outliers_xgb_dictionary = {'Model': cat_no_outliers_xgb_model_name,
  'Training Set R Squared': cat_no_outliers_xgb_rsquared_train, 
  'Test Set R Squared': cat_no_outliers_xgb_rsquared_test, 'Mean Absolute Error': cat_no_outliers_xgb_mae, 
  'Mean Sqaured Error': cat_no_outliers_xgb_mse, 'Root Mean Squared Error': cat_no_outliers_xgb_rmse,
  'Root Mean Squared Percentage Error': cat_no_outliers_xgb_rmspe,
  'Mean Absolute Percentage Error': cat_no_outliers_xgb_mape}

cat_no_outliers_xgb_df = pd.DataFrame(cat_no_outliers_xgb_dictionary, index=[0])

cat_no_outliers_xgb_df

"""### **yrs_exp_no_outliers_df**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7],
              'min_child_weight': [4],
              'silent': [1],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [500]}
  
yrs_exp_no_outliers_xgb_grid = GridSearchCV(XGBRegressor(), param_grid,  cv = 5,
                        n_jobs = 5, verbose=True)
# fitting the model for grid search 
yrs_exp_no_outliers_xgb_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(yrs_exp_no_outliers_xgb_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(yrs_exp_no_outliers_xgb_grid.best_estimator_)

#set model with ideal hyperparameters from above
yrs_exp_no_outliers_xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=0.7, gamma=0,
             importance_type='gain', learning_rate=0.03, max_delta_step=0,
             max_depth=6, min_child_weight=4, missing=None, n_estimators=500,
             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,
             subsample=0.7, verbosity=1)

yrs_exp_no_outliers_xgb.fit(X_train, y_train)

#cross validate
yrs_exp_no_outliers_xgb_scores = cross_val_score(yrs_exp_no_outliers_xgb, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', yrs_exp_no_outliers_xgb_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(yrs_exp_no_outliers_xgb_scores)))

#generate predictions
yrs_exp_no_outliers_xgb_preds = yrs_exp_no_outliers_xgb.predict(X_test)

#more performance metrics
yrs_exp_no_outliers_xgb_rsquared_train = yrs_exp_no_outliers_xgb.score(X_train, y_train)
yrs_exp_no_outliers_xgb_rsquared_test = yrs_exp_no_outliers_xgb.score(X_test, y_test)
yrs_exp_no_outliers_xgb_mae = mean_absolute_error(y_test, yrs_exp_no_outliers_xgb_preds)
yrs_exp_no_outliers_xgb_mse = mse(y_test, yrs_exp_no_outliers_xgb_preds)
yrs_exp_no_outliers_xgb_rmse = rmse(y_test, yrs_exp_no_outliers_xgb_preds)
yrs_exp_no_outliers_xgb_mape = np.mean(np.abs((y_test - yrs_exp_no_outliers_xgb_preds)/y_test)*100)
yrs_exp_no_outliers_xgb_rmspe = (np.sqrt(np.mean(np.square((y_test - yrs_exp_no_outliers_xgb_preds) / y_test)))) * 100
yrs_exp_no_outliers_xgb_model_name = 'Years of Experience w/o Outliers XGBoost'

#create easily readable display of performance metrics
yrs_exp_no_outliers_xgb_dictionary = {'Model': yrs_exp_no_outliers_xgb_model_name,
  'Training Set R Squared': yrs_exp_no_outliers_xgb_rsquared_train, 
  'Test Set R Squared': yrs_exp_no_outliers_xgb_rsquared_test, 'Mean Absolute Error': yrs_exp_no_outliers_xgb_mae, 
  'Mean Sqaured Error': yrs_exp_no_outliers_xgb_mse, 'Root Mean Squared Error': yrs_exp_no_outliers_xgb_rmse,
  'Root Mean Squared Percentage Error': yrs_exp_no_outliers_xgb_rmspe,
  'Mean Absolute Percentage Error': yrs_exp_no_outliers_xgb_mape}

yrs_exp_no_outliers_xgb_df = pd.DataFrame(yrs_exp_no_outliers_xgb_dictionary, index=[0])

yrs_exp_no_outliers_xgb_df

"""### **cat_no_outliers_df log(y)**"""

#set features & target
X = cat_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = cat_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7],
              'min_child_weight': [4],
              'silent': [1],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [500]}
  
log_cat_no_outliers_xgb_grid = GridSearchCV(XGBRegressor(), param_grid,  cv = 5,
                        n_jobs = 5, verbose=True)
# fitting the model for grid search 
log_cat_no_outliers_xgb_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_cat_no_outliers_xgb_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_cat_no_outliers_xgb_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_cat_no_outliers_xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=0.7, gamma=0,
             importance_type='gain', learning_rate=0.03, max_delta_step=0,
             max_depth=7, min_child_weight=4, missing=None, n_estimators=500,
             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,
             subsample=0.7, verbosity=1)

log_cat_no_outliers_xgb.fit(X_train, y_train)

#cross validate
log_cat_no_outliers_xgb_scores = cross_val_score(log_cat_no_outliers_xgb, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_cat_no_outliers_xgb_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_cat_no_outliers_xgb_scores)))

#generate predictions
log_cat_no_outliers_xgb_preds = log_cat_no_outliers_xgb.predict(X_test)

#more performance metrics
log_cat_no_outliers_xgb_rsquared_train = log_cat_no_outliers_xgb.score(X_train, y_train)
log_cat_no_outliers_xgb_rsquared_test = log_cat_no_outliers_xgb.score(X_test, y_test)
log_cat_no_outliers_xgb_mae = mean_absolute_error(y_test, log_cat_no_outliers_xgb_preds)
log_cat_no_outliers_xgb_mse = mse(y_test, log_cat_no_outliers_xgb_preds)
log_cat_no_outliers_xgb_rmse = rmse(y_test, log_cat_no_outliers_xgb_preds)
log_cat_no_outliers_xgb_mape = np.mean(np.abs((y_test - log_cat_no_outliers_xgb_preds)/y_test)*100)
log_cat_no_outliers_xgb_rmspe = (np.sqrt(np.mean(np.square((y_test - log_cat_no_outliers_xgb_preds) / y_test)))) * 100
log_cat_no_outliers_xgb_model_name = 'Categories w/o Outliers XGBoost Log(y)'

#create easily readable display of performance metrics
log_cat_no_outliers_xgb_dictionary = {'Model': log_cat_no_outliers_xgb_model_name,
  'Training Set R Squared': log_cat_no_outliers_xgb_rsquared_train, 
  'Test Set R Squared': log_cat_no_outliers_xgb_rsquared_test, 'Mean Absolute Error': log_cat_no_outliers_xgb_mae, 
  'Mean Sqaured Error': log_cat_no_outliers_xgb_mse, 'Root Mean Squared Error': log_cat_no_outliers_xgb_rmse,
  'Root Mean Squared Percentage Error': log_cat_no_outliers_xgb_rmspe,
  'Mean Absolute Percentage Error': log_cat_no_outliers_xgb_mape}

log_cat_no_outliers_xgb_df = pd.DataFrame(log_cat_no_outliers_xgb_dictionary, index=[0])

log_cat_no_outliers_xgb_df

"""### **yrs_exp_no_outliers_df log(y)**"""

#set features & target
X = yrs_exp_no_outliers_df.drop(columns=['avg_sal', 'log_avg_sal'])
y = yrs_exp_no_outliers_df['log_avg_sal']

#split data into training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)

#Use grid search to find best hyperparameters
# defining parameter range 
param_grid = {'nthread':[4], #when use hyperthread, xgboost may become slower
              'objective':['reg:linear'],
              'learning_rate': [.03, 0.05, .07], #so called `eta` value
              'max_depth': [5, 6, 7],
              'min_child_weight': [4],
              'silent': [1],
              'subsample': [0.7],
              'colsample_bytree': [0.7],
              'n_estimators': [500]}
  
log_yrs_exp_no_outliers_xgb_grid = GridSearchCV(XGBRegressor(), param_grid,  cv = 5,
                        n_jobs = 5, verbose=True)
# fitting the model for grid search 
log_yrs_exp_no_outliers_xgb_grid.fit(X_train, y_train) 
# print best parameter after tuning 
print(log_yrs_exp_no_outliers_xgb_grid.best_params_) 
  
# print how our model looks after hyper-parameter tuning 
print(log_yrs_exp_no_outliers_xgb_grid.best_estimator_)

#set model with ideal hyperparameters from above
log_yrs_exp_no_outliers_xgb = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=0.7, gamma=0,
             importance_type='gain', learning_rate=0.03, max_delta_step=0,
             max_depth=6, min_child_weight=4, missing=None, n_estimators=500,
             n_jobs=1, nthread=4, objective='reg:linear', random_state=0,
             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None, silent=1,
             subsample=0.7, verbosity=1)

log_yrs_exp_no_outliers_xgb.fit(X_train, y_train)

#cross validate
log_yrs_exp_no_outliers_xgb_scores = cross_val_score(log_yrs_exp_no_outliers_xgb, X_train, y_train, cv = 5)

print('Accuracy Scores for the 5 folds: ', log_yrs_exp_no_outliers_xgb_scores)
print('Mean Cross Validation Score: {:.33f}'.format(np.mean(log_yrs_exp_no_outliers_xgb_scores)))

#generate predictions
log_yrs_exp_no_outliers_xgb_preds = log_yrs_exp_no_outliers_xgb.predict(X_test)

#more performance metrics
log_yrs_exp_no_outliers_xgb_rsquared_train = log_yrs_exp_no_outliers_xgb.score(X_train, y_train)
log_yrs_exp_no_outliers_xgb_rsquared_test = log_yrs_exp_no_outliers_xgb.score(X_test, y_test)
log_yrs_exp_no_outliers_xgb_mae = mean_absolute_error(y_test, log_yrs_exp_no_outliers_xgb_preds)
log_yrs_exp_no_outliers_xgb_mse = mse(y_test, log_yrs_exp_no_outliers_xgb_preds)
log_yrs_exp_no_outliers_xgb_rmse = rmse(y_test, log_yrs_exp_no_outliers_xgb_preds)
log_yrs_exp_no_outliers_xgb_mape = np.mean(np.abs((y_test - log_yrs_exp_no_outliers_xgb_preds)/y_test)*100)
log_yrs_exp_no_outliers_xgb_rmspe = (np.sqrt(np.mean(np.square((y_test - log_yrs_exp_no_outliers_xgb_preds) / y_test)))) * 100
log_yrs_exp_no_outliers_xgb_model_name = 'Years of Experience w/o Outliers XGBoost Log(y)'

#create easily readable display of performance metrics
log_yrs_exp_no_outliers_xgb_dictionary = {'Model': log_yrs_exp_no_outliers_xgb_model_name,
  'Training Set R Squared': log_yrs_exp_no_outliers_xgb_rsquared_train, 
  'Test Set R Squared': log_yrs_exp_no_outliers_xgb_rsquared_test, 'Mean Absolute Error': log_yrs_exp_no_outliers_xgb_mae, 
  'Mean Sqaured Error': log_yrs_exp_no_outliers_xgb_mse, 'Root Mean Squared Error': log_yrs_exp_no_outliers_xgb_rmse,
  'Root Mean Squared Percentage Error': log_yrs_exp_no_outliers_xgb_rmspe,
  'Mean Absolute Percentage Error': log_yrs_exp_no_outliers_xgb_mape}

log_yrs_exp_no_outliers_xgb_df = pd.DataFrame(log_yrs_exp_no_outliers_xgb_dictionary, index=[0])

log_yrs_exp_no_outliers_xgb_df

"""# **Model Comparison**

### **Categories & Years of Experience Model Results**
"""

results_list = [
categories_lr_df, yrs_exp_lr_df, log_categories_lr_df, log_yrs_exp_lr_df, 
cat_no_outliers_lr_df, yrs_exp_no_outliers_lr_df, log_cat_no_outliers_lr_df, 
log_yrs_exp_no_outliers_lr_df, 

categories_ridge_df, yrs_exp_ridge_df, log_categories_ridge_df, 
log_yrs_exp_ridge_df, cat_no_outliers_ridge_df, yrs_exp_no_outliers_ridge_df, 
log_cat_no_outliers_ridge_df,log_yrs_exp_no_outliers_ridge_df,

categories_lasso_df, yrs_exp_lasso_df, log_categories_lasso_df, 
log_yrs_exp_lasso_df, cat_no_outliers_lasso_df, yrs_exp_no_outliers_lasso_df, 
log_cat_no_outliers_lasso_df, log_yrs_exp_no_outliers_lasso_df, 

categories_enet_df, yrs_exp_enet_df, log_categories_enet_df, 
log_yrs_exp_enet_df, cat_no_outliers_enet_df, yrs_exp_no_outliers_enet_df, 
log_cat_no_outliers_enet_df, log_yrs_exp_no_outliers_enet_df,

categories_knn_df, years_exp_knn_df, log_categories_knn_df, 
log_years_exp_knn_df, cat_no_outliers_knn_df, years_exp_knn_df, 
log_cat_no_outliers_knn_df, log_years_exp_knn_df, 

categories_rf_df, yrs_exp_rf_df, log_categories_rf_df, 
log_yrs_exp_rf_df, cat_no_outliers_rf_df, yrs_exp_no_outliers_rf_df, 
log_cat_no_outliers_rf_df, log_yrs_exp_no_outliers_rf_df,

categories_svm_df, yrs_exp_svm_df, log_categories_svm_df, 
log_yrs_exp_svm_df, cat_no_outliers_svm_df, yrs_exp_no_outliers_svm_df, 
log_cat_no_outliers_svm_df, log_yrs_exp_no_outliers_svm_df, 

categories_xgb_df,yrs_exp_xgb_df, log_categories_xgb_df, log_yrs_exp_xgb_df, 
cat_no_outliers_xgb_df, yrs_exp_no_outliers_xgb_df, log_cat_no_outliers_xgb_df, 
log_yrs_exp_no_outliers_xgb_df
]

results_df = pd.concat(results_list)
results_df.reset_index(inplace=True)
results_df.drop(columns=['index'], inplace=True)
results_df

results_df[:32]

results_df[32:]

# results_df.to_csv('results_df.csv')

"""### **NLP Model Results**

A separate notebook containing exploration of NLP Bag of Words and TF-IDF models that were tested with XGBoost, since it was the best performing model. The time it takes to run these models is extensive and is therefore the reason a separate notebook was used. The results of these models can be seen below. 

To view this notebook, please click [here](https://https://colab.research.google.com/drive/1wEl-dw0npunSUZgS5qAFXapRA779HcLw?usp=sharing)
"""

nlp_results = pd.read_csv('https://raw.githubusercontent.com/ferrazzijk/Glassdoor-Scraping/main/ScrapedDataCSVs/nlp_results.csv?dl=1')
nlp_results.drop(columns=['Unnamed: 0'], inplace=True)
nlp_results